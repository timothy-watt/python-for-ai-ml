{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Chapter 12 -- MLOps and Production ML\n## *Python for AI/ML: A Complete Learning Journey*\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/CH12_MLOps_Production_ML.ipynb)\n&nbsp;&nbsp;[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n\n---\n\n**Part:** 4 -- Production and Deployment  \n**Prerequisites:** Chapter 6 (scikit-learn), Chapter 7 (PyTorch)  \n**Estimated time:** 5-6 hours\n\n---\n\n### Learning Objectives\n\nBy the end of this chapter you will be able to:\n\n- Explain the ML lifecycle and where MLOps fits within it\n- Track experiments with MLflow: log parameters, metrics, and artefacts\n- Version and register models in the MLflow Model Registry\n- Serve a trained model as a REST API endpoint using FastAPI\n- Write and run unit tests for ML preprocessing and prediction code\n- Detect data drift by comparing training and production distributions\n- Build a lightweight model monitoring dashboard\n- Structure an ML project repository for collaboration and reproducibility\n\n---\n\n### Why MLOps?\n\nTraining a model is roughly 10% of the work in a production ML system.\nThe other 90% is everything around it: tracking what you tried, packaging\nthe model so others can use it, serving predictions reliably, monitoring\nfor when the world changes and the model degrades, and reproducing results\nsix months later when a colleague asks why the model made a particular decision.\n\nMLOps is the engineering discipline that makes ML systems maintainable.\nThis chapter gives you the core toolkit.\n\n---\n\n### Project Thread -- Chapter 10\n\nWe take the Chapter 6 salary regression model through a complete MLOps workflow:\ninstrument the training with MLflow, compare three model variants in the UI,\nregister the best model, wrap it in a FastAPI endpoint, write tests,\nand build a drift monitor that would alert if production salary data\nshifts away from the training distribution.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Setup -- Install and Import\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import subprocess\nsubprocess.run(['pip', 'install', 'mlflow', 'fastapi', 'uvicorn',\n                'httpx', 'evidently', '-q'], check=False)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport mlflow\nimport mlflow.sklearn\nfrom mlflow.models import infer_signature\n\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n\nimport mlflow\nprint(f'MLflow version:  {mlflow.__version__}')\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.dpi']       = 110\nplt.rcParams['axes.titlesize']   = 13\nplt.rcParams['axes.titleweight'] = 'bold'\n\nDATASET_URL  = 'https://raw.githubusercontent.com/timothy-watt/python-for-ai-ml/main/data/so_survey_2025_curated.csv'\nRANDOM_STATE = 42\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Load and clean SO 2025 -- standard pipeline\ndf_raw = pd.read_csv(DATASET_URL)\ndf = df_raw.copy()\ndf = df.dropna(subset=['ConvertedCompYearly'])\ndf['ConvertedCompYearly'] = pd.to_numeric(df['ConvertedCompYearly'], errors='coerce')\nQ1, Q3 = df['ConvertedCompYearly'].quantile([0.25, 0.75])\nIQR = Q3 - Q1\ndf = df[\n    (df['ConvertedCompYearly'] >= max(Q1 - 3*IQR, 5_000)) &\n    (df['ConvertedCompYearly'] <= min(Q3 + 3*IQR, 600_000))\n].copy()\nif 'YearsCodePro' in df.columns:\n    df['YearsCodePro'] = pd.to_numeric(df['YearsCodePro'], errors='coerce')\n    df['YearsCodePro'] = df['YearsCodePro'].fillna(df['YearsCodePro'].median())\ndf['uses_python'] = df.get('LanguageHaveWorkedWith', pd.Series(dtype=str)).str.contains('Python', na=False).astype(int)\ndf['uses_sql']    = df.get('LanguageHaveWorkedWith', pd.Series(dtype=str)).str.contains('SQL', na=False).astype(int)\ndf['uses_js']     = df.get('LanguageHaveWorkedWith', pd.Series(dtype=str)).str.contains('JavaScript', na=False).astype(int)\ndf['uses_ai']     = df.get('AIToolCurrently', pd.Series(dtype=str)).notna().astype(int)\ndf['log_salary']  = np.log(df['ConvertedCompYearly'])\ndf = df.reset_index(drop=True)\n\nFEATURE_COLS = [c for c in ['YearsCodePro','uses_python','uses_sql','uses_js','uses_ai']\n                if c in df.columns]\nX = df[FEATURE_COLS].copy()\nfor col in FEATURE_COLS:\n    med = X[col].median()\n    X[col] = X[col].fillna(med if pd.notna(med) else 0)\ny = df['log_salary']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=RANDOM_STATE\n)\nprint(f'Dataset ready: {len(df):,} rows')\nprint(f'Features: {FEATURE_COLS}')\nprint(f'Train: {len(X_train):,}  Test: {len(X_test):,}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 10.1 -- The ML Lifecycle\n\nA production ML system has six phases that repeat in a continuous loop:\n\n```\n  Define        Collect       Train &      Evaluate     Deploy       Monitor\n  Problem  -->  & Clean  -->  Experiment --> & Select --> & Serve --> & Retrain\n    |           Data          (MLflow)       Model       (FastAPI)    (Drift)\n    |_______________________________________________________________|  (loop)\n```\n\nChapters 3-9 covered the middle phases in depth. This chapter fills in\n**Experiment tracking**, **Deploy & Serve**, and **Monitor & Retrain** --\nthe phases most often skipped in tutorials but most important in practice.\n\n### The core MLOps problems\n\n**Reproducibility** -- can you recreate the exact model that went to production\nsix months ago? Without tracking, the answer is almost always no.\n\n**Collaboration** -- when three people are running experiments simultaneously,\nhow do you compare results and agree on which model to deploy?\n\n**Deployment gap** -- a model that works in a notebook often breaks when moved\nto production because of subtle differences in data preprocessing.\n\n**Model decay** -- the world changes. A model trained on 2024 data will gradually\nbecome less accurate as 2025 data arrives. Monitoring detects this before users do.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 10.2 -- Experiment Tracking with MLflow\n\nMLflow is the most widely used open-source MLOps platform. Its core concept:\nevery training run is a logged **experiment** with parameters, metrics,\nartefacts (model files, plots), and metadata. You can compare runs in a UI\nand promote the best model to a registry.\n\nThe four MLflow components we use:\n- **Tracking** -- log parameters and metrics during training\n- **Models** -- save models in a standard format with schema\n- **Model Registry** -- version and stage models (Staging â†’ Production)\n- **Projects** -- package code for reproducible execution (covered in 10.5)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 10.2.1 -- Configure MLflow tracking\n\nimport os\n\n# In Colab we use a local SQLite tracking store\n# In production this would point to a remote server\nMLFLOW_DIR      = '/tmp/mlflow'\nos.makedirs(MLFLOW_DIR, exist_ok=True)\nmlflow.set_tracking_uri(f'sqlite:///{MLFLOW_DIR}/mlflow.db')\n\nEXPERIMENT_NAME = 'so2025_salary_regression'\nmlflow.set_experiment(EXPERIMENT_NAME)\n\nprint(f'MLflow tracking URI: {mlflow.get_tracking_uri()}')\nprint(f'Experiment: {EXPERIMENT_NAME}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 10.2.2 -- Log three model variants as separate MLflow runs\n#\n# Each run logs:\n#   - Parameters: model hyperparameters and feature list\n#   - Metrics:    CV R^2, test R^2, test MAE\n#   - Artefacts:  the trained model with input/output schema\n\nmodel_configs = [\n    ('Ridge',            Ridge(alpha=1.0),\n     {'alpha': 1.0}),\n    ('RandomForest',     RandomForestRegressor(n_estimators=100, max_depth=8,\n                                               random_state=RANDOM_STATE, n_jobs=-1),\n     {'n_estimators': 100, 'max_depth': 8}),\n    ('GradientBoosting', GradientBoostingRegressor(n_estimators=100, max_depth=4,\n                                                    learning_rate=0.1,\n                                                    random_state=RANDOM_STATE),\n     {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.1}),\n]\n\nrun_results = []\n\nfor model_name, model, params in model_configs:\n    pipe = Pipeline([\n        ('imputer', SimpleImputer(strategy='median')),\n        ('scaler',  StandardScaler()),\n        ('model',   model),\n    ])\n\n    with mlflow.start_run(run_name=model_name):\n        # Log parameters\n        mlflow.log_param('model_type',    model_name)\n        mlflow.log_param('features',      str(FEATURE_COLS))\n        mlflow.log_param('train_size',    len(X_train))\n        for k, v in params.items():\n            mlflow.log_param(k, v)\n\n        # Train\n        pipe.fit(X_train, y_train)\n\n        # Cross-validation on training set\n        cv_scores = cross_val_score(pipe, X_train, y_train, cv=5, scoring='r2')\n        mlflow.log_metric('cv_r2_mean', cv_scores.mean())\n        mlflow.log_metric('cv_r2_std',  cv_scores.std())\n\n        # Test set metrics\n        y_pred_log = pipe.predict(X_test)\n        y_pred_usd = np.exp(y_pred_log)\n        y_true_usd = np.exp(y_test)\n        test_r2  = r2_score(y_test, y_pred_log)\n        test_mae = mean_absolute_error(y_true_usd, y_pred_usd)\n        test_rmse = np.sqrt(mean_squared_error(y_true_usd, y_pred_usd))\n        mlflow.log_metric('test_r2',   test_r2)\n        mlflow.log_metric('test_mae',  test_mae)\n        mlflow.log_metric('test_rmse', test_rmse)\n\n        # Log the model with input/output schema\n        signature = infer_signature(X_train, pipe.predict(X_train))\n        mlflow.sklearn.log_model(\n            pipe, artifact_path='model',\n            signature=signature,\n            input_example=X_train.iloc[:3]\n        )\n\n        run_id = mlflow.active_run().info.run_id\n        run_results.append({\n            'model': model_name, 'run_id': run_id,\n            'cv_r2': cv_scores.mean(), 'test_r2': test_r2,\n            'test_mae': test_mae\n        })\n\n    print(f'{model_name:<20} CV R2={cv_scores.mean():.4f}  '\n          f'Test R2={test_r2:.4f}  MAE=${test_mae:,.0f}')\n\nresults_df = pd.DataFrame(run_results)\nbest_run   = results_df.loc[results_df['test_r2'].idxmax()]\nprint(f'Best model: {best_run[\"model\"]}  (run_id: {best_run[\"run_id\"][:8]}...)')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 10.2.3 -- Query MLflow programmatically and visualise run comparison\n\nclient = mlflow.tracking.MlflowClient()\nexperiment = client.get_experiment_by_name(EXPERIMENT_NAME)\nruns = client.search_runs(\n    experiment_ids=[experiment.experiment_id],\n    order_by=['metrics.test_r2 DESC']\n)\n\nprint(f'Runs logged: {len(runs)}')\nprint()\nprint(f'{\"Run name\":<22} {\"CV R2\":>8} {\"Test R2\":>9} {\"Test MAE\":>12}')\nprint('-' * 55)\nfor run in runs:\n    name = run.data.tags.get('mlflow.runName', run.info.run_id[:8])\n    cv   = run.data.metrics.get('cv_r2_mean', 0)\n    tr2  = run.data.metrics.get('test_r2', 0)\n    mae  = run.data.metrics.get('test_mae', 0)\n    print(f'{name:<22} {cv:>8.4f} {tr2:>9.4f} {mae:>12,.0f}')\n\n# Bar chart comparison\nfig, axes = plt.subplots(1, 2, figsize=(13, 4))\nnames = [r.data.tags.get('mlflow.runName', r.info.run_id[:8]) for r in runs]\nr2s   = [r.data.metrics.get('test_r2', 0) for r in runs]\nmaes  = [r.data.metrics.get('test_mae', 0) for r in runs]\n\naxes[0].bar(names, r2s, color='#2E75B6', edgecolor='white')\naxes[0].set_ylabel('Test R^2 (higher = better)')\naxes[0].set_title('Model Comparison: R^2')\nfor i, v in enumerate(r2s):\n    axes[0].text(i, v + 0.002, f'{v:.4f}', ha='center', fontsize=9)\n\naxes[1].bar(names, [m/1000 for m in maes], color='#E8722A', edgecolor='white')\naxes[1].set_ylabel('Test MAE ($k, lower = better)')\naxes[1].set_title('Model Comparison: MAE')\nfor i, v in enumerate(maes):\n    axes[1].text(i, v/1000 + 0.3, f'${v/1000:.1f}k', ha='center', fontsize=9)\n\nplt.suptitle('MLflow Experiment: SO 2025 Salary Regression',\n             fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 10.3 -- Model Registry: Versioning and Staging\n\nThe MLflow Model Registry is a centralised store for model versions.\nEach registered model can move through stages:\n`None â†’ Staging â†’ Production â†’ Archived`.\n\nThis stage progression is the handoff point between data scientists\n(who produce models) and ML engineers (who deploy them). The registry\nrecords who promoted a model, when, and why -- a full audit trail.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 10.3.1 -- Register the best model and transition to Staging\n\nREGISTERED_MODEL_NAME = 'so2025_salary_predictor'\n\nbest_run_id = best_run['run_id']\nmodel_uri   = f'runs:/{best_run_id}/model'\n\n# Register the model\nregistered = mlflow.register_model(\n    model_uri=model_uri,\n    name=REGISTERED_MODEL_NAME\n)\n\nprint(f'Registered model: {registered.name}')\nprint(f'Version:          {registered.version}')\nprint(f'Status:           {registered.status}')\n\n# Transition to Staging\nimport time\ntime.sleep(2)   # allow registration to complete\n\nclient.transition_model_version_stage(\n    name=REGISTERED_MODEL_NAME,\n    version=registered.version,\n    stage='Staging',\n    archive_existing_versions=False\n)\nprint(f'Transitioned to:  Staging')\n\n# Retrieve model details from registry\nmodel_details = client.get_registered_model(REGISTERED_MODEL_NAME)\nprint(f'Latest versions:  {[(v.version, v.current_stage) for v in model_details.latest_versions]}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 10.3.2 -- Load the registered model and make predictions\n#\n# This is how a serving system loads a model from the registry\n# without needing to know the run ID or file path.\n\nstaging_uri = f'models:/{REGISTERED_MODEL_NAME}/Staging'\nloaded_model = mlflow.sklearn.load_model(staging_uri)\n\n# Make predictions with the loaded model\nsample = X_test.iloc[:5]\npreds_log = loaded_model.predict(sample)\npreds_usd = np.exp(preds_log)\n\nprint('Predictions from registry-loaded model:')\nprint(f'{\"Sample\":>8}  {\"Predicted\":>14}  {\"Actual\":>14}')\nprint('-' * 38)\nfor i, (pred, true) in enumerate(zip(preds_usd, np.exp(y_test.iloc[:5]))):\n    print(f'{i+1:>8}  ${pred:>13,.0f}  ${true:>13,.0f}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 10.4 -- Serving Predictions with FastAPI\n\nA trained model sitting in a file is not useful to anyone who cannot run Python.\nA REST API wraps the model so any application -- a web app, mobile app,\nor another service -- can send a request and receive a prediction.\n\n**FastAPI** is the modern standard for Python REST APIs: fast, type-safe,\nand auto-generates interactive documentation at `/docs`.\n\nWe simulate the API here by writing the app code and testing it in-process.\nIn production you would run `uvicorn app:app --host 0.0.0.0 --port 8000`.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 10.4.1 -- Write the FastAPI app\n\nAPI_CODE = '''\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel, Field\nimport numpy as np\nimport pandas as pd\nimport mlflow.sklearn\nimport os\n\napp = FastAPI(\n    title=\"SO 2025 Salary Predictor\",\n    description=\"Predicts annual developer salary from profile features.\",\n    version=\"1.0.0\"\n)\n\n# Load model at startup -- not on every request\nMODEL = None\n\n@app.on_event(\"startup\")\ndef load_model() -> None:\n    global MODEL\n    MODEL = mlflow.sklearn.load_model(os.environ[\"MODEL_URI\"])\n\nclass DeveloperProfile(BaseModel):\n    years_code_pro: float = Field(..., ge=0, le=50, description=\"Years of professional coding\")\n    uses_python:    int   = Field(..., ge=0, le=1,  description=\"1 if uses Python, else 0\")\n    uses_sql:       int   = Field(..., ge=0, le=1,  description=\"1 if uses SQL, else 0\")\n    uses_js:        int   = Field(..., ge=0, le=1,  description=\"1 if uses JavaScript, else 0\")\n    uses_ai:        int   = Field(..., ge=0, le=1,  description=\"1 if uses AI tools, else 0\")\n\nclass SalaryPrediction(BaseModel):\n    predicted_salary_usd: float\n    predicted_salary_log: float\n    model_version:        str\n\n@app.get(\"/health\")\ndef health() -> dict:\n    return {\"status\": \"ok\", \"model_loaded\": MODEL is not None}\n\n@app.post(\"/predict\", response_model=SalaryPrediction)\ndef predict(profile: DeveloperProfile) -> dict:\n    if MODEL is None:\n        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n    features = pd.DataFrame([{\n        \"YearsCodePro\": profile.years_code_pro,\n        \"uses_python\":  profile.uses_python,\n        \"uses_sql\":     profile.uses_sql,\n        \"uses_js\":      profile.uses_js,\n        \"uses_ai\":      profile.uses_ai,\n    }])\n    log_pred = float(MODEL.predict(features)[0])\n    return SalaryPrediction(\n        predicted_salary_usd=round(np.exp(log_pred), 2),\n        predicted_salary_log=round(log_pred, 6),\n        model_version=\"1.0.0\"\n    )\n'''\n\nwith open('/tmp/salary_api.py', 'w') as f:\n    f.write(API_CODE)\nprint('FastAPI app written to /tmp/salary_api.py')\nprint()\nprint('To run in production:')\nprint('  export MODEL_URI=\"models:/so2025_salary_predictor/Staging\"')\nprint('  uvicorn salary_api:app --host 0.0.0.0 --port 8000')\nprint()\nprint('Auto-generated docs available at: http://localhost:8000/docs')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 10.4.2 -- Test the API in-process using FastAPI's TestClient\n\nimport sys\nsys.path.insert(0, '/tmp')\n\nimport os\nos.environ['MODEL_URI'] = f'runs:/{best_run_id}/model'\n\n# Patch the startup event and load model directly for testing\nfrom fastapi.testclient import TestClient\nimport importlib.util\n\nspec = importlib.util.spec_from_file_location('salary_api', '/tmp/salary_api.py')\nsalary_api = importlib.util.load_from_spec = None\n\n# Instead of importing the module (which needs uvicorn startup),\n# we test the prediction logic directly using the loaded model\ntest_profiles = [\n    {'YearsCodePro': 10, 'uses_python': 1, 'uses_sql': 1, 'uses_js': 0, 'uses_ai': 1},\n    {'YearsCodePro': 2,  'uses_python': 0, 'uses_sql': 1, 'uses_js': 1, 'uses_ai': 0},\n    {'YearsCodePro': 20, 'uses_python': 1, 'uses_sql': 0, 'uses_js': 0, 'uses_ai': 1},\n]\n\nprint('Simulated API predictions:')\nprint(f'{\"Profile\":<45} {\"Predicted Salary\":>18}')\nprint('-' * 65)\nfor p in test_profiles:\n    feat_dict = {col: p.get(col, p.get('YearsCodePro', 0)) for col in FEATURE_COLS}\n    features = pd.DataFrame([feat_dict])\n    log_pred = float(loaded_model.predict(features)[0])\n    usd_pred = np.exp(log_pred)\n    desc = (f\"{p['YearsCodePro']}yrs, \"\n            f\"{'Python' if p['uses_python'] else 'no-Python'}, \"\n            f\"{'SQL' if p['uses_sql'] else 'no-SQL'}, \"\n            f\"{'AI' if p['uses_ai'] else 'no-AI'}\")\n    print(f'{desc:<45} ${usd_pred:>17,.0f}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 10.5 -- Unit Testing ML Code\n\nML code is harder to test than ordinary software because outputs are\nprobabilistic -- you cannot assert that `predict([5, 1, 0, 0, 1]) == 95000`.\nInstead, you test the things that are deterministic:\n\n- **Data contracts:** does the preprocessing produce the expected shape and dtype?\n- **Boundary conditions:** does the model return a finite positive number for valid input?\n- **Regression tests:** does the retrained model perform at least as well as the baseline?\n- **Data validation:** does the pipeline reject inputs that violate the schema?\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 10.5.1 -- Write and run ML unit tests without pytest infrastructure\n# (pytest runs from the command line; we simulate it here inline)\n\nimport traceback\n\ndef assert_equal(val, expected, msg: str = '') -> None:\n    assert val == expected, f'FAIL: {msg} -- got {val}, expected {expected}'\n\ndef assert_true(condition: bool, msg: str = '') -> None:\n    assert condition, f'FAIL: {msg}'\n\ndef assert_close(val: float, expected: float, tol: float = 0.01, msg: str = '') -> None:\n    assert abs(val - expected) <= tol, (\n        f'FAIL: {msg} -- got {val:.4f}, expected {expected:.4f} +/- {tol}'\n    )\n\ntests_passed = 0\ntests_failed = 0\n\ndef run_test(name: str, fn: \"callable\") -> None:\n    global tests_passed, tests_failed\n    try:\n        fn()\n        print(f'  PASS  {name}')\n        tests_passed += 1\n    except Exception as e:\n        print(f'  FAIL  {name}: {e}')\n        tests_failed += 1\n\n# -- Tests --\n\ndef test_feature_count() -> None:\n    assert_equal(X_train.shape[1], len(FEATURE_COLS),\n                 'Training feature count matches FEATURE_COLS')\n\ndef test_no_nulls_after_cleaning() -> None:\n    assert_equal(int(X_train.isnull().sum().sum()), 0,\n                 'No nulls in training features after cleaning')\n\ndef test_prediction_is_finite() -> None:\n    sample = X_test.iloc[:10]\n    preds  = loaded_model.predict(sample)\n    assert_true(np.all(np.isfinite(preds)),\n                'All predictions are finite numbers')\n\ndef test_prediction_in_plausible_range() -> None:\n    sample   = X_test.iloc[:50]\n    preds_usd = np.exp(loaded_model.predict(sample))\n    assert_true(preds_usd.min() > 1_000,  'No predictions below $1,000')\n    assert_true(preds_usd.max() < 2_000_000, 'No predictions above $2M')\n\ndef test_model_beats_mean_baseline() -> None:\n    # A model that always predicts the mean has R^2 = 0\n    # Our model must beat this significantly\n    test_r2 = r2_score(y_test, loaded_model.predict(X_test))\n    assert_true(test_r2 > 0.1, f'Model R^2={test_r2:.4f} exceeds mean baseline (0.0)')\n\ndef test_log_salary_target_range() -> None:\n    # log(5000) ~ 8.5, log(600000) ~ 13.3\n    assert_true(y_train.min() > 8.0,  'Min log salary above log(5000)')\n    assert_true(y_train.max() < 14.0, 'Max log salary below log(1.2M)')\n\nprint('Running ML unit tests...')\nfor name, fn in [\n    ('Feature count matches FEATURE_COLS',    test_feature_count),\n    ('No nulls in training data',             test_no_nulls_after_cleaning),\n    ('Predictions are finite',                test_prediction_is_finite),\n    ('Predictions in plausible salary range', test_prediction_in_plausible_range),\n    ('Model beats mean baseline',             test_model_beats_mean_baseline),\n    ('Log salary target in expected range',   test_log_salary_target_range),\n]:\n    run_test(name, fn)\n\nprint()\nprint(f'Results: {tests_passed} passed, {tests_failed} failed')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 10.6 -- Data Drift Detection\n\n**Data drift** occurs when the statistical distribution of production data\ndiverges from the training data. A salary model trained in 2024 on developers\nearning $50k-$200k will degrade if 2025 production data contains a different\nsalary range, different country mix, or different experience distribution.\n\nDrift detection answers: *Is the data the model is seeing today\nstill similar enough to the data it was trained on?*\n\nWe implement a simple but effective approach using the **Population Stability\nIndex (PSI)** and the **Kolmogorov-Smirnov test** -- both widely used in\nproduction monitoring systems.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 10.6.1 -- Simulate production drift and detect it\n\nfrom scipy import stats\n\n# Simulate 'production' data arriving 12 months after training\n# We inject drift: higher average experience and salary in production\nnp.random.seed(RANDOM_STATE)\nn_prod = 500\n\nprod_data = X_test.sample(n=n_prod, replace=True, random_state=RANDOM_STATE).copy()\n\n# Inject drift: shift YearsCodePro upward (more senior developers in production)\nif 'YearsCodePro' in prod_data.columns:\n    prod_data['YearsCodePro'] = prod_data['YearsCodePro'] + np.random.normal(3, 1, n_prod)\n    prod_data['YearsCodePro'] = prod_data['YearsCodePro'].clip(0, 50)\n\ndef psi(expected: np.ndarray, actual: np.ndarray, bins: int = 10) -> float:\n    \"\"\"\n    Population Stability Index.\n    PSI < 0.1:  no significant drift\n    PSI 0.1-0.2: moderate drift -- investigate\n    PSI > 0.2:  significant drift -- retrain\n    \"\"\"\n    # Build histogram bins from training data\n    breakpoints = np.percentile(expected, np.linspace(0, 100, bins + 1))\n    breakpoints  = np.unique(breakpoints)   # remove duplicates\n    if len(breakpoints) < 3:\n        return 0.0\n    exp_counts = np.histogram(expected, bins=breakpoints)[0] + 1e-6\n    act_counts = np.histogram(actual,   bins=breakpoints)[0] + 1e-6\n    exp_pct = exp_counts / exp_counts.sum()\n    act_pct = act_counts / act_counts.sum()\n    return float(np.sum((act_pct - exp_pct) * np.log(act_pct / exp_pct)))\n\nprint('Data Drift Report')\nprint('=' * 55)\nprint(f'{\"Feature\":<20} {\"KS p-value\":>12} {\"PSI\":>8} {\"Status\"}')\nprint('-' * 55)\n\nfor col in FEATURE_COLS:\n    train_vals = X_train[col].dropna().values\n    prod_vals  = prod_data[col].dropna().values\n    ks_stat, ks_p  = stats.ks_2samp(train_vals, prod_vals)\n    psi_score = psi(train_vals, prod_vals)\n    if psi_score > 0.2 or ks_p < 0.05:\n        status = 'DRIFT DETECTED'\n    elif psi_score > 0.1:\n        status = 'Monitor'\n    else:\n        status = 'OK'\n    print(f'{col:<20} {ks_p:>12.4f} {psi_score:>8.4f} {status}')\n\nprint()\nprint('PSI thresholds: < 0.1 = stable, 0.1-0.2 = investigate, > 0.2 = retrain')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 10.6.2 -- Visualise drift: training vs production distributions\n\ndrift_cols = [c for c in FEATURE_COLS if c == 'YearsCodePro' or\n              X_train[c].nunique() > 2]\nif not drift_cols:\n    drift_cols = FEATURE_COLS[:2]\n\nn_cols = len(drift_cols)\nfig, axes = plt.subplots(1, n_cols, figsize=(6 * n_cols, 4))\nif n_cols == 1:\n    axes = [axes]\n\nfor ax, col in zip(axes, drift_cols):\n    ax.hist(X_train[col].dropna(), bins=30, alpha=0.5,\n            color='#2E75B6', density=True, label='Training')\n    ax.hist(prod_data[col].dropna(), bins=30, alpha=0.5,\n            color='#E8722A', density=True, label='Production (simulated)')\n    psi_val = psi(X_train[col].dropna().values, prod_data[col].dropna().values)\n    ax.set_title(f'{col}\\nPSI={psi_val:.3f}')\n    ax.set_xlabel(col)\n    ax.set_ylabel('Density')\n    ax.legend(fontsize=9)\n\nplt.suptitle('Drift Detection: Training vs Simulated Production Data',\n             fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 10.7 -- CI/CD for ML with GitHub Actions\n\n**CI/CD** (Continuous Integration / Continuous Deployment) means every code\nchange is automatically tested and, if tests pass, deployed.\n\nFor ML projects, a CI pipeline should:\n1. Run data validation tests (does the schema still match?)\n2. Run unit tests on preprocessing and prediction code\n3. Retrain the model on the latest data\n4. Check that the new model meets a minimum performance threshold\n5. Fail the pipeline (and block deployment) if performance regresses\n\n**GitHub Actions** runs this pipeline automatically on every push or pull request.\nIt is free for public repos and included in GitHub's free tier for private repos.\n\nWe write the workflow YAML and the performance regression test here.\nThe cell below generates files you would commit to your repository.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 10.7.1 -- Generate GitHub Actions workflow and regression test\n\nimport os\n\nos.makedirs('/tmp/github_actions_demo/.github/workflows', exist_ok=True)\nos.makedirs('/tmp/github_actions_demo/tests', exist_ok=True)\n\n# â”€â”€ .github/workflows/ml_ci.yml â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nWORKFLOW_YAML = '''\nname: ML CI Pipeline\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  test-and-train:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n          cache: \"pip\"\n\n      - name: Install dependencies\n        run: pip install -r requirements.txt\n\n      - name: Run data validation tests\n        run: pytest tests/test_data.py -v\n\n      - name: Run model unit tests\n        run: pytest tests/test_model.py -v\n\n      - name: Train model\n        run: python src/train.py --output models/salary_model.joblib\n\n      - name: Run performance regression test\n        run: python tests/test_performance.py\n        env:\n          MIN_R2:  \"0.10\"   # pipeline fails if R^2 drops below this\n          MAX_MAE: \"35000\"  # pipeline fails if MAE exceeds this (USD)\n\n      - name: Upload model artefact\n        uses: actions/upload-artifact@v4\n        with:\n          name: salary-model\n          path: models/salary_model.joblib\n          retention-days: 30\n'''\n\n# â”€â”€ tests/test_performance.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nPERF_TEST = '''\n\"\"\"\nPerformance regression test.\nRun after training. Fails CI if model metrics fall below thresholds.\n\"\"\"\nimport os, sys, joblib\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import r2_score, mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\nMIN_R2  = float(os.environ.get(\"MIN_R2\",  \"0.10\"))\nMAX_MAE = float(os.environ.get(\"MAX_MAE\", \"35000\"))\n\nDATASET_URL  = os.environ.get(\"DATASET_URL\",\n    \"https://raw.githubusercontent.com/timothy-watt/python-for-ai-ml/main/data/so_survey_2025_curated.csv\"\n)\nMODEL_PATH   = os.environ.get(\"MODEL_PATH\", \"models/salary_model.joblib\")\n\ndef load_data() -> \"pd.DataFrame\":\n    df = pd.read_csv(DATASET_URL)\n    df = df.dropna(subset=[\"ConvertedCompYearly\"])\n    df[\"ConvertedCompYearly\"] = pd.to_numeric(df[\"ConvertedCompYearly\"], errors=\"coerce\")\n    Q1, Q3 = df[\"ConvertedCompYearly\"].quantile([0.25, 0.75])\n    IQR = Q3 - Q1\n    df = df[(df[\"ConvertedCompYearly\"] >= max(Q1 - 3*IQR, 5000)) &\n            (df[\"ConvertedCompYearly\"] <= min(Q3 + 3*IQR, 600000))].copy()\n    for col in [\"YearsCodePro\"]:\n        if col in df.columns:\n            df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(df[col].median())\n    for lang, col in [(\"Python\", \"uses_python\"), (\"SQL\", \"uses_sql\"),\n                       (\"JavaScript\", \"uses_js\")]:\n        df[col] = df.get(\"LanguageHaveWorkedWith\", pd.Series(dtype=str)).str.contains(lang, na=False).astype(int)\n    df[\"uses_ai\"]   = df.get(\"AIToolCurrently\", pd.Series(dtype=str)).notna().astype(int)\n    df[\"log_salary\"] = np.log(df[\"ConvertedCompYearly\"])\n    return df\n\ndef test_model_performance() -> None:\n    df    = load_data()\n    feats = [c for c in [\"YearsCodePro\",\"uses_python\",\"uses_sql\",\"uses_js\",\"uses_ai\"]\n             if c in df.columns]\n    X = df[feats].fillna(0)\n    y = df[\"log_salary\"]\n    _, X_test, _, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    model    = joblib.load(MODEL_PATH)\n    y_pred   = model.predict(X_test)\n    r2       = r2_score(y_test, y_pred)\n    mae      = mean_absolute_error(np.exp(y_test), np.exp(y_pred))\n\n    print(f\"Test R^2: {r2:.4f}  (min required: {MIN_R2})\")\n    print(f\"Test MAE: ${mae:,.0f}  (max allowed: ${MAX_MAE:,.0f})\")\n\n    assert r2 >= MIN_R2,  f\"REGRESSION: R^2={r2:.4f} below threshold {MIN_R2}\"\n    assert mae <= MAX_MAE, f\"REGRESSION: MAE=${mae:,.0f} above threshold ${MAX_MAE:,.0f}\"\n    print(\"Performance regression test PASSED\")\n\nif __name__ == \"__main__\":\n    test_model_performance()\n'''\n\n# Write files\nwith open('/tmp/github_actions_demo/.github/workflows/ml_ci.yml', 'w') as f:\n    f.write(WORKFLOW_YAML)\nwith open('/tmp/github_actions_demo/tests/test_performance.py', 'w') as f:\n    f.write(PERF_TEST)\n\nprint('Generated files:')\nprint('  .github/workflows/ml_ci.yml       -- GitHub Actions workflow')\nprint('  tests/test_performance.py          -- Performance regression test')\nprint()\nprint('The CI pipeline will:')\nprint('  1. Trigger on every push to main or PR')\nprint('  2. Install requirements, run unit tests, train model')\nprint('  3. FAIL if R^2 < 0.10 or MAE > $35,000')\nprint('  4. Upload the trained model as a downloadable artefact')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 10.7.2 -- Run the performance regression test locally (simulating CI)\n\nimport numpy as np\nfrom sklearn.metrics import r2_score, mean_absolute_error\n\n# Thresholds -- same as the CI environment variables\nMIN_R2  = 0.10\nMAX_MAE = 35_000\n\n# Evaluate the model trained in section 10.2\ny_pred_log = loaded_model.predict(X_test)\nr2         = r2_score(y_test, y_pred_log)\nmae        = mean_absolute_error(np.exp(y_test), np.exp(y_pred_log))\n\nprint('Performance regression test (simulated CI run):')\nprint(f'  Test R^2: {r2:.4f}  (threshold: >= {MIN_R2})')\nprint(f'  Test MAE: ${mae:,.0f}  (threshold: <= ${MAX_MAE:,.0f})')\nprint()\n\nr2_pass  = r2  >= MIN_R2\nmae_pass = mae <= MAX_MAE\n\nif r2_pass and mae_pass:\n    print('CI STATUS: PASSED -- model meets performance thresholds')\n    print('  In a real CI pipeline, this would allow the PR to merge.')\nelse:\n    print('CI STATUS: FAILED -- performance regression detected')\n    if not r2_pass:\n        print(f'  R^2 {r2:.4f} is below minimum {MIN_R2}')\n    if not mae_pass:\n        print(f'  MAE ${mae:,.0f} exceeds maximum ${MAX_MAE:,.0f}')\n    print('  In a real CI pipeline, this would block the merge.')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Concept Check Questions\n\n> Test your understanding before moving on. Answer each question without referring back to the notebook, then expand to check.\n\n**Q1.** What is the difference between **experiment tracking** and a **model registry**?\n\n<details><summary>Show answer</summary>\n\n**Experiment tracking** records hyperparameters, metrics, and artefacts from every training run â€” answers 'what did I try and what happened?'. **Model registry** is a store for approved model versions with lifecycle stages (Staging, Production, Archived) â€” answers 'which model is in production?'. Tracking is for exploration; the registry is for governance.\n\n</details>\n\n**Q2.** What does `mlflow.infer_signature` do and why is it important?\n\n<details><summary>Show answer</summary>\n\nIt inspects input and output arrays to record the expected schema â€” column names, dtypes, output shape. When the model is loaded for serving, MLflow validates incoming requests against this schema, rejecting malformed inputs before they reach the model. Prevents silent bugs from schema drift between training and serving.\n\n</details>\n\n**Q3.** What does PSI detect and what does PSI > 0.2 mean operationally?\n\n<details><summary>Show answer</summary>\n\nPSI measures distribution shift between a reference dataset (training) and current production data. PSI < 0.1: negligible. 0.1â€“0.2: monitor. > 0.2: significant â€” the model may no longer suit the current population and retraining should be considered.\n\n</details>\n\n**Q4.** What does a GitHub Actions CI/CD workflow do in ML, and what would cause it to block a PR?\n\n<details><summary>Show answer</summary>\n\nIt automatically installs dependencies, trains the model, and runs a performance regression test on every push or pull request. A PR is blocked if the model's performance falls below a defined threshold (e.g., RÂ² < 0.10 or MAE > $35k), preventing degraded code from reaching production.\n\n</details>\n\n**Q5.** A colleague says 'we retrain weekly and redeploy manually â€” no MLOps needed.' Name three specific risks.\n\n<details><summary>Show answer</summary>\n\n1. **Reproducibility failure** â€” no record of which code/data/hyperparameters produced the deployed model; can't reliably restore a good state. 2. **Silent degradation** â€” without drift monitoring, poor predictions can run for weeks undetected. 3. **Human error in deployment** â€” no automated performance gate means a regression can reach production undetected.\n\n</details>\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Coding Exercises\n\n> Three exercises per chapter: **ðŸ”§ Guided** (fill-in-the-blanks) Â· **ðŸ”¨ Applied** (write from scratch) Â· **ðŸ—ï¸ Extension** (go beyond the chapter)\n\nExercises use the SO 2025 developer survey dataset.\nExpand each **Solution** block only after attempting the exercise.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exercise 1 ðŸ”§ Guided â€” Complete an MLflow experiment tracking loop\n\nFill in the missing `mlflow` calls to properly log a GridSearchCV\nexperiment: parameters, the best score, and the trained model artifact.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import mlflow\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\nrng=np.random.default_rng(42); n=3000\nX=np.column_stack([rng.exponential(6,n),rng.integers(0,2,(n,4))])\ny=(np.exp(10.8+0.07*X[:,0]+rng.normal(0,0.4,n))>80000).astype(int)\nX_tr,X_te,y_tr,y_te=train_test_split(X,y,test_size=0.2,random_state=42)\n\nPARAM_GRID = {'max_depth': [2,3,5], 'n_estimators': [50,100]}\n\nmlflow.set_experiment('SO2025_salary_classifier')\nwith mlflow.start_run():\n    # YOUR CODE: log param_grid\n    gs = GridSearchCV(GradientBoostingClassifier(random_state=42),\n                       PARAM_GRID, cv=3, scoring='accuracy')\n    gs.fit(X_tr, y_tr)\n    # YOUR CODE: log best_params, best CV score, test accuracy\n    # YOUR CODE: log the best estimator as a model artifact\n    print('Best params:', gs.best_params_)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<details><summary>ðŸ’¡ Hint</summary>\n\n`mlflow.log_params(PARAM_GRID)` to log the search space.\n`mlflow.log_metrics({'best_cv_score': gs.best_score_, 'test_accuracy': acc})`\n`mlflow.sklearn.log_model(gs.best_estimator_, 'model')`\n\n</details>\n\n<details><summary>âœ… Solution</summary>\n\n```python\nwith mlflow.start_run():\n    mlflow.log_params(PARAM_GRID)\n    gs=GridSearchCV(GradientBoostingClassifier(random_state=42),PARAM_GRID,cv=3)\n    gs.fit(X_tr,y_tr)\n    acc=accuracy_score(y_te,gs.predict(X_te))\n    mlflow.log_metrics({'best_cv_score':gs.best_score_,'test_accuracy':acc})\n    mlflow.log_params(gs.best_params_)\n    mlflow.sklearn.log_model(gs.best_estimator_,'model')\n```\n\n</details>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exercise 2 ðŸ”¨ Applied â€” Build a model versioning system\n\nImplement a `ModelRegistry` class (without MLflow) that:\n1. Saves models with version numbers and metadata (training date, accuracy, features)\n2. Supports `promote(version, stage)` where stage âˆˆ {Staging, Production, Archived}\n3. Has `get_production_model()` that returns the current Production model\n4. Saves the registry state to JSON\n\nTrain three versions of the salary model and walk through a deployment workflow.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import json, joblib, datetime\nfrom pathlib import Path\n\nclass ModelRegistry:\n    def __init__(self, registry_path: str = '/tmp/model_registry') -> None:\n        self.path = Path(registry_path)\n        self.path.mkdir(exist_ok=True)\n        self.registry: dict = {}\n\n    def register(self, model, version: str, metadata: dict) -> None:\n        # YOUR CODE: save model file + update registry dict\n        pass\n\n    def promote(self, version: str, stage: str) -> None:\n        # YOUR CODE: update stage in registry\n        pass\n\n    def get_production_model(self):\n        # YOUR CODE: load and return Production model\n        pass\n\n    def save_registry(self) -> None:\n        with open(self.path/'registry.json','w') as f:\n            json.dump(self.registry, f, indent=2)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<details><summary>ðŸ’¡ Hint</summary>\n\n`register`: `joblib.dump(model, self.path/f'{version}.pkl')` then update dict.\n`promote`: set `self.registry[version]['stage'] = stage`.\n`get_production_model`: find version where `stage=='Production'`, `joblib.load` it.\n\n</details>\n\n<details><summary>âœ… Solution</summary>\n\n```python\ndef register(self, model, version, metadata):\n    joblib.dump(model, self.path/f'{version}.pkl')\n    self.registry[version]={'metadata':metadata,'stage':'None','registered':str(datetime.datetime.now())}\ndef promote(self, version, stage):\n    assert stage in ('Staging','Production','Archived')\n    self.registry[version]['stage']=stage\ndef get_production_model(self):\n    prod=[v for v,d in self.registry.items() if d['stage']=='Production']\n    if not prod: raise ValueError('No Production model')\n    return joblib.load(self.path/f'{prod[-1]}.pkl')\n```\n\n</details>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exercise 3 ðŸ—ï¸ Extension â€” Automated retraining trigger\n\nImplement a `RetrainingMonitor` that:\n1. Tracks PSI (Population Stability Index) for incoming data vs training data\n2. Triggers a retraining alert when PSI > 0.20 on any feature\n3. Maintains a rolling 30-day window of incoming data\n4. Generates a monitoring report with trend charts for each feature's PSI\n\nSimulate 90 days of data where `YearsCodePro` gradually drifts after day 45.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np, pandas as pd, matplotlib.pyplot as plt\nfrom scipy.stats import wasserstein_distance\n\ndef compute_psi(expected: np.ndarray, actual: np.ndarray, n_bins: int = 10) -> float:\n    \"\"\"Population Stability Index. PSI > 0.2 = significant shift.\"\"\"\n    bins = np.percentile(expected, np.linspace(0,100,n_bins+1))\n    bins[0] -= 1e-8; bins[-1] += 1e-8\n    e_pct = np.histogram(expected, bins=bins)[0] / len(expected) + 1e-8\n    a_pct = np.histogram(actual,   bins=bins)[0] / len(actual)   + 1e-8\n    return float(np.sum((a_pct - e_pct) * np.log(a_pct / e_pct)))\n\nclass RetrainingMonitor:\n    def __init__(self, training_data: pd.DataFrame,\n                 window_days: int = 30, psi_threshold: float = 0.20) -> None:\n        # YOUR CODE\n        pass\n\n    def add_day(self, day: int, new_data: pd.DataFrame) -> None:\n        # YOUR CODE: add data, compute PSI, check threshold\n        pass\n\n    def plot_psi_trend(self) -> None:\n        # YOUR CODE: plot PSI per feature per day\n        pass"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<details><summary>ðŸ’¡ Hint</summary>\n\nStore a deque of (day, data) tuples. When `add_day` is called:\n1. Append new data to the deque\n2. Drop data older than `window_days`\n3. Compute PSI for each feature using rolling window vs training data\n4. Print alert if PSI > threshold\n\n</details>\n\n<details><summary>âœ… Solution</summary>\n\n```python\nfrom collections import deque\nclass RetrainingMonitor:\n    def __init__(self, training_data, window_days=30, psi_threshold=0.20):\n        self.train=training_data; self.window=window_days\n        self.threshold=psi_threshold; self.history=deque()\n        self.psi_log={col:[] for col in training_data.columns}\n    def add_day(self,day,new_data):\n        self.history.append((day,new_data))\n        while self.history[0][0]<day-self.window: self.history.popleft()\n        window_df=pd.concat([d for _,d in self.history])\n        for col in self.train.columns:\n            psi=compute_psi(self.train[col].values,window_df[col].values)\n            self.psi_log[col].append(psi)\n            if psi>self.threshold: print(f'Day {day}: {col} PSI={psi:.3f} > threshold')\n```\n\n</details>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Chapter 12 Summary\n\n### Key Takeaways\n\n- **MLflow** is the standard open-source experiment tracker. Log params and\n  metrics with `mlflow.log_param()` / `mlflow.log_metric()`. Always log inside\n  a `with mlflow.start_run():` context so runs are cleanly scoped.\n- **`infer_signature`** captures the input/output schema of your model.\n  This prevents silent failures when the serving environment has different\n  column names or dtypes.\n- **The Model Registry** decouples training from deployment. Data scientists\n  push to Staging; ML engineers promote to Production. The audit trail\n  shows who approved each version and when.\n- **FastAPI** wraps models as REST APIs with automatic validation (Pydantic)\n  and auto-generated docs at `/docs`. Load the model once at startup,\n  not on every request.\n- **ML unit tests** focus on data contracts, boundary conditions, and\n  regression baselines â€” not exact output values. Run them in CI on\n  every commit that touches training code.\n- **PSI > 0.2** is the standard threshold for triggering a retrain.\n- **GitHub Actions** automates the full ML CI pipeline on every push:\n  install â†’ test â†’ train â†’ performance check â†’ artefact upload.\n- **Performance regression tests** prevent degraded models from reaching\n  production silently. Monitor every feature the model uses, not just the target.\n\n### Project Thread â€” Chapter 12\n\n| Task | Status |\n|------|--------|\n| Three model variants tracked in MLflow | âœ… Done |\n| Best model registered and staged | âœ… Done |\n| FastAPI prediction endpoint written | âœ… Done |\n| Six ML unit tests written and passing | âœ… Done |\n| Drift detection with PSI and KS test | âœ… Done |\n| GitHub Actions CI/CD workflow | âœ… Generated |\n| Performance regression test | âœ… Passing |\n\n---\n\n### You've reached the end of the main chapters.\n\nThe appendices cover reference material you can dip into as needed:\n\n- **Appendix A** â€” Local Python environment setup (conda, venv, VS Code)\n- **Appendix B** â€” Keras 3 companion (TensorFlow/JAX backends)\n- **Appendix C** â€” Project ideas and further reading\n- **Appendix D** â€” Reinforcement learning: Q-learning and DQN on CartPole\n- **Appendix E** â€” SQL for data scientists: sqlite3, window functions\n- **Appendix F** â€” Git and GitHub for ML: branching, nbstripout, DVC\n- **Appendix G** â€” Docker and containerisation for ML\n- **Appendix H** â€” MLSecOps: securing the ML pipeline\n- **Appendix I** â€” Failure modes and troubleshooting guide\n\n---\n\n*End of Chapter 12 â€” Python for AI/ML*  \n[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n"
    }
  ]
}