{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Appendix F -- Git and GitHub for ML Projects\n## *Python for AI/ML: A Complete Learning Journey*\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/APP_F_Git_GitHub_for_ML.ipynb)\n&nbsp;&nbsp;[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n\n---\n\n**Prerequisites:** None  \n\n### Learning Objectives\n\n- Understand Git's core model: commits, branches, and the working tree\n- Initialise a repo, stage files, commit, and push to GitHub\n- Use branches for experiments and merge them back\n- Write a `.gitignore` that keeps notebooks clean and repos lean\n- Use `nbstripout` to remove cell outputs before committing notebooks\n- Version large datasets with DVC (Data Version Control)\n- Structure an ML project repository for collaboration\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Setup\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import subprocess\nresult = subprocess.run(['git', '--version'], capture_output=True, text=True)\nprint(result.stdout.strip())\n\n# Check if nbstripout is available\nresult2 = subprocess.run(['pip', 'show', 'nbstripout'], capture_output=True, text=True)\nif 'not found' in result2.stderr or not result2.stdout:\n    subprocess.run(['pip', 'install', 'nbstripout', '-q'], check=False)\n    print('nbstripout installed')\nelse:\n    print('nbstripout already available')\n\nprint('\\nGit is available. Run the bash cells below in a terminal')\nprint('or prefix with ! to run in Colab.')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## F.1 -- Core Git Workflow\n\nGit tracks changes to files as a series of **commits**.\nEach commit is a snapshot of the entire project at a point in time.\n\n### The three areas\n\n```\n  Working tree   -->  Staging area  -->  Repository\n  (your files)       (git add)          (git commit)\n```\n\n### Essential commands\n\n```bash\n# Initialise a new repository\ngit init my-ml-project\ncd my-ml-project\n\n# Check status at any time\ngit status\n\n# Stage files for commit\ngit add notebook.ipynb\ngit add .                    # stage everything\n\n# Commit with a message\ngit commit -m 'Add salary regression baseline'\n\n# View history\ngit log --oneline\n\n# Connect to GitHub and push\ngit remote add origin https://github.com/username/repo.git\ngit push -u origin main\n```\n\n### Branching for experiments\n\n```bash\n# Create a branch for a new experiment\ngit checkout -b experiment/gradient-boosting\n\n# ... make changes, commit ...\n\n# Merge back to main when satisfied\ngit checkout main\ngit merge experiment/gradient-boosting\n\n# Delete the branch when done\ngit branch -d experiment/gradient-boosting\n```\n\nBranches are the correct way to manage parallel ML experiments.\nOne branch per experiment means you can always return to a working baseline.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# F.1.1 -- Demonstrate git operations programmatically\n\nimport subprocess\nimport os\nimport tempfile\n\n# Create a temporary project to demonstrate git\ntmpdir = tempfile.mkdtemp()\nos.chdir(tmpdir)\n\ndef git(cmd, check=True):\n    result = subprocess.run(\n        f'git {cmd}', shell=True, capture_output=True, text=True\n    )\n    return result.stdout.strip() or result.stderr.strip()\n\n# Initialise repo\nprint(git('init'))\nprint(git('config user.email \"demo@example.com\"'))\nprint(git('config user.name \"Demo User\"'))\n\n# Create a file and commit it\nwith open('train.py', 'w') as f:\n    f.write('# Salary regression baseline\\nmodel = \\'ridge\\'\\n')\n\nprint(git('add train.py'))\nprint(git('commit -m \"Add baseline model\"'))\n\n# Create an experiment branch\nprint(git('checkout -b experiment/random-forest'))\n\nwith open('train.py', 'w') as f:\n    f.write('# Salary regression\\nmodel = \\'random_forest\\'\\n')\n\nprint(git('add train.py'))\nprint(git('commit -m \"Try random forest\"'))\n\n# Show log\nprint()\nprint('Git log (all branches):')\nprint(git('log --oneline --all --graph'))\n\n# Merge back\nprint(git('checkout main'))\nprint(git('merge experiment/random-forest --no-ff -m \"Merge random forest experiment\"'))\nprint()\nprint('After merge:')\nprint(git('log --oneline --all --graph'))\n\nos.chdir('/')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## F.2 -- .gitignore for ML Projects\n\nCertain files should never be committed: large data files, model weights,\nsecrets, and Python cache files. A `.gitignore` tells Git to ignore them.\n\n```gitignore\n# Python\n__pycache__/\n*.py[cod]\n.env\nvenv/\n.venv/\n\n# Jupyter\n*.ipynb_checkpoints\n.ipynb_checkpoints/\n\n# Data files -- track with DVC instead\ndata/raw/\n*.csv\n*.parquet\n*.json\n\n# Model artefacts -- often too large for GitHub\nmodels/\n*.pt\n*.pkl\n*.joblib\n\n# MLflow\nmlruns/\nmlflow.db\n\n# OS\n.DS_Store\nThumbs.db\n```\n\n## F.3 -- Notebook Version Control with nbstripout\n\nJupyter notebooks store cell outputs (images, tables, print output) inside\nthe `.ipynb` JSON. This causes two problems for version control:\n\n1. Large diffs: a single matplotlib figure adds thousands of lines\n2. Non-reproducible state: committed outputs may not reflect the actual code\n\n`nbstripout` automatically strips outputs before every commit:\n\n```bash\n# Install once per repo\npip install nbstripout\nnbstripout --install       # adds a pre-commit Git hook\n\n# Now every 'git add notebook.ipynb' automatically strips outputs\n# The outputs are visible when you run the notebook; they just aren't committed\n```\n\n## F.4 -- Recommended ML Repository Structure\n\n```\nmy-ml-project/\n├── data/\n│   ├── raw/          # original, immutable data (tracked with DVC)\n│   └── processed/    # cleaned data (tracked with DVC)\n├── notebooks/\n│   ├── 01_eda.ipynb\n│   ├── 02_modelling.ipynb\n│   └── 03_evaluation.ipynb\n├── src/\n│   ├── data.py       # data loading and cleaning functions\n│   ├── features.py   # feature engineering\n│   ├── models.py     # model definitions\n│   └── evaluate.py   # evaluation metrics\n├── tests/\n│   ├── test_data.py\n│   └── test_models.py\n├── models/           # saved model artefacts (gitignored, DVC-tracked)\n├── .dvc/             # DVC config\n├── .gitignore\n├── requirements.txt\n├── README.md\n└── Makefile          # common commands: make train, make test, make clean\n```\n\n## F.5 -- Dataset Versioning with DVC\n\nGitHub has a 100MB file size limit. Large datasets and model weights need\na different tool. **DVC (Data Version Control)** tracks data files with\nsmall pointer files (`.dvc`) that are committed to Git, while the actual\ndata is stored in a remote (S3, GCS, Azure, or even Google Drive).\n\n```bash\npip install dvc\n\n# Initialise DVC in an existing Git repo\ndvc init\n\n# Add a data file to DVC tracking\ndvc add data/raw/so_survey_2025.csv\n# This creates data/raw/so_survey_2025.csv.dvc -- commit this to Git\ngit add data/raw/so_survey_2025.csv.dvc .gitignore\ngit commit -m 'Track SO 2025 dataset with DVC'\n\n# Configure remote storage (example: Google Drive)\ndvc remote add -d gdrive gdrive://YOUR_FOLDER_ID\ndvc push   # upload data to remote\n\n# On another machine: pull the data\ndvc pull\n```\n\n---\n\n*End of Appendix F -- Python for AI/ML*  \n[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n"
    }
  ]
}