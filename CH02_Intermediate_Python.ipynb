{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Chapter 2 -- Intermediate Python\n## *Python for AI/ML: A Complete Learning Journey*\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/CH02_Intermediate_Python.ipynb)\n&nbsp;&nbsp;[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n\n---\n\n**Part:** 1 -- Core Python Fundamentals  \n**Prerequisites:** Chapter 1 (Python Fundamentals)  \n**Estimated time:** 5-6 hours\n\n---\n\n### Learning Objectives\n\nBy the end of this chapter you will be able to:\n\n- Import and use Python's standard library modules and third-party packages\n- Read and write files safely using the `with` statement\n- Load and save CSV and JSON data without Pandas\n- Handle errors gracefully with `try`/`except`/`else`/`finally`\n- Define classes with attributes, methods, and `__dunder__` methods\n- Use inheritance to build specialised subclasses\n- Recognise how OOP maps directly to scikit-learn and Keras APIs\n- Write list, dict, and set comprehensions\n- Use generators to process large data without loading it all into memory\n- Write and apply decorators\n- Use context managers with `with`\n\n---\n\n### Project Thread -- Chapter 2\n\nWe build a `SurveyLoader` class that wraps the SO 2025 CSV loading, validation,\nand basic cleaning into a reusable object. By the end of this chapter you will have\na working data-loader class -- the same architectural pattern used by PyTorch's\n`DataLoader`, scikit-learn's `Pipeline`, and every professional ML codebase.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 2.1 -- Modules and Packages\n\nA **module** is any Python file (`.py`). A **package** is a directory of modules.\nThe `import` statement makes a module's contents available in your current session.\n\nPython ships with an enormous **standard library** -- modules for file handling,\nmath, dates, networking, compression, and hundreds of other tasks. These are always\navailable with no installation. Third-party packages (NumPy, Pandas, scikit-learn)\nare installed separately with `pip`.\n\n> **Why this matters for AI/ML:** You will write `import` as your first line in\n> every notebook for the rest of this book. Understanding what importing actually\n> does makes debugging import errors and version conflicts much easier.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 2.1.1 -- Import styles\n#\n# There are four main ways to import:\n#   import module              -- access as module.thing\n#   import module as alias     -- access as alias.thing (universal convention)\n#   from module import name    -- access as name directly\n#   from module import *       -- import everything (avoid -- pollutes namespace)\n\n# Style 1: import the whole module\nimport math\nprint(f'pi = {math.pi:.6f}')\nprint(f'sqrt(144) = {math.sqrt(144)}')\nprint(f'log2(1024) = {math.log2(1024)}')\n\n# Style 2: import with alias -- this IS the convention for data science\nimport os\nimport sys\nprint(f'Python: {sys.version.split()[0]}')\nprint(f'Working dir: {os.getcwd()}')\n\n# Style 3: import specific names -- useful for commonly-used functions\nfrom datetime import datetime, timedelta\nnow = datetime.now()\nprint(f'Current time: {now.strftime(\"%Y-%m-%d %H:%M\")}')\nprint(f'One week ago: {(now - timedelta(days=7)).strftime(\"%Y-%m-%d\")}')\n\n# Checking what is inside a module with dir()\nmath_contents = [name for name in dir(math) if not name.startswith('_')]\nprint(f'Math module exports: {math_contents[:8]}...')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 2.1.2 -- Standard library modules you will use in this book\n\n# os -- operating system interface\nimport os\nprint('--- os ---')\nprint(f'  os.getcwd():          {os.getcwd()}')\nprint(f'  os.path.exists():     {os.path.exists(\"/tmp\")}')\nprint(f'  os.path.join():       {os.path.join(\"data\", \"survey\", \"file.csv\")}')\nprint(f'  os.path.basename():   {os.path.basename(\"/data/survey/file.csv\")}')\n\n# collections -- specialised container types\nfrom collections import Counter, defaultdict, OrderedDict\nprint()\nprint('--- collections ---')\nlanguages = ['Python', 'SQL', 'Python', 'JavaScript', 'Python', 'SQL', 'Rust']\ncounts = Counter(languages)         # counts occurrences automatically\nprint(f'  Counter: {dict(counts)}')\nprint(f'  Most common 3: {counts.most_common(3)}')\n\n# defaultdict: like dict but auto-creates missing keys with a default value\nsalary_by_country = defaultdict(list)   # missing keys return empty list\ndata = [('USA', 135_000), ('UK', 88_000), ('USA', 210_000), ('UK', 72_000)]\nfor country, sal in data:\n    salary_by_country[country].append(sal)   # no KeyError if country is new\nfor country, sals in salary_by_country.items():\n    print(f'  {country}: avg ${sum(sals)/len(sals):,.0f}')\n\n# itertools -- tools for efficient looping\nimport itertools\nprint()\nprint('--- itertools ---')\npairs = list(itertools.combinations(['Python', 'SQL', 'JS'], 2))\nprint(f'  Language pairs: {pairs}')\n\n# random -- random number generation (we used this in Chapter 1)\nimport random\nrandom.seed(42)\nsample_idx = random.sample(range(15_000), 5)   # 5 random row indices from 15k\nprint(f'  5 random indices: {sample_idx}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 2.2 -- File I/O: Reading and Writing Files\n\nFile I/O (Input/Output) is how Python reads from and writes to files on disk.\nIn data science you will constantly read CSVs, write processed outputs, save\nmodel results, and load configuration files.\n\nPython's `with` statement is the safe way to open files -- it guarantees the file\nis closed properly even if an error occurs mid-read. **Never use `open()` without\n`with` in production code.**\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 2.2.1 -- Reading and writing plain text files\n\nimport os\n\n# Write a small text file -- 'w' mode creates or overwrites\nfile_path = '/tmp/survey_notes.txt'\n\n# The 'with' statement opens the file, gives us the handle 'f',\n# and automatically closes it when the block exits -- even on error.\nwith open(file_path, 'w', encoding='utf-8') as f:\n    f.write('SO 2025 Developer Survey -- Analysis Notes\\n')\n    f.write('=' * 45 + '\\n')\n    f.write('Dataset: 15,000 respondents (curated subset)\\n')\n    f.write('Primary target: ConvertedCompYearly (salary USD)\\n')\n\nprint(f'Written: {file_path}  ({os.path.getsize(file_path)} bytes)')\n\n# Read it back -- 'r' mode (default) opens for reading\nwith open(file_path, 'r', encoding='utf-8') as f:\n    content = f.read()   # read entire file as one string\nprint('File contents:')\nprint(content)\n\n# Append to an existing file -- 'a' mode adds to the end\nwith open(file_path, 'a', encoding='utf-8') as f:\n    f.write('Added: additional note on second open\\n')\n\n# Read line by line -- efficient for large files (one line in memory at a time)\nprint('Line by line:')\nwith open(file_path, 'r', encoding='utf-8') as f:\n    for line_num, line in enumerate(f, start=1):\n        print(f'  Line {line_num}: {line.rstrip()}')   # .rstrip() removes trailing newline\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 2.2.2 -- CSV files with the built-in csv module\n#\n# CSV (Comma-Separated Values) is the most common format for tabular data.\n# Python's csv module handles quoting, escaping, and dialects correctly.\n# (In Chapter 3 we switch to Pandas -- but knowing the raw approach matters.)\n\nimport csv\n\ncsv_path = '/tmp/sample_respondents.csv'\n\n# Writing CSV\nrespondents = [\n    {'id': 'R001', 'country': 'USA',     'salary': 135000, 'language': 'Python'},\n    {'id': 'R002', 'country': 'UK',      'salary':  88000, 'language': 'JavaScript'},\n    {'id': 'R003', 'country': 'Germany', 'salary': 105000, 'language': 'Python'},\n    {'id': 'R004', 'country': 'India',   'salary':  42000, 'language': 'Java'},\n    {'id': 'R005', 'country': 'Canada',  'salary': 118000, 'language': 'Python'},\n]\n\nfieldnames = ['id', 'country', 'salary', 'language']\n\nwith open(csv_path, 'w', newline='', encoding='utf-8') as f:\n    # DictWriter writes dicts as rows, using fieldnames as the header\n    writer = csv.DictWriter(f, fieldnames=fieldnames)\n    writer.writeheader()       # writes the column name row\n    writer.writerows(respondents)   # writes all data rows\n\nprint(f'CSV written: {csv_path}')\n\n# Reading CSV\nwith open(csv_path, 'r', encoding='utf-8') as f:\n    reader = csv.DictReader(f)   # each row becomes a dict keyed by column name\n    loaded = list(reader)        # materialise into a list of dicts\n\nprint(f'Rows loaded: {len(loaded)}')\nfor row in loaded:\n    print(f'  {row[\"id\"]}  {row[\"country\"]:<10}  ${int(row[\"salary\"]):,}  {row[\"language\"]}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 2.2.3 -- JSON files\n#\n# JSON (JavaScript Object Notation) is the standard for API responses,\n# configuration files, and nested data structures.\n# Python's json module maps JSON directly to Python dicts and lists.\n\nimport json\n\njson_path = '/tmp/survey_config.json'\n\n# Any Python object that is JSON-serialisable can be written\nconfig = {\n    'dataset': 'so_survey_2025_curated.csv',\n    'target_column': 'ConvertedCompYearly',\n    'feature_columns': ['YearsCodePro', 'Country', 'EdLevel', 'RemoteWork'],\n    'salary_filter': {'min': 10_000, 'max': 500_000},\n    'random_seed': 42,\n    'test_size': 0.2,\n}\n\n# json.dump() writes to a file  (json.dumps() returns a string)\nwith open(json_path, 'w', encoding='utf-8') as f:\n    json.dump(config, f, indent=2)   # indent=2 makes it human-readable\n\nprint(f'JSON written: {json_path}')\n\n# json.load() reads from a file  (json.loads() parses a string)\nwith open(json_path, 'r', encoding='utf-8') as f:\n    loaded_config = json.load(f)\n\nprint('Loaded config:')\nfor key, value in loaded_config.items():\n    print(f'  {key:<20}: {value}')\n\n# Round-trip verification\nassert loaded_config['random_seed'] == 42\nassert loaded_config['salary_filter']['min'] == 10_000\nprint('Round-trip verification passed.')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2.2.4 -- Project: Loading the SO 2025 CSV with Error Handling\n\nWe load the SO 2025 dataset from GitHub using `urllib` and `csv` -- the same approach\nas Chapter 1, but now wrapped in proper error handling so the loader fails gracefully\nrather than crashing on network issues, malformed rows, or missing columns.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 2.2.4 -- Robust CSV loader with error handling\n\nimport csv\nimport urllib.request\n\nDATASET_URL = 'https://raw.githubusercontent.com/timothy-watt/python-for-ai-ml/main/data/so_survey_2025_curated.csv'\n\ndef load_so_survey(url, max_rows=50, required_cols=None):\n    \"\"\"\n    Load SO 2025 survey data from a URL into a list of dicts.\n\n    Parameters\n    ----------\n    url : str\n        Raw CSV URL to fetch.\n    max_rows : int\n        Maximum number of valid rows to return.\n    required_cols : list of str, optional\n        Columns that must be non-empty for a row to be kept.\n\n    Returns\n    -------\n    list of dict, or None if the fetch failed.\n    \"\"\"\n    if required_cols is None:\n        required_cols = ['ConvertedCompYearly', 'Country']\n\n    try:\n        with urllib.request.urlopen(url, timeout=15) as response:\n            raw = response.read().decode('utf-8')\n    except urllib.error.URLError as e:\n        print(f'Network error: {e.reason}')\n        return None\n    except Exception as e:\n        print(f'Unexpected fetch error: {e}')\n        return None\n\n    rows = []\n    skipped = 0\n    reader = csv.DictReader(raw.splitlines())\n\n    for row in reader:\n        # Skip rows missing any required column\n        if any(not row.get(col, '').strip() for col in required_cols):\n            skipped += 1\n            continue\n\n        # Convert salary to float safely\n        try:\n            row['ConvertedCompYearly'] = float(row['ConvertedCompYearly'])\n        except (ValueError, KeyError):\n            skipped += 1\n            continue\n\n        rows.append(dict(row))\n        if len(rows) >= max_rows:\n            break\n\n    print(f'Loaded {len(rows)} rows  (skipped {skipped} incomplete rows)')\n    return rows\n\n\nsurvey_data = load_so_survey(DATASET_URL, max_rows=50)\n\nif survey_data:\n    salaries = [r['ConvertedCompYearly'] for r in survey_data]\n    countries = set(r['Country'] for r in survey_data)\n    print(f'Countries represented: {len(countries)}')\n    print(f'Salary range: ${min(salaries):,.0f} to ${max(salaries):,.0f}')\n    print(f'Mean salary:  ${sum(salaries)/len(salaries):,.0f}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 2.3 -- Error and Exception Handling\n\nErrors are inevitable in data science code -- missing files, malformed data,\ntype mismatches, network timeouts. Python's exception handling lets you respond\nto errors gracefully rather than crashing.\n\nThe full syntax is:\n```python\ntry:\n    # code that might raise an exception\nexcept SomeError as e:\n    # runs if SomeError (or a subclass) was raised\nexcept AnotherError:\n    # multiple except blocks handle different error types\nelse:\n    # runs only if NO exception was raised in the try block\nfinally:\n    # ALWAYS runs -- use for cleanup (closing files, releasing resources)\n```\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 2.3.1 -- Common exception types in data science code\n\nprint('--- ValueError: wrong type of value ---')\ntry:\n    salary = float('not_a_number')   # csv column that should be numeric\nexcept ValueError as e:\n    print(f'  Caught ValueError: {e}')\n\nprint()\nprint('--- KeyError: missing dict key ---')\nrow = {'Country': 'USA', 'YearsCodePro': '7'}\ntry:\n    salary = row['ConvertedCompYearly']   # column absent from this row\nexcept KeyError as e:\n    print(f'  Caught KeyError: {e}')\n    salary = 0.0   # use a default instead of crashing\n\nprint()\nprint('--- IndexError: list index out of range ---')\nresults = [0.91, 0.93, 0.89]\ntry:\n    fifth_result = results[10]   # only 3 items\nexcept IndexError as e:\n    print(f'  Caught IndexError: {e}')\n\nprint()\nprint('--- TypeError: wrong type for operation ---')\ntry:\n    total = '135000' + 88000   # string + int not valid\nexcept TypeError as e:\n    print(f'  Caught TypeError: {e}')\n    total = int('135000') + 88000   # fix by converting first\n    print(f'  Fixed: {total:,}')\n\nprint()\nprint('--- ZeroDivisionError ---')\ntry:\n    avg = sum([]) / 0   # empty list -- dividing by zero\nexcept ZeroDivisionError:\n    print('  Caught ZeroDivisionError: no data to average')\n    avg = 0.0\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 2.3.2 -- try / except / else / finally in full\n\ndef safe_average(values, label=''):\n    \"\"\"\n    Compute the mean of a list, handling all common failure cases.\n    Demonstrates the full try/except/else/finally pattern.\n    \"\"\"\n    try:\n        # This block runs first -- the 'risky' code\n        numeric = [float(v) for v in values if str(v).strip()]   # convert all to float\n        result  = sum(numeric) / len(numeric)   # ZeroDivisionError if numeric is empty\n\n    except ValueError as e:\n        # Runs if any value could not be converted to float\n        print(f'  [{label}] Value error: {e} -- returning None')\n        return None\n\n    except ZeroDivisionError:\n        # Runs if the list was empty after filtering\n        print(f'  [{label}] No valid values -- returning None')\n        return None\n\n    except Exception as e:\n        # Catch-all for anything unexpected -- log it, do not swallow silently\n        print(f'  [{label}] Unexpected error: {type(e).__name__}: {e}')\n        return None\n\n    else:\n        # Runs ONLY if the try block succeeded with no exceptions\n        print(f'  [{label}] Success: mean = {result:,.2f} (n={len(numeric)})')\n\n    finally:\n        # ALWAYS runs -- even if an exception was raised and caught\n        # Use for cleanup: closing files, releasing locks, logging\n        pass   # nothing to clean up here\n\n    return result\n\n\n# Test all paths\nsafe_average([95_000, 135_000, 88_000, 210_000], label='valid data')\nsafe_average(['95000', 'not_a_number', '88000'],  label='bad value')\nsafe_average([],                                   label='empty list')\nsafe_average([None, '', '  '],                     label='all empty strings')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 2.3.3 -- Raising your own exceptions\n#\n# Sometimes YOU want to signal that something has gone wrong.\n# Raise built-in exceptions for standard situations;\n# define custom exceptions for domain-specific errors.\n\nclass SalaryValidationError(ValueError):\n    \"\"\"Raised when a salary value fails business validation rules.\"\"\"\n    pass   # inherits everything from ValueError; we just gave it a new name\n\n\ndef validate_salary(salary, label=''):\n    \"\"\"Validate that a salary is plausible. Raises SalaryValidationError if not.\"\"\"\n    if not isinstance(salary, (int, float)):\n        raise TypeError(f'Salary must be numeric, got {type(salary).__name__}')\n    if salary < 0:\n        raise SalaryValidationError(f'Negative salary not valid: {salary}')\n    if salary < 1_000:\n        raise SalaryValidationError(f'Salary suspiciously low (< $1k): {salary}')\n    if salary > 10_000_000:\n        raise SalaryValidationError(f'Salary suspiciously high (> $10M): {salary}')\n    return salary\n\n\ntest_cases = [135_000, -5_000, 500, 15_000_000, 'eighty thousand']\n\nfor val in test_cases:\n    try:\n        result = validate_salary(val)\n        print(f'  ${result:>12,.0f}  -- valid')\n    except SalaryValidationError as e:\n        print(f'  {str(val):>14}  -- SalaryValidationError: {e}')\n    except TypeError as e:\n        print(f'  {str(val):>14}  -- TypeError: {e}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 2.4 -- Object-Oriented Programming\n\nObject-Oriented Programming (OOP) organises code around **objects** -- bundles\nof data (attributes) and behaviour (methods). A **class** is the blueprint;\nan **instance** is one concrete object built from that blueprint.\n\nOOP is the reason scikit-learn and Keras look the way they do:\n\n```python\nscaler = StandardScaler()   # create an instance\nscaler.fit(X_train)         # call a method that stores learned parameters\nX_scaled = scaler.transform(X_test)  # call a method that uses those parameters\n```\n\nAfter this section you will understand exactly why that pattern works -- and\nbe able to build your own classes that follow the same design.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 2.4.1 -- Defining a class with __init__ and methods\n\nclass SurveyRespondent:\n    \"\"\"\n    Represents one SO 2025 Developer Survey respondent.\n\n    Attributes\n    ----------\n    respondent_id : str\n    country : str\n    salary : float  -- annual compensation in USD\n    languages : list of str\n    years_exp : int or None\n    \"\"\"\n\n    # Class variable: shared by ALL instances\n    # Changed here, it changes for every SurveyRespondent object.\n    survey_year = 2025\n\n    def __init__(self, respondent_id, country, salary, languages=None, years_exp=None):\n        \"\"\"\n        Initialiser -- called automatically when you write SurveyRespondent(...).\n        'self' refers to the specific instance being created.\n        \"\"\"\n        # Instance variables: unique to each object\n        self.respondent_id = respondent_id\n        self.country       = country\n        self.salary        = float(salary)\n        self.languages     = languages or []   # default to empty list if None passed\n        self.years_exp     = years_exp\n\n    def salary_band(self):\n        \"\"\"Return a salary band label based on annual compensation.\"\"\"\n        if self.salary >= 200_000: return 'Principal'\n        if self.salary >= 150_000: return 'Senior+'\n        if self.salary >= 100_000: return 'Senior'\n        if self.salary >=  60_000: return 'Mid-level'\n        return 'Junior'\n\n    def uses_python(self):\n        \"\"\"Return True if Python is in this respondent's language list.\"\"\"\n        return 'Python' in self.languages\n\n    def summary(self):\n        \"\"\"Return a formatted one-line summary string.\"\"\"\n        langs = ', '.join(self.languages[:3])   # show up to 3 languages\n        if len(self.languages) > 3:\n            langs += f' (+{len(self.languages)-3} more)'\n        return (f'[{self.respondent_id}] {self.country} | '\n                f'${self.salary:,.0f} ({self.salary_band()}) | '\n                f'{langs}')\n\n    # __repr__ is called when you print the object or inspect it in a REPL\n    def __repr__(self):\n        return f'SurveyRespondent(id={self.respondent_id!r}, country={self.country!r}, salary={self.salary:.0f})'\n\n    # __len__ lets len(respondent) work -- returns number of languages\n    def __len__(self):\n        return len(self.languages)\n\n\n# Create instances\nr1 = SurveyRespondent('R001', 'USA',     135_000, ['Python', 'SQL', 'JavaScript'], 7)\nr2 = SurveyRespondent('R002', 'UK',       88_000, ['JavaScript', 'TypeScript'],    3)\nr3 = SurveyRespondent('R003', 'Germany', 105_000, ['Python', 'SQL', 'Rust', 'Go'], 9)\n\nprint(r1)                      # calls __repr__\nprint(r1.summary())\nprint(r2.summary())\nprint(r3.summary())\nprint()\nprint(f'r1 uses Python:   {r1.uses_python()}')\nprint(f'r2 uses Python:   {r2.uses_python()}')\nprint(f'len(r1) [langs]:  {len(r1)}')\nprint(f'Survey year:      {SurveyRespondent.survey_year}')   # class variable\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 2.4.2 -- Dunder (magic) methods\n#\n# Dunder methods (double-underscore on both sides) let your class integrate\n# with Python's built-in syntax and functions:\n#   __str__      called by str() and print()\n#   __repr__     called in REPL and for debugging\n#   __eq__       called by ==\n#   __lt__       called by < (and sorted())\n#   __len__      called by len()\n#   __contains__ called by 'in'\n\nclass SalaryRecord:\n    \"\"\"A comparable salary record -- demonstrates sorting and equality via dunders.\"\"\"\n\n    def __init__(self, respondent_id, salary):\n        self.respondent_id = respondent_id\n        self.salary = salary\n\n    def __repr__(self):\n        return f'SalaryRecord({self.respondent_id!r}, ${self.salary:,.0f})'\n\n    def __str__(self):\n        return f'{self.respondent_id}: ${self.salary:,.0f}'\n\n    def __eq__(self, other):\n        # == compares salary values, not object identity\n        if not isinstance(other, SalaryRecord):\n            return NotImplemented\n        return self.salary == other.salary\n\n    def __lt__(self, other):\n        # < enables sorted() to work on a list of SalaryRecords\n        return self.salary < other.salary\n\n    def __add__(self, other):\n        # + operator -- returns a new SalaryRecord with combined salary (for demo)\n        return SalaryRecord(f'{self.respondent_id}+{other.respondent_id}',\n                            self.salary + other.salary)\n\n\nrecords = [\n    SalaryRecord('R001', 135_000),\n    SalaryRecord('R002',  88_000),\n    SalaryRecord('R003', 210_000),\n    SalaryRecord('R004',  72_000),\n]\n\nprint('Unsorted:', records)\nprint('Sorted:  ', sorted(records))         # uses __lt__\nprint('Max:     ', max(records))             # uses __lt__\nprint('Equal?   ', records[0] == SalaryRecord('X', 135_000))  # uses __eq__\ncombined = records[0] + records[1]           # uses __add__\nprint(f'Combined: {combined}')               # uses __str__\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 2.4.3 -- Inheritance\n#\n# Inheritance lets a 'child' class reuse and extend a 'parent' class.\n# The child gets all the parent's methods automatically, and can:\n#   - Add new methods\n#   - Override existing methods with new behaviour\n#   - Call the parent's version with super()\n\nclass SeniorRespondent(SurveyRespondent):\n    \"\"\"\n    A SurveyRespondent who is senior (>= 5 years experience).\n    Inherits everything from SurveyRespondent and adds mentorship tracking.\n    \"\"\"\n\n    def __init__(self, respondent_id, country, salary, languages=None,\n                 years_exp=None, mentors=0):\n        # super().__init__() calls the parent class's __init__\n        # so we do not duplicate that code here\n        super().__init__(respondent_id, country, salary, languages, years_exp)\n        self.mentors = mentors   # new attribute specific to SeniorRespondent\n\n    def summary(self):\n        # Override the parent's summary() method with a more detailed version\n        base = super().summary()   # get the parent's summary string\n        return f'{base} | mentors={self.mentors}'\n\n    def is_eligible_principal(self):\n        \"\"\"Senior respondent is eligible for Principal if exp >= 10 and salary >= 180k.\"\"\"\n        exp = self.years_exp or 0\n        return exp >= 10 and self.salary >= 180_000\n\n\nsenior = SeniorRespondent('R010', 'USA', 195_000,\n                          languages=['Python', 'Scala', 'SQL'],\n                          years_exp=12, mentors=3)\n\nprint(senior.summary())                         # uses overridden summary()\nprint(f'Uses Python:         {senior.uses_python()}')    # inherited from parent\nprint(f'Salary band:         {senior.salary_band()}')    # inherited from parent\nprint(f'Principal eligible:  {senior.is_eligible_principal()}')  # new method\nprint()\n# isinstance() checks the full inheritance chain\nprint(f'Is SeniorRespondent:  {isinstance(senior, SeniorRespondent)}')\nprint(f'Is SurveyRespondent:  {isinstance(senior, SurveyRespondent)}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2.4.4 -- Bridge: How OOP Maps to scikit-learn and Keras\n\nThis is one of the most important conceptual bridges in the book.\nEvery scikit-learn transformer and estimator follows *exactly* the pattern\nyou just learned. Once you see this, the entire scikit-learn and Keras APIs\nbecome predictable and easy to read.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 2.4.4 -- The fit/transform pattern: OOP in disguise\n#\n# scikit-learn's API is built entirely on two ideas from OOP:\n#   1. __init__ sets configuration (hyperparameters)\n#   2. fit() learns from data and stores results as instance attributes\n#   3. transform() / predict() uses those stored results\n#\n# Here we build a MinMaxScaler from scratch using pure Python OOP.\n# Then we show the identical interface from scikit-learn.\n\nclass MinMaxScaler:\n    \"\"\"\n    Scales values to [0, 1] by subtracting the min and dividing by the range.\n    Mirrors scikit-learn's MinMaxScaler API exactly.\n    \"\"\"\n\n    def __init__(self, feature_range=(0, 1)):\n        # __init__ stores CONFIGURATION -- not yet learned from data\n        self.feature_range = feature_range\n        self.min_   = None   # will be set by fit()\n        self.scale_ = None   # will be set by fit()\n\n    def fit(self, values):\n        \"\"\"\n        Learn min and scale from the training data.\n        Stores them as instance attributes (note the trailing underscore --\n        scikit-learn convention: attributes set by fit() end in _).\n        \"\"\"\n        self.min_   = min(values)\n        data_range  = max(values) - min(values)\n        self.scale_ = data_range if data_range != 0 else 1.0\n        return self   # return self so fit() can be chained: scaler.fit(X).transform(X)\n\n    def transform(self, values):\n        \"\"\"\n        Apply the learned scaling.\n        Requires fit() to have been called first.\n        \"\"\"\n        if self.min_ is None:\n            raise RuntimeError('Call fit() before transform()')\n        lo, hi = self.feature_range\n        return [lo + (v - self.min_) / self.scale_ * (hi - lo) for v in values]\n\n    def fit_transform(self, values):\n        \"\"\"Convenience: fit and transform in one call.\"\"\"\n        return self.fit(values).transform(values)\n\n\n# Use our hand-built scaler\nsalaries = [72_000, 88_000, 105_000, 135_000, 155_000, 195_000, 210_000]\n\nscaler = MinMaxScaler()          # create instance -- no data yet\nscaler.fit(salaries)             # learn min and scale from training data\nscaled = scaler.transform(salaries)  # apply the scaling\n\nprint('Our MinMaxScaler:')\nfor sal, sc in zip(salaries, scaled):\n    print(f'  ${sal:>10,}  ->  {sc:.4f}')\nprint(f'  Learned min_:   {scaler.min_:,}')\nprint(f'  Learned scale_: {scaler.scale_:,}')\n\nprint()\n\n# Now the IDENTICAL API from scikit-learn\nfrom sklearn.preprocessing import MinMaxScaler as SklearnScaler\nimport numpy as np\n\nsk_scaler = SklearnScaler()\nX = np.array(salaries).reshape(-1, 1)   # sklearn needs a 2D array\nsk_scaler.fit(X)\nsk_scaled = sk_scaler.transform(X).flatten()\n\nprint('scikit-learn MinMaxScaler (same API, same results):')\nfor sal, sc in zip(salaries, sk_scaled):\n    print(f'  ${sal:>10,}  ->  {sc:.4f}')\nprint()\nprint('The OOP pattern is identical. This is why understanding classes')\nprint('makes every scikit-learn and Keras API immediately readable.')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 2.5 -- Advanced Techniques: Comprehensions, Generators, Decorators\n\nThese three features are what make Python code feel professional.\nYou will see them constantly in library source code, production ML pipelines,\nand code reviews. Knowing them makes you a confident Python reader and writer.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 2.5.1 -- List, Dict, and Set Comprehensions\n#\n# A comprehension is a compact, readable way to build a list, dict, or set\n# from an existing iterable -- without writing an explicit for loop.\n#\n# Syntax:\n#   [expression  for item in iterable  if condition]\n#   {key: value  for item in iterable  if condition}\n#   {expression  for item in iterable  if condition}\n\nsalaries  = [72_000, 88_000, 0, 105_000, -1, 135_000, 155_000, 195_000]\ncountries = ['USA', 'UK', 'Germany', 'India', 'Canada', 'France']\n\n# List comprehension\nvalid_salaries = [s for s in salaries if s > 0]   # filter out invalid values\nprint(f'Valid salaries:   {valid_salaries}')\n\nscaled_salaries = [s / 200_000 for s in valid_salaries]   # transform\nprint(f'Scaled (0-1 ish): {[round(x, 3) for x in scaled_salaries]}')\n\n# Dict comprehension\nsalary_map = {f'R{i+1:03d}': s for i, s in enumerate(valid_salaries)}\nprint(f'Salary map: {salary_map}')\n\n# Set comprehension -- unique first letters of countries\nfirst_letters = {c[0] for c in countries}\nprint(f'First letters: {sorted(first_letters)}')\n\n# Nested comprehension -- all (country, salary) pairs above $100k\nhigh_pairs = [(c, s) for c in countries[:3] for s in valid_salaries if s > 100_000]\nprint(f'High-salary country pairs: {high_pairs[:4]}...')\n\n# Equivalent for-loop vs comprehension -- same result, comprehension is faster\nloop_result = []\nfor s in salaries:\n    if s > 0:\n        loop_result.append(s)\nassert loop_result == valid_salaries, 'Results must match'\nprint('Loop and comprehension produce identical results: confirmed')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 2.5.2 -- Generators: Memory-Efficient Iteration\n#\n# A generator produces values ONE AT A TIME on demand -- it never\n# builds the full sequence in memory. This matters enormously when\n# processing large datasets that do not fit in RAM.\n#\n# A generator function uses 'yield' instead of 'return'.\n# A generator expression is a comprehension with () instead of [].\n\nimport sys\n\n# Compare memory usage: list vs generator\nbig_list = [s * 1.1 for s in range(100_000)]        # builds full list in memory\nbig_gen  = (s * 1.1 for s in range(100_000))         # generates on demand\n\nprint(f'List size:      {sys.getsizeof(big_list):>10,} bytes')\nprint(f'Generator size: {sys.getsizeof(big_gen):>10,} bytes')\nprint(f'Memory ratio:   {sys.getsizeof(big_list) / sys.getsizeof(big_gen):.0f}x more for the list')\n\n# Generator function -- streams rows from CSV one at a time\ndef stream_valid_salaries(rows, min_salary=10_000, max_salary=500_000):\n    \"\"\"\n    Generator that yields valid salary values one at a time.\n    Handles an arbitrarily large list without loading all values.\n    \"\"\"\n    for row in rows:\n        try:\n            sal = float(row.get('salary', 0) or 0)\n        except (ValueError, TypeError):\n            continue   # skip malformed values\n        if min_salary <= sal <= max_salary:\n            yield sal   # 'yield' pauses the function and sends the value\n                        # execution resumes here on the next next() call\n\n# Simulate a dataset\nfake_rows = [\n    {'salary': 135_000}, {'salary': 0}, {'salary': 88_000},\n    {'salary': None},    {'salary': 210_000}, {'salary': 'bad'},\n    {'salary': 72_000},  {'salary': 600_000},  # too high -- filtered\n]\n\nsalary_gen = stream_valid_salaries(fake_rows)\nprint()\nprint('Streaming valid salaries:')\ntotal = 0\ncount = 0\nfor sal in salary_gen:   # iterate the generator -- one value at a time\n    print(f'  ${sal:,.0f}')\n    total += sal\n    count += 1\n\nprint(f'Mean: ${total/count:,.0f}  (n={count})')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 2.5.3 -- Decorators\n#\n# A decorator is a function that WRAPS another function to add behaviour\n# before or after it runs -- without modifying the original function.\n# Syntax: @decorator_name above the function definition.\n#\n# You have already used decorators without knowing it:\n#   @property, @staticmethod, @classmethod in OOP\n#   @app.route() in Flask/FastAPI web frameworks\n\nimport time\nimport functools\n\n# Decorator 1: timer -- measures how long a function takes\ndef timer(func):\n    \"\"\"\n    Decorator that prints the execution time of any function.\n    functools.wraps preserves the original function's name and docstring.\n    \"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        start  = time.perf_counter()         # high-resolution timer\n        result = func(*args, **kwargs)       # call the original function\n        end    = time.perf_counter()\n        print(f'  [{func.__name__}] took {(end-start)*1000:.2f} ms')\n        return result\n    return wrapper\n\n\n# Decorator 2: validate_input -- checks that salary is positive\ndef require_positive_salary(func):\n    \"\"\"Decorator that raises ValueError if the first argument is not positive.\"\"\"\n    @functools.wraps(func)\n    def wrapper(salary, *args, **kwargs):\n        if salary <= 0:\n            raise ValueError(f'{func.__name__}: salary must be positive, got {salary}')\n        return func(salary, *args, **kwargs)\n    return wrapper\n\n\n# Apply decorators with @ syntax\n@timer\n@require_positive_salary\ndef compute_tax(salary, rate=0.30):\n    \"\"\"Compute tax owed at a given rate.\"\"\"\n    time.sleep(0.001)   # simulate a small computation\n    return salary * rate\n\n\nprint('Decorated function calls:')\ntax = compute_tax(135_000)\nprint(f'  Tax on $135,000 @ 30%: ${tax:,.0f}')\n\ntax = compute_tax(88_000, rate=0.25)\nprint(f'  Tax on $88,000 @ 25%:  ${tax:,.0f}')\n\nprint()\ntry:\n    compute_tax(-5_000)\nexcept ValueError as e:\n    print(f'  Caught: {e}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 2.5.4 -- Context Managers with 'with'\n#\n# You have already used context managers: 'with open(...)' is the most common.\n# Here we see HOW they work and how to write your own.\n#\n# A context manager implements __enter__ and __exit__ methods.\n# __enter__ runs at the start of the 'with' block.\n# __exit__ runs at the end -- even if an exception occurred.\n\nimport time\n\nclass Timer:\n    \"\"\"Context manager that times a block of code.\"\"\"\n\n    def __init__(self, label=''):\n        self.label = label\n\n    def __enter__(self):\n        self.start = time.perf_counter()\n        return self   # the value bound to 'as t' in 'with Timer() as t'\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        # exc_type is None if no exception occurred\n        elapsed = (time.perf_counter() - self.start) * 1000\n        status  = 'ERROR' if exc_type else 'OK'\n        print(f'  [{self.label}] {elapsed:.2f} ms  ({status})')\n        return False   # False = do not suppress exceptions\n\n\nprint('Context manager timing blocks:')\n\nwith Timer('list comprehension') as t:\n    result = [x ** 2 for x in range(50_000)]\n\nwith Timer('generator sum') as t:\n    result = sum(x ** 2 for x in range(50_000))\n\n# contextlib.contextmanager: write context managers as generators\nfrom contextlib import contextmanager\n\n@contextmanager\ndef log_step(name):\n    \"\"\"Log the start and end of a processing step.\"\"\"\n    print(f'  Starting: {name}')\n    try:\n        yield   # the body of the 'with' block runs here\n    finally:\n        print(f'  Finished: {name}')\n\nprint()\nwith log_step('data cleaning'):\n    time.sleep(0.01)   # simulated work\n\nwith log_step('feature engineering'):\n    time.sleep(0.01)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 2.5.5 -- Project: SurveyLoader Class\n\nWe now build the `SurveyLoader` class that ties together everything from Chapter 2:\nmodules, file I/O, error handling, OOP, and the fit/transform pattern.\n\n`SurveyLoader` follows the same architectural pattern as scikit-learn's\n`Pipeline` and PyTorch's `DataLoader` -- a class that encapsulates all\nthe complexity of data loading behind a clean, predictable interface.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import csv\nimport json\nimport urllib.request\nimport urllib.error\nfrom collections import Counter\n\nDATASET_URL = 'https://raw.githubusercontent.com/timothy-watt/python-for-ai-ml/main/data/so_survey_2025_curated.csv'\n\nclass SurveyLoader:\n    \"\"\"\n    Loads, validates, and provides access to SO 2025 Developer Survey data.\n\n    Follows the fit/load pattern:\n        loader = SurveyLoader(url)\n        loader.load()               # fetch and parse the CSV\n        loader.validate()           # check data quality\n        df = loader.get_records()   # return the cleaned data\n\n    Attributes set after load():\n        records_      : list of dicts -- all loaded rows\n        column_names_ : list of str -- column headers\n        n_rows_       : int -- number of loaded rows\n    \"\"\"\n\n    # Columns we require to be non-empty for a row to be kept\n    REQUIRED_COLS = ['ConvertedCompYearly', 'Country']\n\n    # Salary bounds for outlier filtering\n    SALARY_MIN =      5_000\n    SALARY_MAX = 10_000_000\n\n    def __init__(self, url, max_rows=None, random_seed=42):\n        self.url         = url\n        self.max_rows    = max_rows\n        self.random_seed = random_seed\n        # Attributes set by load() -- trailing _ follows scikit-learn convention\n        self.records_      = None\n        self.column_names_ = None\n        self.n_rows_       = 0\n        self._is_loaded    = False\n\n    def load(self):\n        \"\"\"\n        Fetch the CSV from the URL and parse into a list of dicts.\n        Sets self.records_, self.column_names_, self.n_rows_.\n        Returns self for method chaining.\n        \"\"\"\n        print(f'Loading: {self.url}')\n        try:\n            with urllib.request.urlopen(self.url, timeout=20) as resp:\n                raw = resp.read().decode('utf-8')\n        except urllib.error.URLError as e:\n            raise ConnectionError(f'Could not reach dataset URL: {e.reason}') from e\n\n        reader = csv.DictReader(raw.splitlines())\n        self.column_names_ = reader.fieldnames or []\n\n        records = []\n        skipped = 0\n        for row in reader:\n            # Filter: required columns must be non-empty\n            if any(not row.get(c, '').strip() for c in self.REQUIRED_COLS):\n                skipped += 1\n                continue\n            # Filter: salary must be a valid number in range\n            try:\n                sal = float(row['ConvertedCompYearly'])\n                if not (self.SALARY_MIN <= sal <= self.SALARY_MAX):\n                    skipped += 1\n                    continue\n                row['ConvertedCompYearly'] = sal   # store as float\n            except (ValueError, KeyError):\n                skipped += 1\n                continue\n\n            records.append(dict(row))\n            if self.max_rows and len(records) >= self.max_rows:\n                break\n\n        self.records_   = records\n        self.n_rows_    = len(records)\n        self._is_loaded = True\n        print(f'  Loaded {self.n_rows_:,} rows  (skipped {skipped:,} incomplete/invalid)')\n        return self\n\n    def validate(self):\n        \"\"\"Run data quality checks. Prints a report. Returns self.\"\"\"\n        self._require_loaded()\n        print('Data Quality Report:')\n        print(f'  Total rows:       {self.n_rows_:,}')\n        print(f'  Total columns:    {len(self.column_names_)}')\n        # Missing value counts per column\n        missing = {}\n        for col in self.column_names_:\n            n_missing = sum(1 for r in self.records_ if not str(r.get(col, '')).strip())\n            if n_missing > 0:\n                missing[col] = n_missing\n        if missing:\n            print(f'  Columns with missing values:')\n            for col, n in sorted(missing.items(), key=lambda x: -x[1])[:5]:\n                pct = n / self.n_rows_ * 100\n                print(f'    {col:<35} {n:>5,} ({pct:.1f}%)')\n        else:\n            print('  No missing values in required columns.')\n        return self\n\n    def get_records(self, columns=None):\n        \"\"\"\n        Return the loaded records, optionally selecting specific columns.\n        Returns a list of dicts.\n        \"\"\"\n        self._require_loaded()\n        if columns is None:\n            return self.records_\n        return [{c: r.get(c) for c in columns} for r in self.records_]\n\n    def salary_summary(self):\n        \"\"\"Print salary statistics.\"\"\"\n        self._require_loaded()\n        sals = sorted(r['ConvertedCompYearly'] for r in self.records_)\n        n    = len(sals)\n        mean = sum(sals) / n\n        mid  = n // 2\n        median = sals[mid] if n % 2 else (sals[mid-1] + sals[mid]) / 2\n        print(f'Salary Summary (n={n:,}):')\n        print(f'  Mean:   ${mean:>10,.0f}')\n        print(f'  Median: ${median:>10,.0f}')\n        print(f'  Min:    ${sals[0]:>10,.0f}')\n        print(f'  Max:    ${sals[-1]:>10,.0f}')\n\n    def top_countries(self, n=5):\n        \"\"\"Print the top n countries by respondent count.\"\"\"\n        self._require_loaded()\n        counts = Counter(r['Country'] for r in self.records_)\n        print(f'Top {n} countries:')\n        for country, count in counts.most_common(n):\n            print(f'  {country:<30} {count:>5,} respondents')\n\n    def _require_loaded(self):\n        \"\"\"Internal helper: raise if load() has not been called yet.\"\"\"\n        if not self._is_loaded:\n            raise RuntimeError('Call load() before using this method.')\n\n    def __repr__(self):\n        status = f'{self.n_rows_:,} rows loaded' if self._is_loaded else 'not yet loaded'\n        return f'SurveyLoader(url={self.url!r}, status={status!r})'\n\n\n# Run the full loader pipeline\nloader = SurveyLoader(DATASET_URL, max_rows=200)\nloader.load().validate()   # method chaining: load() returns self\nprint()\nloader.salary_summary()\nprint()\nloader.top_countries(n=8)\nprint()\nprint(repr(loader))\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 2.5.6 -- Extending SurveyLoader: a subclass with language analytics\n#\n# Demonstrates inheritance from our own class -- same pattern as\n# subclassing PyTorch's nn.Module or scikit-learn's BaseEstimator.\n\nclass SurveyAnalyser(SurveyLoader):\n    \"\"\"\n    Extends SurveyLoader with language and AI tool analytics.\n    Inherits all loading and validation from SurveyLoader.\n    \"\"\"\n\n    def top_languages(self, n=10):\n        \"\"\"\n        Count language popularity across all respondents.\n        Languages are semicolon-separated; we split and flatten.\n        \"\"\"\n        self._require_loaded()\n        all_langs = []\n        for r in self.records_:\n            lang_str = r.get('LanguageHaveWorkedWith', '')\n            if lang_str:\n                all_langs.extend(l.strip() for l in lang_str.split(';') if l.strip())\n        counts = Counter(all_langs)\n        print(f'Top {n} languages (from {len(self.records_):,} respondents):')\n        for rank, (lang, count) in enumerate(counts.most_common(n), 1):\n            pct = count / len(self.records_) * 100\n            bar = '#' * int(pct / 2)\n            print(f'  #{rank:>2}: {lang:<25} {count:>5,} ({pct:>5.1f}%)  {bar}')\n\n    def ai_tool_adoption(self):\n        \"\"\"Summarise AI tool usage.\"\"\"\n        self._require_loaded()\n        using_ai   = sum(1 for r in self.records_\n                         if r.get('AIToolCurrently', '').strip())\n        pct        = using_ai / len(self.records_) * 100\n        print(f'AI Tool Adoption:')\n        print(f'  Respondents using AI tools: {using_ai:,} of {len(self.records_):,} ({pct:.1f}%)')\n        all_tools = []\n        for r in self.records_:\n            tools_str = r.get('AIToolCurrently', '')\n            if tools_str:\n                all_tools.extend(t.strip() for t in tools_str.split(';') if t.strip())\n        if all_tools:\n            print('  Top AI tools:')\n            for tool, count in Counter(all_tools).most_common(5):\n                print(f'    {tool:<35} {count:>4,}')\n\n\nanalyser = SurveyAnalyser(DATASET_URL, max_rows=500)\nanalyser.load()\nprint()\nanalyser.top_languages(n=8)\nprint()\nanalyser.ai_tool_adoption()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Concept Check Questions\n\n> Test your understanding before moving on. Answer each question without referring back to the notebook, then expand to check.\n\n**Q1.** Explain what a **dunder method** is and give two examples with their purpose.\n\n<details><summary>Show answer</summary>\n\nDunder (double underscore) methods are special methods Python calls implicitly. `__init__(self, ...)` — called when an object is created. `__repr__(self)` — called by `repr()` to produce a string representation. Others: `__len__`, `__add__`, `__iter__`.\n\n</details>\n\n**Q2.** What is the difference between a **class attribute** and an **instance attribute**?\n\n<details><summary>Show answer</summary>\n\nA **class attribute** is defined on the class body and shared by all instances. An **instance attribute** is set on `self` inside `__init__` and unique to each object. Changing a class attribute affects all instances; changing an instance attribute affects only that object.\n\n</details>\n\n**Q3.** Why does a scikit-learn `Pipeline` fit naturally with Python OOP? Name the two core methods they share.\n\n<details><summary>Show answer</summary>\n\nscikit-learn estimators are Python classes. Every estimator implements `fit()` (learn from data) and `transform()` or `predict()` (apply what was learned). A `Pipeline` chains these objects — exactly the `SurveyAnalyser` pattern built in this chapter.\n\n</details>\n\n**Q4.** What is a **generator** and why use one instead of a list?\n\n<details><summary>Show answer</summary>\n\nA generator uses `yield` to produce values one at a time rather than storing the entire sequence in memory. Use a generator when the sequence is large or potentially infinite — it uses O(1) memory instead of O(n).\n\n</details>\n\n**Q5.** Write a decorator `@timer` that prints the execution time of any function.\n\n<details><summary>Show answer</summary>\n\n```python\nimport time\ndef timer(func):\n    def wrapper(*args, **kwargs):\n        start = time.time()\n        result = func(*args, **kwargs)\n        print(f'{func.__name__} took {time.time()-start:.4f}s')\n        return result\n    return wrapper\n```\n\n</details>\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Coding Exercises\n\n> Three exercises per chapter: **🔧 Guided** (fill-in-the-blanks) · **🔨 Applied** (write from scratch) · **🏗️ Extension** (go beyond the chapter)\n\nExercises use the SO 2025 developer survey dataset.\nExpand each **Solution** block only after attempting the exercise.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exercise 1 🔧 Guided — SurveyRecord dataclass with validation\n\nComplete the `SurveyRecord` dataclass with a `__post_init__` that validates:\n- `years_exp` is between 0 and 50\n- `salary` is positive\n- `country` is a non-empty string\nRaise `ValueError` with a descriptive message for each violation.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from dataclasses import dataclass\n\n@dataclass\nclass SurveyRecord:\n    respondent_id: int\n    years_exp:     float\n    salary:        float\n    country:       str\n    uses_python:   bool = False\n\n    def __post_init__(self) -> None:\n        # YOUR CODE HERE\n        pass\n\n# Test\ntry:\n    bad = SurveyRecord(1, years_exp=200, salary=50000, country='US')\nexcept ValueError as e:\n    print(f'Caught: {e}')\ngood = SurveyRecord(2, years_exp=5, salary=90000, country='Germany')\nprint('Valid:', good)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<details><summary>💡 Hint</summary>\n\nUse `if not 0 <= self.years_exp <= 50: raise ValueError(...)` pattern.\n`__post_init__` runs automatically after `__init__` in dataclasses.\n\n</details>\n\n<details><summary>✅ Solution</summary>\n\n```python\n@dataclass\nclass SurveyRecord:\n    respondent_id: int\n    years_exp: float\n    salary: float\n    country: str\n    uses_python: bool = False\n\n    def __post_init__(self):\n        if not 0 <= self.years_exp <= 50:\n            raise ValueError(f'years_exp must be 0-50, got {self.years_exp}')\n        if self.salary <= 0:\n            raise ValueError(f'salary must be positive, got {self.salary}')\n        if not self.country.strip():\n            raise ValueError('country must be a non-empty string')\n```\n\n</details>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exercise 2 🔨 Applied — Generator pipeline for survey processing\n\nWrite a generator pipeline that lazily processes survey records:\n1. `read_records(n)` — yields `n` SurveyRecord objects (use random data)\n2. `filter_by_country(records, country)` — yields only matching records\n3. `add_salary_band(records)` — yields `(record, band)` tuples where\n   `band` is `'low'` (<50k), `'mid'` (50k–120k), or `'high'` (>120k)\n\nChain them together and count how many UK high-earners are in 5,000 records.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import random\nrandom.seed(42)\nCOUNTRIES = ['US', 'UK', 'Germany', 'India', 'Canada']\n\ndef read_records(n: int):\n    # YOUR CODE — yield SurveyRecord objects\n    pass\n\ndef filter_by_country(records, country: str):\n    # YOUR CODE\n    pass\n\ndef add_salary_band(records):\n    # YOUR CODE — yield (record, band) tuples\n    pass\n\nuk_high = sum(1 for r, b in add_salary_band(\n    filter_by_country(read_records(5000), 'UK'))\n    if b == 'high')\nprint(f'UK high-earners: {uk_high}')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<details><summary>💡 Hint</summary>\n\nEach function should use `yield` — no lists, no `.append()`. This is a lazy pipeline: records are processed one at a time without loading all 5,000 into memory.\n\n</details>\n\n<details><summary>✅ Solution</summary>\n\n```python\ndef read_records(n):\n    for i in range(n):\n        yield SurveyRecord(i, random.uniform(0,30),\n                           random.gauss(85000,35000), random.choice(COUNTRIES))\ndef filter_by_country(records, country):\n    for r in records:\n        if r.country == country: yield r\ndef add_salary_band(records):\n    for r in records:\n        if r.salary < 50000: band = 'low'\n        elif r.salary < 120000: band = 'mid'\n        else: band = 'high'\n        yield r, band\n```\n\n</details>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exercise 3 🏗️ Extension — Context manager for timed ML experiments\n\nImplement an `Experiment` context manager that:\n1. Records start time on `__enter__`\n2. Records end time and duration on `__exit__`\n3. Accepts a `name` parameter\n4. Stores all completed experiments in a class-level list `Experiment.history`\n5. Implements `Experiment.summary()` that prints a table of all runs\n\nUse it to time three different classifier fits on the salary dataset.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import time\nfrom typing import Optional\n\nclass Experiment:\n    history: list = []\n\n    def __init__(self, name: str) -> None:\n        self.name = name\n        self._start: Optional[float] = None\n\n    def __enter__(self) -> 'Experiment':\n        # YOUR CODE\n        return self\n\n    def __exit__(self, *args) -> None:\n        # YOUR CODE\n        pass\n\n    @classmethod\n    def summary(cls) -> None:\n        # YOUR CODE\n        pass\n\n# Example usage:\n# with Experiment('GBM depth=3') as exp:\n#     clf.fit(X_tr, y_tr)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<details><summary>💡 Hint</summary>\n\nIn `__enter__`: `self._start = time.perf_counter()`. In `__exit__`: compute duration and append `{'name': self.name, 'duration_s': ...}` to `Experiment.history`.\n\n</details>\n\n<details><summary>✅ Solution</summary>\n\n```python\nclass Experiment:\n    history = []\n    def __init__(self, name): self.name = name; self._start = None\n    def __enter__(self):\n        self._start = time.perf_counter(); return self\n    def __exit__(self, *args):\n        duration = time.perf_counter() - self._start\n        Experiment.history.append({'name': self.name, 'duration_s': round(duration, 4)})\n    @classmethod\n    def summary(cls):\n        print(f'{'Experiment':<30} {'Time (s)':>10}')\n        for e in cls.history:\n            print(f'{e[\"name\"]:<30} {e[\"duration_s\"]:>10.4f}')\n```\n\n</details>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Chapter 2 Summary\n\nChapter 2 covers the techniques that separate beginner Python from professional Python.\nEvery pattern here reappears in the data science and ML chapters ahead.\n\n### Key Takeaways\n\n- **Modules:** `import module as alias` is the universal data science convention.\n  `collections.Counter` and `collections.defaultdict` are indispensable.\n- **File I/O:** Always use `with open(...)` -- it guarantees the file closes on error.\n  `csv.DictReader` and `json.load()` are your primary tools for raw file parsing.\n- **Error handling:** `try/except/else/finally` -- use specific exception types,\n  never use bare `except:`, and always re-raise or log unexpected errors.\n- **OOP:** A class bundles data (attributes) and behaviour (methods).\n  `__init__` stores config; `fit()`-style methods learn from data; attributes\n  set by fit end in `_` by convention.\n- **The scikit-learn bridge:** `StandardScaler`, `Pipeline`, `RandomForestClassifier` --\n  all of them are just classes with `__init__`, `fit()`, and `transform()`/`predict()`.\n  You now understand exactly how they work.\n- **Comprehensions** are faster and more readable than equivalent for-loops.\n- **Generators** use `yield` and produce values on demand -- essential for large data.\n- **Decorators** wrap functions to add behaviour (timing, validation, logging)\n  without modifying the original function.\n\n### Project Thread Status\n\n| Task | Status |\n|------|--------|\n| Built `SurveyLoader` class with `load()`, `validate()`, `get_records()` | Done |\n| Error handling for network failures and malformed rows | Done |\n| Built `SurveyAnalyser` subclass with language and AI tool analytics | Done |\n| Followed scikit-learn fit/transform pattern throughout | Done |\n\n---\n\n### What's Next: Chapter 3 -- NumPy and Pandas\n\nChapter 3 introduces the data science stack. With NumPy and Pandas,\neverything from Chapter 2 that we did manually -- loading, cleaning,\nsummarising -- becomes 10-100x faster and dramatically more concise.\nWe load the full 15,000-row SO 2025 dataset and build a complete cleaning pipeline.\n\n---\n\n*End of Chapter 2 -- Python for AI/ML*  \n[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n"
    }
  ]
}