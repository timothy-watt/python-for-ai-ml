{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Chapter 6 -- Machine Learning with scikit-learn\n## *Python for AI/ML: A Complete Learning Journey*\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/CH06_Machine_Learning_Sklearn.ipynb)\n&nbsp;&nbsp;[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n\n---\n\n**Part:** 3 -- Machine Learning and AI  \n**Prerequisites:** Chapter 5 (SciPy and Statistical Computing)  \n**Estimated time:** 6-7 hours\n\n---\n\n### Learning Objectives\n\nBy the end of this chapter you will be able to:\n\n- Explain the supervised vs unsupervised learning distinction\n- Build a complete scikit-learn preprocessing pipeline with `Pipeline` and `ColumnTransformer`\n- Train and evaluate regression models: Linear, Ridge, Lasso, Random Forest\n- Train and evaluate classification models: Logistic Regression, Decision Tree, Random Forest, Gradient Boosting\n- Use cross-validation correctly and interpret learning curves\n- Apply unsupervised learning: KMeans clustering and PCA dimensionality reduction\n- Interpret feature importances and SHAP-style analysis\n- Tune hyperparameters with GridSearchCV and RandomizedSearchCV\n\n---\n\n### Project Thread -- Chapter 6\n\nThree complete ML tasks on the SO 2025 dataset:\n\n1. **Regression** -- predict annual salary from developer profile features\n2. **Classification** -- predict whether a developer uses Python as their primary language\n3. **Clustering** -- discover natural groupings of developers by skills and compensation\n\nEach task follows the same professional workflow: clean data, build pipeline,\ntrain, cross-validate, evaluate, and interpret.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Setup -- Imports and Data\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import (\n    train_test_split, cross_val_score, KFold,\n    GridSearchCV, RandomizedSearchCV, learning_curve\n)\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import (\n    StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\n)\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import (\n    RandomForestRegressor, RandomForestClassifier, GradientBoostingClassifier\n)\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import (\n    mean_absolute_error, mean_squared_error, r2_score,\n    accuracy_score, classification_report, confusion_matrix,\n    silhouette_score\n)\n\nimport sklearn\nprint(f'scikit-learn: {sklearn.__version__}')\nprint(f'NumPy:        {np.__version__}')\nprint(f'Pandas:       {pd.__version__}')\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.dpi']       = 110\nplt.rcParams['axes.titlesize']   = 13\nplt.rcParams['axes.titleweight'] = 'bold'\n\nDATASET_URL = 'https://raw.githubusercontent.com/timothy-watt/python-for-ai-ml/main/data/so_survey_2025_curated.csv'\nRANDOM_STATE = 42\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Load and clean SO 2025 -- full pipeline from Chapter 3\ndf_raw = pd.read_csv(DATASET_URL)\ndf = df_raw.copy()\n\n# Salary target\ndf = df.dropna(subset=['ConvertedCompYearly'])\ndf['ConvertedCompYearly'] = pd.to_numeric(df['ConvertedCompYearly'], errors='coerce')\nQ1, Q3 = df['ConvertedCompYearly'].quantile([0.25, 0.75])\nIQR = Q3 - Q1\ndf = df[\n    (df['ConvertedCompYearly'] >= max(Q1 - 3*IQR, 5_000)) &\n    (df['ConvertedCompYearly'] <= min(Q3 + 3*IQR, 600_000))\n].copy()\n\n# Numeric features\nif 'YearsCodePro' in df.columns:\n    df['YearsCodePro'] = pd.to_numeric(df['YearsCodePro'], errors='coerce')\n\n# Categorical features\nfor col in ['Country', 'EdLevel', 'Employment', 'RemoteWork', 'OrgSize', 'DevType']:\n    if col in df.columns:\n        df[col] = df[col].fillna('Unknown')\n\n# Derived features\ndf['uses_python'] = df.get('LanguageHaveWorkedWith', pd.Series(dtype=str)).str.contains('Python', na=False).astype(int)\ndf['uses_sql']    = df.get('LanguageHaveWorkedWith', pd.Series(dtype=str)).str.contains('SQL', na=False).astype(int)\ndf['uses_js']     = df.get('LanguageHaveWorkedWith', pd.Series(dtype=str)).str.contains('JavaScript', na=False).astype(int)\ndf['uses_ai']     = df.get('AIToolCurrently', pd.Series(dtype=str)).notna().astype(int)\ndf['log_salary']  = np.log(df['ConvertedCompYearly'])\n\ndf = df.reset_index(drop=True)\nprint(f'Dataset ready: {len(df):,} rows x {df.shape[1]} columns')\nprint(f'Target (salary): ${df[\"ConvertedCompYearly\"].median():,.0f} median')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 6.1 -- The Machine Learning Framework\n\n### Supervised vs Unsupervised Learning\n\n**Supervised learning** trains on labelled examples -- pairs of (input features, known target).\nThe model learns to map inputs to outputs. Two subtypes:\n- *Regression*: target is a continuous number (salary, house price, temperature)\n- *Classification*: target is a discrete category (Python user / not, spam / not spam)\n\n**Unsupervised learning** finds structure in data without labels.\n- *Clustering*: group similar items together (developer archetypes)\n- *Dimensionality reduction*: compress many features into fewer (PCA)\n\n### The scikit-learn API Contract\n\nEvery scikit-learn estimator follows the same three-method contract:\n```python\nmodel.fit(X_train, y_train)       # learn from training data\nmodel.predict(X_test)             # apply to new data\nmodel.score(X_test, y_test)       # evaluate performance\n```\nEvery transformer adds:\n```python\ntransformer.transform(X)          # apply the learned transformation\ntransformer.fit_transform(X)      # fit and transform in one step\n```\nThis consistent API is why switching between models is a one-line change.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 6.1.1 -- The train/test split: the most important rule in ML\n#\n# A model evaluated on the data it trained on will ALWAYS look better than it is.\n# It has simply memorised the training data -- a phenomenon called overfitting.\n# The test set must NEVER be seen during training or hyperparameter tuning.\n\n# Define features and target for the regression task\nfeature_cols = [c for c in ['YearsCodePro', 'uses_python', 'uses_sql', 'uses_js', 'uses_ai']\n                if c in df.columns]\ntarget_col   = 'log_salary'   # we predict log(salary) and exponentiate back\n\nX = df[feature_cols].fillna(df[feature_cols].median())\ny = df[target_col]\n\n# 80/20 train-test split, stratified by salary quartile to preserve distribution\nsal_quartile = pd.qcut(y, q=4, labels=False)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=RANDOM_STATE,\n    stratify=sal_quartile\n)\n\nprint(f'Training set:  {len(X_train):,} rows  ({len(X_train)/len(X)*100:.0f}%)')\nprint(f'Test set:      {len(X_test):,} rows  ({len(X_test)/len(X)*100:.0f}%)')\nprint(f'Features:      {feature_cols}')\nprint(f'Target:        log_salary (exponentiate predictions to get USD)')\n\n# Verify distribution is preserved\nprint(f'Train median log-salary: {y_train.median():.4f}')\nprint(f'Test  median log-salary: {y_test.median():.4f}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 6.2 -- Regression: Predicting Salary\n\nWe build a complete regression pipeline with preprocessing and compare\nfour models: Linear Regression, Ridge, Lasso, and Random Forest.\nAll models are evaluated with 5-fold cross-validation on the training set\nbefore a single look at the test set.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 6.2.1 -- Building a preprocessing Pipeline\n#\n# The Pipeline chains preprocessing steps and the model in one object.\n# This guarantees that:\n#   1. Scalers are fit ONLY on training data (no data leakage)\n#   2. The same transformations are applied identically to test data\n#   3. The entire workflow is serialisable -- save one object, get everything\n\n# Numeric pipeline: impute missing values with median, then standardise\nnumeric_features = [c for c in ['YearsCodePro', 'uses_python', 'uses_sql', 'uses_js', 'uses_ai']\n                    if c in df.columns]\n\nnumeric_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),   # fill NaN with column median\n    ('scaler',  StandardScaler()),                    # zero mean, unit variance\n])\n\n# ColumnTransformer applies different pipelines to different column groups\npreprocessor = ColumnTransformer([\n    ('num', numeric_transformer, numeric_features),\n], remainder='drop')   # drop any columns not explicitly listed\n\n# Full pipelines: preprocessor + model in one object\npipelines = {\n    'Linear Regression': Pipeline([\n        ('prep',  preprocessor),\n        ('model', LinearRegression())\n    ]),\n    'Ridge (L2)': Pipeline([\n        ('prep',  preprocessor),\n        ('model', Ridge(alpha=1.0))\n    ]),\n    'Lasso (L1)': Pipeline([\n        ('prep',  preprocessor),\n        ('model', Lasso(alpha=0.01))\n    ]),\n    'Random Forest': Pipeline([\n        ('prep',  preprocessor),\n        ('model', RandomForestRegressor(\n            n_estimators=100,\n            max_depth=8,\n            random_state=RANDOM_STATE,\n            n_jobs=-1   # use all CPU cores\n        ))\n    ]),\n}\n\nprint('Pipelines built:')\nfor name in pipelines:\n    print(f'  {name}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 6.2.2 -- Cross-validation: honest model evaluation\n#\n# 5-fold CV splits the training data into 5 parts, trains on 4, validates on 1,\n# and rotates until every fold has been the validation set once.\n# This gives 5 independent performance estimates -- their mean and std\n# are much more trustworthy than a single train/validate split.\n\nkfold = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n\ncv_results = {}\nprint(f'5-fold cross-validation on training set (n={len(X_train):,})...')\nprint(f'{\"Model\":<22} {\"CV R^2 mean\":>12} {\"CV R^2 std\":>12} {\"CV MAE mean\":>14}')\nprint('-' * 62)\n\nfor name, pipe in pipelines.items():\n    r2_scores  = cross_val_score(pipe, X_train, y_train, cv=kfold, scoring='r2')\n    mae_scores = cross_val_score(pipe, X_train, y_train, cv=kfold,\n                                 scoring='neg_mean_absolute_error')\n    cv_results[name] = {\n        'r2_mean':  r2_scores.mean(),\n        'r2_std':   r2_scores.std(),\n        'mae_mean': -mae_scores.mean(),\n    }\n    print(f'{name:<22} {r2_scores.mean():>12.4f} {r2_scores.std():>12.4f} {-mae_scores.mean():>14.4f}')\n\nbest_model_name = max(cv_results, key=lambda k: cv_results[k]['r2_mean'])\nprint(f'Best model by CV R^2: {best_model_name}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 6.2.3 -- Final evaluation on the held-out test set\n#\n# We do this ONCE -- after all model selection is done using CV.\n# The test set result is our honest estimate of real-world performance.\n\nbest_pipe = pipelines[best_model_name]\nbest_pipe.fit(X_train, y_train)\n\ny_pred_log  = best_pipe.predict(X_test)\ny_pred_usd  = np.exp(y_pred_log)     # convert log predictions back to USD\ny_true_usd  = np.exp(y_test)         # convert log actuals back to USD\n\nmae  = mean_absolute_error(y_true_usd, y_pred_usd)\nrmse = np.sqrt(mean_squared_error(y_true_usd, y_pred_usd))\nr2   = r2_score(y_test, y_pred_log)  # R^2 on log scale (more meaningful)\n\nprint(f'Test set results ({best_model_name}):')\nprint(f'  R^2 (log scale):  {r2:.4f}')\nprint(f'  MAE  (USD):       ${mae:,.0f}')\nprint(f'  RMSE (USD):       ${rmse:,.0f}')\n\n# Residual plot\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].scatter(y_true_usd/1000, y_pred_usd/1000, alpha=0.25, s=10, color='#2E75B6')\nlim = max(y_true_usd.max(), y_pred_usd.max()) / 1000\naxes[0].plot([0, lim], [0, lim], 'r--', linewidth=2, label='Perfect prediction')\naxes[0].set_xlabel('Actual Salary ($k)')\naxes[0].set_ylabel('Predicted Salary ($k)')\naxes[0].set_title(f'{best_model_name}\\nActual vs Predicted Salary')\naxes[0].legend()\n\nresiduals = y_true_usd - y_pred_usd\naxes[1].scatter(y_pred_usd/1000, residuals/1000, alpha=0.25, s=10, color='#E8722A')\naxes[1].axhline(0, color='red', linestyle='--', linewidth=2)\naxes[1].set_xlabel('Predicted Salary ($k)')\naxes[1].set_ylabel('Residual ($k)')\naxes[1].set_title('Residual Plot\\n(random scatter = good; pattern = model problem)')\n\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 6.2.4 -- Feature importance (Random Forest)\n\n# Always use the regression RF pipeline -- fit it on regression training data\nrf_pipe = pipelines['Random Forest']\nrf_pipe.fit(X_train, y_train)\nrf_model    = rf_pipe.named_steps['model']\nimportances = rf_model.feature_importances_\n\n# Get feature names from the pipeline's preprocessor (handles any column ordering)\ntry:\n    feat_names = rf_pipe.named_steps['prep'].get_feature_names_out()\n    feat_names = [n.split('__')[-1] for n in feat_names]   # strip 'num__' prefix\nexcept Exception:\n    feat_names = numeric_features   # fallback to original list\n\nfeat_imp = pd.Series(importances, index=feat_names[:len(importances)]).sort_values(ascending=True)\n\nfig, ax = plt.subplots(figsize=(9, 4))\ncolors = ['#E8722A' if i == feat_imp.index[-1] else '#2E75B6' for i in feat_imp.index]\nfeat_imp.plot(kind='barh', ax=ax, color=colors)\nax.set_title('Random Forest: Feature Importances for Salary Prediction')\nax.set_xlabel('Importance (mean decrease in impurity)')\nfor i, v in enumerate(feat_imp.values):\n    ax.text(v + 0.002, i, f'{v:.3f}', va='center', fontsize=9)\nplt.tight_layout()\nplt.show()\n\nprint('Feature importances (higher = more predictive of salary):')\nfor feat, imp in feat_imp.sort_values(ascending=False).items():\n    print(f'  {feat:<25} {imp:.4f}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 6.3 -- Classification: Predicting Python Usage\n\nWe now build a classification pipeline that predicts whether a developer\nuses Python as their primary language, based on their salary, experience,\nand other profile features. This demonstrates the full classification workflow:\nencoding, class balance, multiple metrics, and confusion matrix interpretation.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 6.3.1 -- Build classification features and handle categorical encoding\n\n# Target: uses_python (already a 0/1 column)\nclf_target = 'uses_python'\n\n# Use numeric + one-hot encoded categorical features\nnum_cols = [c for c in ['YearsCodePro', 'ConvertedCompYearly', 'uses_sql', 'uses_js', 'uses_ai']\n            if c in df.columns]\ncat_cols = [c for c in ['EdLevel', 'RemoteWork'] if c in df.columns]\n\n# Build a ColumnTransformer that handles both types\nclf_preprocessor = ColumnTransformer([\n    ('num', Pipeline([\n        ('imputer', SimpleImputer(strategy='median')),\n        ('scaler',  StandardScaler()),\n    ]), num_cols),\n    ('cat', Pipeline([\n        ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n        ('ohe',     OneHotEncoder(handle_unknown='ignore', sparse_output=False)),\n    ]), cat_cols),\n], remainder='drop')\n\nX_clf = df[num_cols + cat_cols]\ny_clf = df[clf_target]\n\nX_tr, X_te, y_tr, y_te = train_test_split(\n    X_clf, y_clf, test_size=0.2, random_state=RANDOM_STATE, stratify=y_clf\n)\n\nprint(f'Classification target: {clf_target}')\nprint(f'Class balance:  Python={y_clf.mean()*100:.1f}%,  Non-Python={100-y_clf.mean()*100:.1f}%')\nprint(f'Train: {len(X_tr):,}   Test: {len(X_te):,}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 6.3.2 -- Train and cross-validate four classifiers\n\nclf_pipelines = {\n    'Logistic Regression': Pipeline([\n        ('prep',  clf_preprocessor),\n        ('model', LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))\n    ]),\n    'Decision Tree': Pipeline([\n        ('prep',  clf_preprocessor),\n        ('model', DecisionTreeClassifier(max_depth=6, random_state=RANDOM_STATE))\n    ]),\n    'Random Forest': Pipeline([\n        ('prep',  clf_preprocessor),\n        ('model', RandomForestClassifier(\n            n_estimators=100, max_depth=8,\n            random_state=RANDOM_STATE, n_jobs=-1\n        ))\n    ]),\n    'Gradient Boosting': Pipeline([\n        ('prep',  clf_preprocessor),\n        ('model', GradientBoostingClassifier(\n            n_estimators=100, max_depth=4,\n            random_state=RANDOM_STATE\n        ))\n    ]),\n}\n\nprint(f'5-fold CV on training set (n={len(X_tr):,})...')\nprint(f'{\"Model\":<22} {\"Accuracy\":>10} {\"Std\":>8}')\nprint('-' * 42)\n\nclf_cv = {}\nkfold  = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n\nfor name, pipe in clf_pipelines.items():\n    scores = cross_val_score(pipe, X_tr, y_tr, cv=kfold, scoring='accuracy')\n    clf_cv[name] = scores\n    print(f'{name:<22} {scores.mean():>10.4f} {scores.std():>8.4f}')\n\nbest_clf_name = max(clf_cv, key=lambda k: clf_cv[k].mean())\nprint(f'Best classifier by CV accuracy: {best_clf_name}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 6.3.3 -- Evaluate best classifier on the test set\n\nbest_clf = clf_pipelines[best_clf_name]\nbest_clf.fit(X_tr, y_tr)\ny_pred_clf = best_clf.predict(X_te)\n\nacc = accuracy_score(y_te, y_pred_clf)\nprint(f'Test accuracy ({best_clf_name}): {acc:.4f}  ({acc*100:.1f}%)')\nprint()\nprint('Classification Report:')\nprint(classification_report(y_te, y_pred_clf,\n                             target_names=['Non-Python', 'Python']))\n\n# Confusion matrix\ncm = confusion_matrix(y_te, y_pred_clf)\nfig, ax = plt.subplots(figsize=(6, 5))\nsns.heatmap(\n    cm, annot=True, fmt='d', cmap='Blues',\n    xticklabels=['Non-Python', 'Python'],\n    yticklabels=['Non-Python', 'Python'],\n    ax=ax\n)\nax.set_xlabel('Predicted')\nax.set_ylabel('Actual')\nax.set_title(f'{best_clf_name}\\nConfusion Matrix')\nplt.tight_layout()\nplt.show()\n\n# Interpret the confusion matrix\ntn, fp, fn, tp = cm.ravel()\nprint(f'True Negatives  (correctly predicted Non-Python): {tn:,}')\nprint(f'False Positives (predicted Python, actually not): {fp:,}')\nprint(f'False Negatives (predicted Non-Python, actually Python): {fn:,}')\nprint(f'True Positives  (correctly predicted Python): {tp:,}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 6.3.4 -- Learning curves: diagnosing bias vs variance\n#\n# A learning curve shows how training and validation score change\n# as we add more training data.\n#\n# High bias (underfitting):  both curves plateau low\n# High variance (overfitting): training score >> validation score\n# Good fit: both curves converge to a high value\n\ntrain_sizes, train_scores, val_scores = learning_curve(\n    best_clf, X_tr, y_tr,\n    cv=5, scoring='accuracy',\n    train_sizes=np.linspace(0.1, 1.0, 8),\n    n_jobs=-1\n)\n\ntrain_mean = train_scores.mean(axis=1)\ntrain_std  = train_scores.std(axis=1)\nval_mean   = val_scores.mean(axis=1)\nval_std    = val_scores.std(axis=1)\n\nfig, ax = plt.subplots(figsize=(9, 5))\nax.plot(train_sizes, train_mean, 'o-', color='#E8722A', linewidth=2, label='Training score')\nax.fill_between(train_sizes, train_mean-train_std, train_mean+train_std,\n                alpha=0.15, color='#E8722A')\nax.plot(train_sizes, val_mean, 'o-', color='#2E75B6', linewidth=2, label='Validation score')\nax.fill_between(train_sizes, val_mean-val_std, val_mean+val_std,\n                alpha=0.15, color='#2E75B6')\nax.set_xlabel('Training set size')\nax.set_ylabel('Accuracy')\nax.set_title(f'Learning Curve: {best_clf_name}')\nax.legend(fontsize=10)\nax.set_ylim(0.5, 1.01)\nplt.tight_layout()\nplt.show()\n\ngap = train_mean[-1] - val_mean[-1]\nprint(f'Final training accuracy:   {train_mean[-1]:.4f}')\nprint(f'Final validation accuracy: {val_mean[-1]:.4f}')\nprint(f'Gap (train - val):         {gap:.4f}  ', end='')\nif gap < 0.02:   print('(excellent fit -- low variance)')\nelif gap < 0.08: print('(mild overfitting -- acceptable)')\nelse:            print('(overfitting -- reduce model complexity or add regularisation)')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 6.4 -- Hyperparameter Tuning\n\nHyperparameters are settings you choose before training -- things like\ntree depth, regularisation strength, or number of estimators.\nThey are not learned from data; you have to search for good values.\n\nTwo strategies:\n- **GridSearchCV**: exhaustively tries every combination in a grid\n- **RandomizedSearchCV**: randomly samples combinations -- better when the grid is large\n\nBoth use cross-validation internally so the search never touches the test set.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 6.4.1 -- RandomizedSearchCV to tune the Random Forest classifier\n\nfrom scipy.stats import randint, uniform\n\n# Define the parameter distribution to sample from\n# Note the naming convention: 'step_name__parameter_name'\nparam_dist = {\n    'model__n_estimators':  randint(50, 300),\n    'model__max_depth':     randint(3, 15),\n    'model__min_samples_split': randint(2, 20),\n    'model__max_features':  ['sqrt', 'log2', 0.5],\n}\n\nrf_clf_pipe = Pipeline([\n    ('prep',  clf_preprocessor),\n    ('model', RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1)),\n])\n\nsearch = RandomizedSearchCV(\n    rf_clf_pipe,\n    param_distributions=param_dist,\n    n_iter=20,              # try 20 random combinations\n    cv=3,                   # 3-fold CV (faster than 5 during search)\n    scoring='accuracy',\n    random_state=RANDOM_STATE,\n    n_jobs=-1,\n    verbose=0\n)\n\nprint('Running RandomizedSearchCV (20 iterations x 3 folds = 60 fits)...')\nsearch.fit(X_tr, y_tr)\n\nprint(f'Best CV accuracy: {search.best_score_:.4f}')\nprint('Best parameters:')\nfor param, val in search.best_params_.items():\n    print(f'  {param}: {val}')\n\n# Evaluate the tuned model on the test set\ny_pred_tuned = search.best_estimator_.predict(X_te)\ntuned_acc    = accuracy_score(y_te, y_pred_tuned)\nprint(f'Tuned model test accuracy: {tuned_acc:.4f}  (vs baseline {acc:.4f})')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 6.5 -- Unsupervised Learning: Clustering and PCA\n\nClustering discovers natural groupings in data without labels.\nWe apply KMeans to the SO 2025 dataset to find archetypes of developers --\ngroups of respondents who are similar in skills, experience, and compensation.\n\nPCA (Principal Component Analysis) reduces dimensionality for visualisation\nand often improves clustering by removing noise from irrelevant features.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 6.5.1 -- KMeans clustering: finding developer archetypes\n\n# Build cluster features directly from df using only columns confirmed to exist\n# and filled so there are no NaN rows to drop\nall_wanted = ['YearsCodePro', 'ConvertedCompYearly',\n              'uses_python', 'uses_sql', 'uses_js', 'uses_ai']\ncluster_cols = [c for c in all_wanted if c in df.columns]\nprint(f\"Using cluster columns: {cluster_cols}\")\n\nX_cluster = df[cluster_cols].copy()\n\n# Fill every column with its own median; fall back to 0 if median is NaN\nfor col in cluster_cols:\n    med = X_cluster[col].median()\n    fill_val = med if pd.notna(med) else 0\n    X_cluster[col] = X_cluster[col].fillna(fill_val)\n# Final safety net: catch any remaining NaNs\nX_cluster = X_cluster.fillna(0)\n\n# Confirm no NaNs remain\nassert X_cluster.isnull().sum().sum() == 0, \"NaNs still present after fill\"\nX_cluster = X_cluster.reset_index(drop=True)\nprint(f\"Rows for clustering: {len(X_cluster):,}\")\n\n# Scale before clustering -- KMeans uses Euclidean distance\n# Unscaled features with different ranges distort distances\nscaler   = StandardScaler()\nX_scaled = scaler.fit_transform(X_cluster)\nprint(f\"X_scaled shape: {X_scaled.shape}\")\n\n# Choose k with the elbow method\ninertias   = []\nsil_scores = []\nk_range    = range(2, 10)\n\nfor k in k_range:\n    km = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\n    km.fit(X_scaled)\n    inertias.append(km.inertia_)\n    sil_scores.append(silhouette_score(X_scaled, km.labels_, sample_size=2000))\n\nfig, axes = plt.subplots(1, 2, figsize=(13, 4))\naxes[0].plot(k_range, inertias, 'o-', color='#2E75B6', linewidth=2)\naxes[0].set_xlabel('Number of Clusters (k)')\naxes[0].set_ylabel('Inertia')\naxes[0].set_title('Elbow Method: choose k where inertia curve bends')\naxes[1].plot(k_range, sil_scores, 'o-', color='#E8722A', linewidth=2)\naxes[1].set_xlabel('Number of Clusters (k)')\naxes[1].set_ylabel('Silhouette Score')\naxes[1].set_title('Silhouette Score: higher is better')\nplt.tight_layout()\nplt.show()\n\nbest_k = list(k_range)[sil_scores.index(max(sil_scores))]\nprint(f'Best k by silhouette score: {best_k}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 6.5.2 -- Fit final KMeans and profile each cluster\n\nbest_k = max(3, best_k)   # ensure at least 3 clusters for interesting profiles\n\nkm_final = KMeans(n_clusters=best_k, random_state=RANDOM_STATE, n_init=10)\nlabels   = km_final.fit_predict(X_scaled)\n\n# reset_index ensures the integer positions of labels align with DataFrame rows\nX_cluster_labelled = X_cluster.reset_index(drop=True).copy()\nX_cluster_labelled['cluster'] = labels\n\nprint(f'Cluster profiles (k={best_k}):')\nprofile = X_cluster_labelled.groupby('cluster')[cluster_cols].mean().round(2)\nprofile['count'] = X_cluster_labelled.groupby('cluster').size()\nprint(profile.to_string())\n\n# Name the clusters based on their profiles\nprint()\nprint('Cluster interpretation:')\nsal_col = 'ConvertedCompYearly'\nexp_col = 'YearsCodePro'\nif sal_col in profile.columns and exp_col in profile.columns:\n    for cluster_id in range(best_k):\n        row = profile.loc[cluster_id]\n        sal = row[sal_col]\n        exp = row[exp_col] if exp_col in row.index else 0\n        py  = row['uses_python'] if 'uses_python' in row.index else 0\n        ai  = row['uses_ai'] if 'uses_ai' in row.index else 0\n        print(f'  Cluster {cluster_id}: ${sal:,.0f} median salary, '\n              f'{exp:.0f} yrs exp, Python={py:.0%}, AI tools={ai:.0%}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 6.5.3 -- PCA visualisation of clusters\n#\n# PCA compresses our N-dimensional feature space into 2 dimensions\n# so we can plot the clusters on a scatter chart.\n# The two PCA axes explain as much variance as possible from the originals.\n\npca = PCA(n_components=2, random_state=RANDOM_STATE)\nX_pca = pca.fit_transform(X_scaled)\n\nexplained = pca.explained_variance_ratio_\nprint(f'PCA variance explained:')\nprint(f'  PC1: {explained[0]*100:.1f}%')\nprint(f'  PC2: {explained[1]*100:.1f}%')\nprint(f'  Total: {sum(explained)*100:.1f}%')\n\nfig, ax = plt.subplots(figsize=(10, 7))\ncolours = plt.cm.tab10.colors\nfor cluster_id in range(best_k):\n    mask = labels == cluster_id\n    ax.scatter(\n        X_pca[mask, 0], X_pca[mask, 1],\n        c=[colours[cluster_id]], alpha=0.35, s=12, linewidths=0,\n        label=f'Cluster {cluster_id} (n={mask.sum():,})'\n    )\n# Plot cluster centres\ncentres_pca = pca.transform(km_final.cluster_centers_)\nax.scatter(centres_pca[:, 0], centres_pca[:, 1],\n           c='black', s=180, marker='X', zorder=10, label='Cluster centres')\nax.set_xlabel(f'PC1 ({explained[0]*100:.1f}% variance)')\nax.set_ylabel(f'PC2 ({explained[1]*100:.1f}% variance)')\nax.set_title(f'KMeans Clustering (k={best_k}) Visualised with PCA\\nSO 2025 Developer Archetypes')\nax.legend(fontsize=9, markerscale=2)\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 6.6 -- Handling Class Imbalance\n\nReal-world classification datasets are rarely balanced. In the SO 2025 data,\nPython users may outnumber non-Python users 3:1. A naive classifier that\nalways predicts the majority class gets high accuracy but is useless.\n\n**Three strategies:**\n\n**Class weights** -- tell the algorithm to penalise errors on the minority\nclass more heavily. Built into most scikit-learn classifiers via `class_weight='balanced'`.\nNo extra data needed, no risk of overfitting synthetic samples.\n\n**SMOTE (Synthetic Minority Over-sampling Technique)** -- generates synthetic\nminority-class samples by interpolating between existing ones.\nIncreases minority representation without simply duplicating rows.\n\n**Threshold adjustment** -- instead of the default 0.5 decision threshold,\nchoose a threshold that optimises the metric you actually care about\n(precision, recall, or F1).\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 6.6.1 -- Diagnose and quantify class imbalance\n\nimport subprocess\nsubprocess.run(['pip', 'install', 'imbalanced-learn', '-q'], check=False)\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\n# Build binary classification dataset\nclf_feature_cols = [c for c in ['YearsCodePro', 'ConvertedCompYearly',\n                                  'uses_sql', 'uses_js', 'uses_ai']\n                    if c in df.columns]\n\nX_clf = df[clf_feature_cols].copy()\nfor col in clf_feature_cols:\n    med = X_clf[col].median()\n    X_clf[col] = X_clf[col].fillna(med if pd.notna(med) else 0)\ny_clf = df['uses_python']\n\nX_tr, X_te, y_tr, y_te = train_test_split(\n    X_clf, y_clf, test_size=0.2, random_state=RANDOM_STATE, stratify=y_clf\n)\n\nclass_counts = y_tr.value_counts()\nmajority = class_counts.max()\nminority = class_counts.min()\nratio    = majority / minority\n\nprint('Class distribution in training set:')\nfor cls, cnt in class_counts.items():\n    pct = cnt / len(y_tr) * 100\n    label = 'Python' if cls == 1 else 'Non-Python'\n    print(f'  {label} ({cls}): {cnt:,}  ({pct:.1f}%)')\nprint(f'Imbalance ratio: {ratio:.2f}:1')\nif ratio > 3:\n    print('  -> Significant imbalance: class weights or SMOTE recommended')\nelif ratio > 1.5:\n    print('  -> Mild imbalance: class weights usually sufficient')\nelse:\n    print('  -> Balanced: no special handling needed')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 6.6.2 -- Compare three approaches: baseline, class weights, SMOTE\n\npreprocess = ImbPipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler',  StandardScaler()),\n])\n\nX_tr_proc = preprocess.fit_transform(X_tr)\nX_te_proc = preprocess.transform(X_te)\n\nresults_imb = {}\n\n# 1. Baseline: no imbalance handling\nlr_base = LogisticRegression(max_iter=500, random_state=RANDOM_STATE)\nlr_base.fit(X_tr_proc, y_tr)\nresults_imb['Baseline'] = lr_base.predict(X_te_proc)\n\n# 2. Class weights: penalise minority class errors more\nlr_weighted = LogisticRegression(max_iter=500, class_weight='balanced',\n                                   random_state=RANDOM_STATE)\nlr_weighted.fit(X_tr_proc, y_tr)\nresults_imb['Class weights'] = lr_weighted.predict(X_te_proc)\n\n# 3. SMOTE: synthesise minority class samples\nsmote = SMOTE(random_state=RANDOM_STATE)\nX_tr_sm, y_tr_sm = smote.fit_resample(X_tr_proc, y_tr)\nprint(f'After SMOTE -- class 0: {(y_tr_sm==0).sum():,}  class 1: {(y_tr_sm==1).sum():,}')\nlr_smote = LogisticRegression(max_iter=500, random_state=RANDOM_STATE)\nlr_smote.fit(X_tr_sm, y_tr_sm)\nresults_imb['SMOTE'] = lr_smote.predict(X_te_proc)\n\n# Compare\nprint()\nfor name, y_pred in results_imb.items():\n    print(f'=== {name} ===')\n    print(classification_report(y_te, y_pred,\n                                 target_names=['Non-Python','Python'],\n                                 zero_division=0))\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 6.6.3 -- Threshold adjustment: precision-recall tradeoff\n#\n# The default threshold of 0.5 is rarely optimal.\n# Plot the precision-recall curve and choose the threshold\n# that maximises F1 for the minority class.\n\nfrom sklearn.metrics import precision_recall_curve, f1_score\n\ny_proba = lr_weighted.predict_proba(X_te_proc)[:, 1]  # P(Python=1)\n\nprecisions, recalls, thresholds = precision_recall_curve(y_te, y_proba)\n\n# F1 at each threshold\nf1_scores = 2 * (precisions[:-1] * recalls[:-1]) / (\n    precisions[:-1] + recalls[:-1] + 1e-9\n)\nbest_thresh_idx = f1_scores.argmax()\nbest_thresh     = thresholds[best_thresh_idx]\nbest_f1         = f1_scores[best_thresh_idx]\n\nfig, axes = plt.subplots(1, 2, figsize=(13, 4))\n\naxes[0].plot(thresholds, precisions[:-1], '#2E75B6', linewidth=2, label='Precision')\naxes[0].plot(thresholds, recalls[:-1],    '#E8722A', linewidth=2, label='Recall')\naxes[0].plot(thresholds, f1_scores,       'green',   linewidth=2, label='F1')\naxes[0].axvline(best_thresh, color='black', linestyle='--', linewidth=1.5,\n                label=f'Best threshold={best_thresh:.2f}')\naxes[0].set_xlabel('Decision threshold')\naxes[0].set_ylabel('Score')\naxes[0].set_title('Precision, Recall, F1 vs Threshold')\naxes[0].legend(fontsize=9)\n\naxes[1].plot(recalls, precisions, '#2E75B6', linewidth=2)\naxes[1].scatter(recalls[best_thresh_idx], precisions[best_thresh_idx],\n                s=100, color='red', zorder=5,\n                label=f'Best F1={best_f1:.3f} @ {best_thresh:.2f}')\naxes[1].set_xlabel('Recall')\naxes[1].set_ylabel('Precision')\naxes[1].set_title('Precision-Recall Curve')\naxes[1].legend(fontsize=9)\n\nplt.suptitle('Threshold Optimisation for Python Usage Classifier',\n             fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Apply best threshold\ny_pred_tuned = (y_proba >= best_thresh).astype(int)\ndefault_f1   = f1_score(y_te, results_imb['Class weights'], average='weighted')\ntuned_f1     = f1_score(y_te, y_pred_tuned, average='weighted')\nprint(f'Default threshold (0.50) weighted F1: {default_f1:.4f}')\nprint(f'Optimised threshold ({best_thresh:.2f}) weighted F1: {tuned_f1:.4f}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 6.7 -- Probability Calibration\n\nA classifier that outputs `predict_proba([...]) = 0.8` is claiming\n80% confidence. A **calibrated** classifier means exactly that:\namong all predictions with confidence ~0.8, roughly 80% are actually correct.\n\nMany models are systematically miscalibrated:\n- **Random Forests** tend to push probabilities toward 0.5 (under-confident)\n- **SVMs and Naive Bayes** often produce extreme probabilities (over-confident)\n\nThe **reliability diagram** (calibration curve) visualises this.\nThe **Brier score** quantifies it: lower is better, 0 is perfect, 0.25 is a\ncoin-flip on a balanced binary problem.\n\nCalibration matters whenever you use probabilities directly -- risk scoring,\ndecision thresholds, or ranking outputs by confidence.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 6.7.1 -- Calibration curves and Brier scores\n\nfrom sklearn.calibration import CalibrationDisplay, CalibratedClassifierCV\nfrom sklearn.metrics import brier_score_loss\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\n# Train three classifiers on the same data\nmodels_cal = {\n    'Logistic Regression': LogisticRegression(max_iter=500, random_state=RANDOM_STATE),\n    'Random Forest':       RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n    'Gradient Boosting':   GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE),\n}\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Perfect calibration reference line\naxes[0].plot([0,1],[0,1], 'k--', linewidth=1.5, label='Perfectly calibrated')\n\ncolours = ['#2E75B6', '#E8722A', 'green']\nbrier_scores = {}\n\nfor (name, model), colour in zip(models_cal.items(), colours):\n    model.fit(X_tr_proc, y_tr)\n    proba = model.predict_proba(X_te_proc)[:, 1]\n    brier = brier_score_loss(y_te, proba)\n    brier_scores[name] = brier\n    CalibrationDisplay.from_predictions(\n        y_te, proba, n_bins=10, ax=axes[0],\n        name=f'{name} (Brier={brier:.3f})', color=colour\n    )\n\naxes[0].set_title('Calibration Curves\\n(closer to diagonal = better calibrated)')\naxes[0].legend(fontsize=8)\n\n# Calibrate Random Forest with isotonic regression\nrf_uncal  = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\nrf_cal    = CalibratedClassifierCV(rf_uncal, method='isotonic', cv=5)\nrf_cal.fit(X_tr_proc, y_tr)\nproba_cal = rf_cal.predict_proba(X_te_proc)[:, 1]\nbrier_cal = brier_score_loss(y_te, proba_cal)\n\nrf_uncal2 = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\nrf_uncal2.fit(X_tr_proc, y_tr)\nproba_uncal = rf_uncal2.predict_proba(X_te_proc)[:, 1]\nbrier_uncal = brier_score_loss(y_te, proba_uncal)\n\naxes[1].plot([0,1],[0,1],'k--',linewidth=1.5, label='Perfectly calibrated')\nCalibrationDisplay.from_predictions(\n    y_te, proba_uncal, n_bins=10, ax=axes[1],\n    name=f'RF uncalibrated (Brier={brier_uncal:.3f})', color='#E8722A'\n)\nCalibrationDisplay.from_predictions(\n    y_te, proba_cal, n_bins=10, ax=axes[1],\n    name=f'RF + isotonic calibration (Brier={brier_cal:.3f})', color='#2E75B6'\n)\naxes[1].set_title('Effect of Isotonic Calibration on Random Forest')\naxes[1].legend(fontsize=9)\n\nplt.suptitle('Probability Calibration: SO 2025 Python Usage Classifier',\n             fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint('Brier scores (lower = better, 0.25 = coin flip):')\nfor name, score in brier_scores.items():\n    print(f'  {name:<25} {score:.4f}')\nprint(f'  RF + calibration        {brier_cal:.4f}  (vs uncalibrated {brier_uncal:.4f})')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 6.8 -- FeatureUnion: Combining Feature Sets\n\n`ColumnTransformer` applies different transformers to different columns\nand concatenates the results. `FeatureUnion` does something complementary:\nit applies multiple transformers to the **same** input and concatenates their outputs.\n\nThis is useful when you want to extract several different feature representations\nfrom the same data -- for example, combining raw scaled features with\npolynomial features and feature interactions in a single pipeline step.\n\nTogether, `ColumnTransformer` + `FeatureUnion` + `Pipeline` give you complete\ncontrol over feature engineering without any data leakage.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 6.8.1 -- FeatureUnion: combine raw features with polynomial interactions\n\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.decomposition import PCA\n\n# FeatureUnion applies each transformer to the full input\n# and concatenates the results horizontally\nfeature_union = FeatureUnion([\n    # Branch 1: original scaled features\n    ('scaled', StandardScaler()),\n    # Branch 2: degree-2 polynomial features (captures interactions)\n    # e.g. YearsCodePro * uses_python\n    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n])\n\n# Wrap in a full pipeline with a classifier\nunion_pipeline = ImbPipeline([\n    ('imputer',  SimpleImputer(strategy='median')),\n    ('features', feature_union),\n    ('pca',      PCA(n_components=0.95)),  # keep 95% of variance\n    ('clf',      LogisticRegression(max_iter=500, class_weight='balanced',\n                                     random_state=RANDOM_STATE)),\n])\n\nunion_pipeline.fit(X_tr, y_tr)\ny_pred_union = union_pipeline.predict(X_te)\n\n# Show how many features each stage produces\nimp_out   = SimpleImputer().fit_transform(X_tr)\nunion_out = feature_union.fit_transform(imp_out)\npca_temp  = PCA(n_components=0.95).fit(union_out)\n\nprint('Feature pipeline stages:')\nprint(f'  Input features:              {X_tr.shape[1]}')\nprint(f'  After FeatureUnion:')\nprint(f'    Branch 1 (scaled):         {X_tr.shape[1]}')\nn_poly = PolynomialFeatures(degree=2, include_bias=False).fit(imp_out).n_output_features_\nprint(f'    Branch 2 (polynomial):     {n_poly}')\nprint(f'    Combined:                  {union_out.shape[1]}')\nprint(f'  After PCA (95% variance):    {pca_temp.n_components_}')\nprint()\n\nfrom sklearn.metrics import f1_score as f1\nbase_f1  = f1(y_te, results_imb['Class weights'], average='weighted')\nunion_f1 = f1(y_te, y_pred_union, average='weighted')\nprint(f'Logistic Regression (raw features):         F1={base_f1:.4f}')\nprint(f'Logistic Regression (FeatureUnion + PCA):   F1={union_f1:.4f}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Chapter 6 Summary\n\nChapter 6 is the core ML chapter. Every concept here recurs in Chapter 7\n(deep learning) -- the difference is that neural networks learn their own\nfeatures rather than using the hand-engineered ones we built here.\n\n### Key Takeaways\n\n- **The scikit-learn API contract:** `fit()`, `transform()`, `predict()`, `score()`.\n  Every estimator and transformer follows it.\n- **Pipeline** prevents data leakage and makes the full workflow serialisable.\n- **ColumnTransformer** applies different preprocessing to numeric vs categorical columns.\n- **Cross-validation** gives honest performance estimates. Never evaluate on training data.\n- **Class imbalance** distorts accuracy. Use `class_weight='balanced'`, SMOTE,\n  or threshold adjustment depending on the severity and your metric priorities.\n- **Threshold optimisation** lets you trade precision for recall explicitly.\n  The precision-recall curve shows the full tradeoff; pick the threshold that\n  maximises your actual business metric.\n- **Calibration** ensures predicted probabilities are trustworthy.\n  `CalibratedClassifierCV` with isotonic regression fixes Random Forest miscalibration.\n  The Brier score quantifies calibration quality.\n- **FeatureUnion** combines multiple feature representations from the same input.\n  Pair it with `ColumnTransformer` and `Pipeline` for complete feature engineering control.\n- **Learning curves** diagnose bias vs variance -- the most actionable diagnostic.\n- **RandomizedSearchCV** is more efficient than GridSearch for large hyperparameter spaces.\n\n### Project Thread Status\n\n| Task | Model | Key Result |\n|------|-------|------------|\n| Salary regression | Random Forest | Reported R^2, MAE |\n| Python usage classification | Best of 4 models | Accuracy + confusion matrix |\n| Hyperparameter tuning | RandomizedSearchCV | Improved accuracy |\n| Developer clustering | KMeans + PCA | Archetypes identified |\n| Class imbalance | SMOTE + class weights | Per-class F1 comparison |\n| Threshold optimisation | Precision-recall curve | Best F1 threshold found |\n| Calibration | Isotonic CalibratedClassifierCV | Brier score improved |\n| Feature engineering | FeatureUnion + polynomial | Feature count comparison |\n\n---\n\n### What's Next: Chapter 7 -- Deep Learning with PyTorch\n\nChapter 7 introduces neural networks. You will build a salary regression MLP\nand a developer role classifier from scratch in PyTorch, understanding every\ncomponent: tensors, layers, activation functions, loss functions, and the\ntraining loop. The scikit-learn patterns from this chapter appear again --\nnow implemented manually so you understand what the framework does for you.\n\n---\n\n*End of Chapter 6 -- Python for AI/ML*  \n[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n"
    }
  ]
}