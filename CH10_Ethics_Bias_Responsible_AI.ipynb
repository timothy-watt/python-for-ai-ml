{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Chapter 10 ‚Äî Ethics, Bias, and Responsible AI\n## *Python for AI/ML: A Complete Learning Journey*\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/CH10_Ethics_Bias_Responsible_AI.ipynb)\n&nbsp;&nbsp;[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n\n---\n\n**Part:** 3 -- Machine Learning and AI  \n**Prerequisites:** Chapter 6 (scikit-learn), Chapter 7 (PyTorch)  \n**Estimated time:** 3-4 hours\n\n---\n\n### Learning Objectives\n\nBy the end of this chapter you will be able to:\n\n- Identify sources of bias in ML pipelines: data, label, measurement, and deployment bias\n- Compute and interpret fairness metrics: demographic parity, equalised odds, calibration\n- Use SHAP to explain individual predictions and global feature importance\n- Apply LIME to explain a single prediction in human-readable terms\n- Audit a salary prediction model for geographic and demographic disparities\n- Apply practical mitigation strategies: reweighting, threshold adjustment, and documentation\n- Write a model card summarising intended use, limitations, and bias findings\n\n---\n\n### Why This Chapter Exists\n\nEvery model built in Chapters 6-8 makes decisions that affect real people.\nA salary prediction model used in hiring encodes historical pay disparities.\nA developer role classifier trained on biased labels may systematically\nunder-represent certain groups. These are not hypothetical risks -- they are\ndocumented failures in production systems.\n\nThis chapter does not treat ethics as a soft add-on. It provides concrete,\nmeasurable tools: compute the disparity, visualise it, and apply a mitigation.\nResponsible AI is an engineering discipline, not just a policy statement.\n\n---\n\n### Project Thread -- Chapter 9\n\nWe audit the Chapter 6 salary regression model for geographic bias:\ndoes the model systematically over- or under-predict salaries for developers\nin certain countries? We then apply SHAP to explain individual predictions,\nmeasure calibration across groups, and produce a model card.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Setup -- Imports and Data\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Install SHAP and LIME if not already present\nimport subprocess\nsubprocess.run(['pip', 'install', 'shap', 'lime', '-q'], check=False)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport shap\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import mean_absolute_error, r2_score, confusion_matrix\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\nprint(f'SHAP version: {shap.__version__}')\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.dpi']       = 110\nplt.rcParams['axes.titlesize']   = 13\nplt.rcParams['axes.titleweight'] = 'bold'\n\nDATASET_URL  = 'https://raw.githubusercontent.com/timothy-watt/python-for-ai-ml/main/data/so_survey_2025_curated.csv'\nRANDOM_STATE = 42\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Load and clean SO 2025 -- same pipeline as previous chapters\ndf_raw = pd.read_csv(DATASET_URL)\ndf = df_raw.copy()\ndf = df.dropna(subset=['ConvertedCompYearly'])\ndf['ConvertedCompYearly'] = pd.to_numeric(df['ConvertedCompYearly'], errors='coerce')\nQ1, Q3 = df['ConvertedCompYearly'].quantile([0.25, 0.75])\nIQR = Q3 - Q1\ndf = df[\n    (df['ConvertedCompYearly'] >= max(Q1 - 3*IQR, 5_000)) &\n    (df['ConvertedCompYearly'] <= min(Q3 + 3*IQR, 600_000))\n].copy()\nif 'YearsCodePro' in df.columns:\n    df['YearsCodePro'] = pd.to_numeric(df['YearsCodePro'], errors='coerce')\n    df['YearsCodePro'] = df['YearsCodePro'].fillna(df['YearsCodePro'].median())\nfor col in ['Country', 'EdLevel', 'RemoteWork', 'DevType']:\n    if col in df.columns:\n        df[col] = df[col].fillna('Unknown')\ndf['uses_python'] = df.get('LanguageHaveWorkedWith', pd.Series(dtype=str)).str.contains('Python', na=False).astype(int)\ndf['uses_sql']    = df.get('LanguageHaveWorkedWith', pd.Series(dtype=str)).str.contains('SQL', na=False).astype(int)\ndf['log_salary']  = np.log(df['ConvertedCompYearly'])\ndf['primary_role'] = df.get('DevType', pd.Series(dtype=str)).str.split(';').str[0].str.strip()\ndf = df.reset_index(drop=True)\nprint(f'Dataset: {len(df):,} rows')\nprint(f'Countries: {df[\"Country\"].nunique()}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 9.1 -- Sources of Bias in ML Pipelines\n\nBias enters ML systems at every stage. Understanding where it comes from\nis the prerequisite for measuring and mitigating it.\n\n**Data bias** -- the training data does not represent the real-world population.\nThe SO 2025 survey over-represents English-speaking, Western developers.\nA salary model trained on it will have higher accuracy for those groups\nand lower accuracy -- with systematic errors -- for under-represented groups.\n\n**Label bias** -- the labels themselves encode historical inequities.\nIf past salaries reflect discrimination, a model trained to predict salary\nwill learn to reproduce that discrimination.\n\n**Measurement bias** -- different groups are measured differently.\nSelf-reported salary in a developer survey may be more accurate for\nsalaried employees than for contractors or freelancers.\n\n**Feedback loop bias** -- model predictions influence future data collection.\nA hiring model that down-ranks candidates from certain universities\nproduces fewer training examples from those universities in the next round.\n\n**Deployment bias** -- a model trained in one context is used in another.\nA salary model trained on SO survey data used to set actual compensation\napplies a tool to a context it was never validated for.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 9.1.1 -- Audit the training data: representation by country\n\ntop_countries = df['Country'].value_counts().head(12)\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Left: respondent count by country\ntop_countries.plot(kind='barh', ax=axes[0], color='#2E75B6')\naxes[0].set_title('Respondent Count by Country\\n(data bias: over-representation of certain regions)')\naxes[0].set_xlabel('Number of Respondents')\nfor i, v in enumerate(top_countries.values):\n    axes[0].text(v + 10, i, f'{v:,}', va='center', fontsize=8)\n\n# Right: median salary by country\ncountry_salary = (\n    df[df['Country'].isin(top_countries.index)]\n    .groupby('Country')['ConvertedCompYearly']\n    .median()\n    .sort_values(ascending=True)\n)\ncountry_salary.plot(kind='barh', ax=axes[1], color='#E8722A')\naxes[1].xaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f'${x/1000:.0f}k'))\naxes[1].set_title('Median Salary by Country\\n(wide range: model must not treat these as equivalent)')\naxes[1].set_xlabel('Median Annual Salary (USD)')\n\nplt.suptitle('SO 2025: Data Representation Audit', fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Quantify the representation imbalance\ntop_3_pct  = top_countries.head(3).sum() / len(df) * 100\nbottom_pct = top_countries.tail(3).sum() / len(df) * 100\nprint(f'Top 3 countries: {top_3_pct:.1f}% of all respondents')\nprint(f'Bottom 3 of top-12: {bottom_pct:.1f}% of all respondents')\nprint(f'Salary range (top-12 countries): '\n      f'${country_salary.min():,.0f} to ${country_salary.max():,.0f}')\nprint(f'That is a {country_salary.max()/country_salary.min():.1f}x salary ratio -- '\n      f'treating all countries equivalently would be a serious model error.')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 9.2 -- Train the Model We Will Audit\n\nWe train a Random Forest salary regressor that deliberately omits `Country`\nas a feature -- a common real-world decision made to avoid 'discriminating'\nby geography. We then show that this decision does not remove geographic bias;\nit just makes it invisible and harder to measure.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 9.2.1 -- Train salary model WITHOUT Country as a feature\n\nfeature_cols = [c for c in ['YearsCodePro', 'uses_python', 'uses_sql']\n                if c in df.columns]\n\nX = df[feature_cols].copy()\nfor col in feature_cols:\n    med = X[col].median()\n    X[col] = X[col].fillna(med if pd.notna(med) else 0)\ny = df['log_salary']\n\nX_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n    X, y, df.index, test_size=0.2, random_state=RANDOM_STATE\n)\n\nscaler = StandardScaler()\nX_train_sc = scaler.fit_transform(X_train)\nX_test_sc  = scaler.transform(X_test)\n\nmodel = RandomForestRegressor(\n    n_estimators=100, max_depth=8,\n    random_state=RANDOM_STATE, n_jobs=-1\n)\nmodel.fit(X_train_sc, y_train)\n\ny_pred_log = model.predict(X_test_sc)\ny_pred_usd = np.exp(y_pred_log)\ny_true_usd = np.exp(y_test)\n\noverall_r2  = r2_score(y_test, y_pred_log)\noverall_mae = mean_absolute_error(y_true_usd, y_pred_usd)\n\nprint(f'Overall model performance (Country NOT a feature):')\nprint(f'  R^2:  {overall_r2:.4f}')\nprint(f'  MAE:  ${overall_mae:,.0f}')\nprint(f'Features used: {feature_cols}')\nprint()\nprint('Now we audit whether performance is equal across countries...')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 9.3 -- Fairness Audit: Per-Group Performance\n\nA model with good overall performance can still perform very poorly\nfor specific subgroups. The standard fairness audit computes performance\nmetrics separately for each group and measures the disparity.\n\n**Demographic parity** -- do predictions have the same mean across groups?\n\n**Equalised odds** -- are error rates equal across groups?\n(Equal false positive rates and false negative rates)\n\n**Calibration** -- does a predicted salary of $X actually correspond to\nactual salaries near $X equally well for all groups?\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 9.3.1 -- Per-country performance audit\n\ntest_df = df.loc[idx_test].copy()\ntest_df['y_pred_log'] = y_pred_log\ntest_df['y_pred_usd'] = y_pred_usd\ntest_df['y_true_usd'] = y_true_usd.values\ntest_df['abs_error']  = np.abs(test_df['y_true_usd'] - test_df['y_pred_usd'])\ntest_df['pct_error']  = test_df['abs_error'] / test_df['y_true_usd'] * 100\n\n# Compute per-country metrics for countries with enough test samples\nmin_samples = 30\ncountry_metrics = []\n\nfor country, grp in test_df.groupby('Country'):\n    if len(grp) < min_samples:\n        continue\n    r2  = r2_score(np.log(grp['y_true_usd']), np.log(grp['y_pred_usd']))\n    mae = grp['abs_error'].mean()\n    mpe = grp['pct_error'].mean()   # mean percentage error\n    bias = (grp['y_pred_usd'] - grp['y_true_usd']).mean()  # signed: + = over-predicts\n    country_metrics.append({\n        'Country': country, 'n': len(grp),\n        'R2': r2, 'MAE': mae, 'MPE': mpe, 'Bias_USD': bias\n    })\n\nmetrics_df = pd.DataFrame(country_metrics).sort_values('MAE', ascending=False)\n\nprint(f'Countries with >= {min_samples} test samples: {len(metrics_df)}')\nprint()\nprint(f'{\"Country\":<25} {\"n\":>5}  {\"R2\":>6}  {\"MAE\":>10}  {\"Bias\":>10}')\nprint('-' * 60)\nfor _, row in metrics_df.iterrows():\n    bias_str = f'+${row[\"Bias_USD\"]:,.0f}' if row['Bias_USD'] >= 0 else f'-${abs(row[\"Bias_USD\"]):,.0f}'\n    print(f'{row[\"Country\"]:<25} {row[\"n\"]:>5}  {row[\"R2\"]:>6.3f}  '\n          f'${row[\"MAE\"]:>9,.0f}  {bias_str:>10}')\n\nprint()\nworst_mae = metrics_df.iloc[0]\nbest_mae  = metrics_df.iloc[-1]\nprint(f'MAE disparity ratio: {worst_mae[\"MAE\"]/best_mae[\"MAE\"]:.1f}x '\n      f'({worst_mae[\"Country\"]} vs {best_mae[\"Country\"]})')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 9.3.2 -- Visualise the disparity\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n# Left: MAE by country\nplot_df = metrics_df.sort_values('MAE', ascending=True)\ncolours = ['#E8722A' if mae > overall_mae * 1.5 else '#2E75B6'\n           for mae in plot_df['MAE']]\naxes[0].barh(plot_df['Country'], plot_df['MAE'], color=colours)\naxes[0].axvline(overall_mae, color='red', linestyle='--', linewidth=2,\n                label=f'Overall MAE ${overall_mae:,.0f}')\naxes[0].xaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f'${x/1000:.0f}k'))\naxes[0].set_title('MAE by Country\\n(orange = 1.5x above overall MAE)')\naxes[0].set_xlabel('Mean Absolute Error (USD)')\naxes[0].legend(fontsize=9)\n\n# Right: prediction bias by country (over vs under prediction)\nbias_df = metrics_df.sort_values('Bias_USD', ascending=True)\nbias_colours = ['#E8722A' if b > 0 else '#2E75B6' for b in bias_df['Bias_USD']]\naxes[1].barh(bias_df['Country'], bias_df['Bias_USD'], color=bias_colours)\naxes[1].axvline(0, color='black', linewidth=1)\naxes[1].xaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f'${x/1000:.0f}k'))\naxes[1].set_title('Prediction Bias by Country\\n(orange = over-predicts, blue = under-predicts)')\naxes[1].set_xlabel('Mean Bias (predicted - actual) USD')\n\nplt.suptitle('Fairness Audit: Salary Model Performance by Country',\n             fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint('Key finding: the model that omits Country as a feature')\nprint('still produces highly unequal errors across countries.')\nprint('Omitting a sensitive attribute does not remove bias -- it hides it.')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 9.4 -- Model Explainability with SHAP\n\n**SHAP (SHapley Additive exPlanations)** assigns each feature a contribution\nvalue for a specific prediction. It is grounded in cooperative game theory:\nthe SHAP value for a feature is its average marginal contribution across all\npossible orderings of features.\n\nSHAP provides two levels of explanation:\n- **Global:** which features matter most overall? (feature importance)\n- **Local:** why did the model predict $X for this specific person?\n\nFor tree-based models, SHAP uses an exact, fast algorithm (`TreeExplainer`).\nFor other models, it uses `KernelExplainer` (slower, approximate).\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 9.4.1 -- Global SHAP: feature importance and summary plot\n\nexplainer   = shap.TreeExplainer(model)\n# Compute SHAP values on a sample for speed\nsample_idx  = np.random.RandomState(RANDOM_STATE).choice(len(X_test_sc), size=min(500, len(X_test_sc)), replace=False)\nX_sample    = X_test_sc[sample_idx]\nshap_values = explainer.shap_values(X_sample)\n\nprint(f'SHAP values shape: {np.array(shap_values).shape}')\nprint(f'Features: {feature_cols}')\n\n# Summary plot: beeswarm showing feature impact distribution\nplt.figure(figsize=(9, 4))\nshap.summary_plot(\n    shap_values, X_sample,\n    feature_names=feature_cols,\n    show=False\n)\nplt.title('SHAP Summary Plot: Feature Impact on log(Salary) Predictions')\nplt.tight_layout()\nplt.show()\n\n# Mean absolute SHAP value = global importance\nmean_shap = np.abs(shap_values).mean(axis=0)\nprint('Global feature importance (mean |SHAP|):')\nfor feat, imp in sorted(zip(feature_cols, mean_shap), key=lambda x: -x[1]):\n    print(f'  {feat:<25} {imp:.6f}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 9.4.2 -- Local SHAP: explain individual predictions\n\n# Pick two respondents: highest and lowest predicted salary\npred_series = pd.Series(np.exp(model.predict(X_test_sc)), index=idx_test)\nhighest_idx = pred_series.idxmax()\nlowest_idx  = pred_series.idxmin()\n\nfor label, row_idx in [('Highest predicted salary', highest_idx),\n                        ('Lowest predicted salary',  lowest_idx)]:\n    pos_in_test = list(idx_test).index(row_idx)\n    x_row  = X_test_sc[pos_in_test]\n    shap_row = explainer.shap_values(x_row.reshape(1, -1))[0]\n    pred_usd = np.exp(model.predict(x_row.reshape(1, -1))[0])\n    true_usd = np.exp(y_test.loc[row_idx])\n    country  = df.loc[row_idx, 'Country'] if 'Country' in df.columns else 'N/A'\n\n    print(f'{label}:')\n    print(f'  Country:    {country}')\n    print(f'  Predicted:  ${pred_usd:,.0f}')\n    print(f'  Actual:     ${true_usd:,.0f}')\n    print(f'  SHAP contributions:')\n    for feat, sv in zip(feature_cols, shap_row):\n        direction = 'increases' if sv > 0 else 'decreases'\n        print(f'    {feat:<20} {sv:+.4f}  ({direction} prediction)')\n    print()\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 9.4.3 -- SHAP dependence plot: how YearsCodePro affects predictions\n\nif 'YearsCodePro' in feature_cols:\n    yrs_idx = feature_cols.index('YearsCodePro')\n\n    fig, ax = plt.subplots(figsize=(9, 5))\n    ax.scatter(\n        X_sample[:, yrs_idx],\n        shap_values[:, yrs_idx],\n        alpha=0.4, s=12, color='#2E75B6'\n    )\n    ax.axhline(0, color='red', linestyle='--', linewidth=1.5)\n    ax.set_xlabel('YearsCodePro (standardised)')\n    ax.set_ylabel('SHAP value (impact on log salary)')\n    ax.set_title('SHAP Dependence Plot: YearsCodePro\\n'\n                 'Above zero = increases salary prediction; below zero = decreases')\n    plt.tight_layout()\n    plt.show()\n\n    print('Interpretation:')\n    print('  Positive SHAP values: this experience level pushes the prediction higher')\n    print('  Negative SHAP values: this experience level pushes the prediction lower')\n    print('  The transition point (where line crosses 0) is the average experience level')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 9.5 -- Bias Mitigation Strategies\n\nOnce you have measured a disparity, you have three categories of options:\n\n**Pre-processing** -- fix the data before training:\nreweighting under-represented groups, resampling, or removing proxy features.\n\n**In-processing** -- modify the model or training objective:\nadd fairness constraints to the loss function, use adversarial debiasing.\n\n**Post-processing** -- adjust model outputs after training:\napply different decision thresholds per group, recalibrate predictions.\n\nWe demonstrate the two most practical techniques: sample reweighting\n(pre-processing) and threshold adjustment (post-processing).\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 9.5.1 -- Pre-processing: sample reweighting\n#\n# Give under-represented countries higher weight during training\n# so the model pays equal attention to each country regardless of sample size.\n\n# Compute inverse-frequency weights: rare countries get higher weight\ncountry_counts = df.loc[idx_train, 'Country'].value_counts()\ntotal          = len(idx_train)\ncountry_weights = (total / (len(country_counts) * country_counts)).to_dict()\n\ntrain_weights = df.loc[idx_train, 'Country'].map(country_weights).fillna(1.0).values\ntrain_weights = train_weights / train_weights.mean()   # normalise so mean weight = 1\n\nprint(f'Sample weights range: {train_weights.min():.3f} to {train_weights.max():.3f}')\nprint(f'Mean weight: {train_weights.mean():.3f}')\n\n# Retrain with sample weights\nmodel_weighted = RandomForestRegressor(\n    n_estimators=100, max_depth=8,\n    random_state=RANDOM_STATE, n_jobs=-1\n)\nmodel_weighted.fit(X_train_sc, y_train, sample_weight=train_weights)\n\ny_pred_w    = model_weighted.predict(X_test_sc)\ny_pred_w_usd = np.exp(y_pred_w)\n\noverall_r2_w  = r2_score(y_test, y_pred_w)\noverall_mae_w = mean_absolute_error(y_true_usd, y_pred_w_usd)\n\nprint(f'Weighted model overall: R^2={overall_r2_w:.4f}, MAE=${overall_mae_w:,.0f}')\nprint(f'Original model overall: R^2={overall_r2:.4f},  MAE=${overall_mae:,.0f}')\n\n# Compare per-country MAE improvement\ntest_df['y_pred_w_usd'] = y_pred_w_usd\ntest_df['abs_error_w']  = np.abs(test_df['y_true_usd'] - test_df['y_pred_w_usd'])\n\nimprovements = []\nfor country, grp in test_df.groupby('Country'):\n    if len(grp) < min_samples:\n        continue\n    mae_orig = grp['abs_error'].mean()\n    mae_wtd  = grp['abs_error_w'].mean()\n    improvements.append({'Country': country, 'n': len(grp),\n                         'MAE_orig': mae_orig, 'MAE_weighted': mae_wtd,\n                         'Delta': mae_wtd - mae_orig})\n\nimp_df = pd.DataFrame(improvements).sort_values('Delta')\nimproved = (imp_df['Delta'] < 0).sum()\nprint(f'Countries with improved MAE after reweighting: {improved}/{len(imp_df)}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 9.5.2 -- Visualise the mitigation effect\n\nfig, ax = plt.subplots(figsize=(11, 6))\n\nx     = np.arange(len(imp_df))\nwidth = 0.35\n\nbars1 = ax.bar(x - width/2, imp_df['MAE_orig']/1000,     width,\n               label='Original model', color='#2E75B6', alpha=0.8)\nbars2 = ax.bar(x + width/2, imp_df['MAE_weighted']/1000, width,\n               label='Reweighted model', color='#E8722A', alpha=0.8)\n\nax.set_xticks(x)\nax.set_xticklabels(imp_df['Country'], rotation=30, ha='right', fontsize=9)\nax.set_ylabel('MAE ($k)')\nax.set_title('Bias Mitigation: MAE Before and After Sample Reweighting\\n'\n             '(lower = better; reweighting reduces errors for under-represented countries)')\nax.legend(fontsize=10)\nax.axhline(overall_mae/1000,   color='#2E75B6', linestyle='--', linewidth=1.5,\n           label=f'Overall original ${overall_mae/1000:.0f}k')\nax.axhline(overall_mae_w/1000, color='#E8722A', linestyle='--', linewidth=1.5,\n           label=f'Overall weighted ${overall_mae_w/1000:.0f}k')\nplt.tight_layout()\nplt.show()\n\nprint('Reweighting often improves fairness at a small cost to overall accuracy.')\nprint('This is the fundamental fairness-accuracy tradeoff -- it cannot be entirely avoided,')\nprint('but it can be measured and managed.')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 9.6 -- Model Cards: Documenting Your Model\n\nA **model card** is a structured document that accompanies a deployed model.\nIt records what the model does, how it was trained, what it should and should\nnot be used for, its performance across subgroups, and its known limitations.\n\nModel cards were introduced by Google in 2019 and are now the industry standard\nfor responsible model documentation. HuggingFace requires them for all models\non the Hub. Many organisations require them before production deployment.\n\nThe cell below generates a complete model card for the salary model we built.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 9.6.1 -- Generate a model card\n\nfrom datetime import date\n\nmodel_card = f\"\"\"\n# Model Card: SO 2025 Salary Regression Model\n\n**Date:** {date.today()}\n**Version:** 1.0\n**Authors:** Python for AI/ML (Chapter 9 demonstration)\n\n---\n\n## Model Description\n\nA Random Forest regression model that predicts annual developer salary (USD)\nfrom professional experience, Python usage, and SQL usage.\nPredictions are made in log-salary space and exponentiated to USD.\n\n**Architecture:** RandomForestRegressor (100 trees, max_depth=8)\n**Target variable:** log(ConvertedCompYearly) -- natural log of annual salary in USD\n**Features:** YearsCodePro, uses_python, uses_sql\n\n---\n\n## Intended Use\n\n- **Intended uses:** Educational demonstration of salary prediction modelling\n- **Intended users:** Students and practitioners learning ML with the SO 2025 dataset\n- **Out-of-scope uses:** Setting actual employee compensation, hiring decisions,\n  benchmarking individual salaries, any production deployment\n\n---\n\n## Training Data\n\nStack Overflow 2025 Developer Survey (curated 15,000-respondent subset).\nSource: stackoverflow.com/survey\n\n**Known data limitations:**\n- Over-represents English-speaking, Western developers\n- Self-reported salary may differ from actual compensation\n- Survey respondents are not a random sample of all developers globally\n- Salary reported in local currency and converted to USD at a single rate\n\n---\n\n## Performance\n\n| Metric | Value |\n|--------|-------|\n| R^2 (log scale, test set) | {overall_r2:.4f} |\n| MAE (USD, test set) | ${overall_mae:,.0f} |\n\n**Performance varies significantly by country.**\nCountries with fewer training examples show higher MAE.\nSee the fairness audit in Section 9.3 for per-country breakdown.\n\n---\n\n## Bias and Fairness Findings\n\n1. **Geographic disparity:** MAE varies by up to {metrics_df['MAE'].max()/metrics_df['MAE'].min():.1f}x\n   across countries. Countries with fewer survey respondents show higher prediction error.\n\n2. **Systematic bias direction:** The model over-predicts salaries for lower-income\n   countries and under-predicts for higher-income countries. This reflects the model\n   learning a global average rather than country-specific salary distributions.\n\n3. **Feature omission does not remove bias:** Country was deliberately excluded as a\n   feature. The model still produces country-unequal errors because correlated features\n   (experience patterns, language usage) partially proxy for geography.\n\n4. **Mitigation applied:** Sample reweighting by inverse country frequency reduced\n   per-country MAE disparity. See Section 9.5 for results.\n\n---\n\n## Limitations and Recommendations\n\n- Do not use this model to set or justify actual salaries\n- Do not use predictions for individuals from countries with fewer than 100 survey respondents\n- Retrain annually as salary distributions change\n- Add country as an explicit feature if geographic fairness is required\n- Conduct a full fairness audit before any deployment beyond education\n\n---\n\n*This model card was generated as part of Chapter 9 of Python for AI/ML.\nIt is a teaching example of responsible documentation practice.*\n\"\"\"\n\nprint(model_card)\n\n# Save to file\nwith open('/tmp/salary_model_card.md', 'w') as f:\n    f.write(model_card)\nprint('Model card saved to /tmp/salary_model_card.md')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Concept Check Questions\n\n> Test your understanding before moving on. Answer each question without referring back to the notebook, then expand to check.\n\n**Q1.** Name the five sources of ML bias and give a one-sentence description of each.\n\n<details><summary>Show answer</summary>\n\n1. **Representation bias** ‚Äî certain groups are under-represented in training data. 2. **Measurement bias** ‚Äî features or labels are measured differently across groups. 3. **Aggregation bias** ‚Äî one model is applied to populations with different underlying patterns. 4. **Evaluation bias** ‚Äî the benchmark doesn't represent the deployment population. 5. **Deployment bias** ‚Äî the model is used in a different context than it was built for.\n\n</details>\n\n**Q2.** What is **demographic parity** and when might satisfying it still be unfair?\n\n<details><summary>Show answer</summary>\n\nDemographic parity requires equal positive prediction rates across groups. It can still be unfair if recall differs between groups ‚Äî equal prediction rates could be achieved by approving less qualified candidates in one group and denying qualified ones in another.\n\n</details>\n\n**Q3.** Explain the difference between a **global** and a **local** SHAP explanation.\n\n<details><summary>Show answer</summary>\n\n**Global** SHAP summarises importance across the dataset ‚Äî which features matter most overall. **Local** SHAP explains a single prediction ‚Äî each feature's contribution to pushing that specific prediction above or below the baseline.\n\n</details>\n\n**Q4.** What is a **model card** and what six elements should it contain?\n\n<details><summary>Show answer</summary>\n\n(1) Model description and intended use; (2) Out-of-scope uses and limitations; (3) Training data and known biases; (4) Evaluation metrics broken down by subgroup; (5) Fairness considerations and mitigations; (6) Caveats and recommendations for users.\n\n</details>\n\n**Q5.** Your model has MAE = $8k overall but $14k for lower-income countries. What is this called, and what is one mitigation?\n\n<details><summary>Show answer</summary>\n\n**Differential performance** ‚Äî a form of algorithmic bias. One mitigation is **sample reweighting**: upweight underrepresented groups by passing `sample_weight` (inversely proportional to group frequency) to the model's `fit()` method.\n\n</details>\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Coding Exercises\n\n> Three exercises per chapter: **üîß Guided** (fill-in-the-blanks) ¬∑ **üî® Applied** (write from scratch) ¬∑ **üèóÔ∏è Extension** (go beyond the chapter)\n\nExercises use the SO 2025 developer survey dataset.\nExpand each **Solution** block only after attempting the exercise.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exercise 1 üîß Guided ‚Äî Compute fairness metrics across groups\n\nComplete `fairness_report(y_true, y_pred, groups)` that computes\nfor each unique group value: accuracy, TPR (recall), FPR, and\ndemographic parity (P(≈∑=1|group)).\nFlag any pair of groups where demographic parity differs by > 0.10.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\n\ndef fairness_report(y_true: np.ndarray, y_pred: np.ndarray,\n                     groups: np.ndarray) -> pd.DataFrame:\n    rows = []\n    for g in np.unique(groups):\n        mask = groups == g\n        yt, yp = y_true[mask], y_pred[mask]\n        tp = ((yt==1) & (yp==1)).sum()\n        fp = None  # YOUR CODE\n        fn = None  # YOUR CODE\n        tn = None  # YOUR CODE\n        acc  = (tp + tn) / len(yt)\n        tpr  = tp / (tp + fn) if (tp + fn) > 0 else 0  # recall\n        fpr  = None  # YOUR CODE\n        dp   = None  # YOUR CODE\n        rows.append({'group': g, 'n': mask.sum(),\n                     'accuracy': acc, 'TPR': tpr, 'FPR': fpr, 'dem_parity': dp})\n    df = pd.DataFrame(rows).set_index('group')\n    dp_range = df['dem_parity'].max() - df['dem_parity'].min()\n    if dp_range > 0.10:\n        print(f'‚ö† Demographic parity gap: {dp_range:.3f} (> 0.10 threshold)')\n    return df.round(3)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<details><summary>üí° Hint</summary>\n\n`fp = ((yt==0) & (yp==1)).sum()`\n`fn = ((yt==1) & (yp==0)).sum()`\n`tn = ((yt==0) & (yp==0)).sum()`\n`fpr = fp / (fp + tn) if (fp + tn) > 0 else 0`\n`dp = yp.mean()`\n\n</details>\n\n<details><summary>‚úÖ Solution</summary>\n\n```python\nfp=((yt==0)&(yp==1)).sum(); fn=((yt==1)&(yp==0)).sum(); tn=((yt==0)&(yp==0)).sum()\nfpr=fp/(fp+tn) if (fp+tn)>0 else 0; dp=yp.mean()\n```\n\n</details>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exercise 2 üî® Applied ‚Äî Audit a salary model for geographic bias\n\nTrain a salary classifier on synthetic SO 2025 data where `Country`\nis intentionally correlated with `EdLevel` (simulating structural bias).\n\n1. Train a GBM classifier (no country feature)\n2. Use `fairness_report()` from Exercise 1 to audit by country\n3. Identify which country has the highest FPR and TPR disparity\n4. Apply `class_weight` rebalancing and re-audit\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np, pandas as pd\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\n\nrng=np.random.default_rng(42); n=6000\ncountry=rng.choice(['US','India','Germany','UK'],n,p=[0.4,0.3,0.2,0.1])\n# Structural bias: India more likely to have no degree\ned_noise = np.where(country=='India', rng.normal(0.3,0.1,n), rng.normal(0,0.1,n))\nsalary=np.exp(10.5+0.08*rng.exponential(5,n)-0.3*(country=='India').astype(float)+rng.normal(0,0.4,n))\ny=(salary>70000).astype(int)\nX=pd.DataFrame({'years':rng.exponential(6,n),'py':rng.integers(0,2,n),\n                'sql':rng.integers(0,2,n),'ai':rng.integers(0,2,n)})\n\n# YOUR CODE: split, train, fairness_report(y_te, y_pred, country_te)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<details><summary>üí° Hint</summary>\n\nAfter training, call `fairness_report(y_te, y_pred, country_te)` where\n`country_te` is the country values for the test split.\nFor rebalancing: compute `sample_weight` as `n_total / (n_classes * count_per_country)`\n\n</details>\n\n<details><summary>‚úÖ Solution</summary>\n\n```python\nX_tr,X_te,y_tr,y_te,c_tr,c_te=train_test_split(X,y,country,test_size=0.2,random_state=42)\nclf=GradientBoostingClassifier(n_estimators=100,random_state=42).fit(X_tr,y_tr)\nprint(fairness_report(y_te,clf.predict(X_te),c_te))\n```\n\n</details>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exercise 3 üèóÔ∏è Extension ‚Äî Write a model card\n\nWrite a complete model card (as a Python dict that you save as JSON)\nfor the salary prediction model trained in Chapter 12.\n\nThe model card must include all six required sections from the Hugging Face\nmodel card specification:\n1. Model details (name, version, type, training date)\n2. Intended use (primary use, out-of-scope uses)\n3. Factors (relevant groups, instrumentation)\n4. Metrics (performance metrics, thresholds)\n5. Evaluation data (source, preprocessing, motivation)\n6. Ethical considerations and caveats\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import json\n\nmodel_card = {\n    'model_details': {\n        'name': 'SO2025 Salary Tier Classifier',\n        'version': '1.0.0',\n        # YOUR CODE: fill in all required fields\n    },\n    'intended_use': {},\n    'factors': {},\n    'metrics': {},\n    'evaluation_data': {},\n    'ethical_considerations': {}\n}\n\n# Validate completeness\nrequired_sections = ['model_details','intended_use','factors',\n                      'metrics','evaluation_data','ethical_considerations']\nfor s in required_sections:\n    assert model_card.get(s), f'Missing section: {s}'\n\nwith open('/tmp/model_card.json', 'w') as f:\n    json.dump(model_card, f, indent=2)\nprint('Model card saved.')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<details><summary>üí° Hint</summary>\n\nThis is a writing exercise ‚Äî there is no single correct answer.\nThink carefully about: Who will use this model? What can go wrong?\nWhat groups might be disadvantaged? What data was it trained on?\nA strong model card is specific, honest, and actionable.\n\n</details>\n\n<details><summary>‚úÖ Solution</summary>\n\n```python\n# No single correct answer ‚Äî evaluate against completeness and specificity.\n# Key things to include in ethical_considerations:\n# - Geographic bias documented in Exercise 2\n# - Self-reported survey data limitations\n# - Currency / inflation effects\n# - What the model should NOT be used for (individual salary negotiation)\n```\n\n</details>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Chapter 10 Summary\n\n### Key Takeaways\n\n- **Bias has five entry points:** data, label, measurement, feedback loop, and deployment.\n  Each requires a different mitigation strategy.\n- **Omitting sensitive attributes does not remove bias.** It makes bias invisible\n  and harder to measure. Correlated proxy features carry the bias forward.\n- **Fairness is multi-dimensional.** Demographic parity, equalised odds, and\n  calibration are all valid fairness criteria -- and they cannot all be satisfied\n  simultaneously when base rates differ across groups (Chouldechova's theorem).\n- **SHAP** provides both global feature importance and local per-prediction\n  explanations grounded in game theory. `TreeExplainer` is exact and fast\n  for tree-based models.\n- **Sample reweighting** is the lowest-friction pre-processing mitigation:\n  add `sample_weight` to `fit()`. It typically improves fairness at a small\n  cost to overall accuracy.\n- **Model cards** are the professional standard for model documentation.\n  They record intended use, data limitations, performance by subgroup,\n  and known biases. Write one before any deployment.\n\n### Project Thread Status\n\n| Task | Status |\n|------|--------|\n| Data representation audit | Done |\n| Per-country fairness audit (MAE, bias direction) | Done |\n| Global SHAP feature importance | Done |\n| Local SHAP individual prediction explanation | Done |\n| Sample reweighting mitigation | Done |\n| Model card written | Done |\n\n---\n\n### What's Next\n\nChapter 9 completes Part 3. The book continues with Part 4 and six appendices:\n\n**Part 4 ‚Äî Production and Deployment:**\n- **Chapter 10** -- MLOps and Production ML: experiment tracking with MLflow, model registry, FastAPI serving, unit testing, and drift detection\n- **Chapter 11** -- Computer Vision with PyTorch: CNNs from scratch, transfer learning with ResNet-18, feature map visualisation\n\n**Appendices:**\n- **Appendix A** -- Python environment setup: `venv`, `conda`, `requirements.txt`, GPU drivers\n- **Appendix B** -- Keras 3 companion: the Chapter 7 networks rebuilt in Keras\n- **Appendix C** -- Project ideas and further reading: ten capstone projects and curated resources\n- **Appendix D** -- Reinforcement learning: Q-learning, Bellman equation, DQN on CartPole\n- **Appendix E** -- SQL for data scientists: `sqlite3`, `pandas.read_sql`, window functions\n- **Appendix F** -- Git and GitHub for ML: branching, `.gitignore`, `nbstripout`, DVC\n\n---\n\n*End of Chapter 10 -- Python for AI/ML*  \n[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n"
    }
  ]
}