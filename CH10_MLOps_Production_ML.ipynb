{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Chapter 10 -- MLOps and Production ML\n## *Python for AI/ML: A Complete Learning Journey*\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/CH10_MLOps_Production_ML.ipynb)\n&nbsp;&nbsp;[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n\n---\n\n**Part:** 4 -- Production and Deployment  \n**Prerequisites:** Chapter 6 (scikit-learn), Chapter 7 (PyTorch)  \n**Estimated time:** 5-6 hours\n\n---\n\n### Learning Objectives\n\nBy the end of this chapter you will be able to:\n\n- Explain the ML lifecycle and where MLOps fits within it\n- Track experiments with MLflow: log parameters, metrics, and artefacts\n- Version and register models in the MLflow Model Registry\n- Serve a trained model as a REST API endpoint using FastAPI\n- Write and run unit tests for ML preprocessing and prediction code\n- Detect data drift by comparing training and production distributions\n- Build a lightweight model monitoring dashboard\n- Structure an ML project repository for collaboration and reproducibility\n\n---\n\n### Why MLOps?\n\nTraining a model is roughly 10% of the work in a production ML system.\nThe other 90% is everything around it: tracking what you tried, packaging\nthe model so others can use it, serving predictions reliably, monitoring\nfor when the world changes and the model degrades, and reproducing results\nsix months later when a colleague asks why the model made a particular decision.\n\nMLOps is the engineering discipline that makes ML systems maintainable.\nThis chapter gives you the core toolkit.\n\n---\n\n### Project Thread -- Chapter 10\n\nWe take the Chapter 6 salary regression model through a complete MLOps workflow:\ninstrument the training with MLflow, compare three model variants in the UI,\nregister the best model, wrap it in a FastAPI endpoint, write tests,\nand build a drift monitor that would alert if production salary data\nshifts away from the training distribution.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Setup -- Install and Import\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import subprocess\nsubprocess.run(['pip', 'install', 'mlflow', 'fastapi', 'uvicorn',\n                'httpx', 'evidently', '-q'], check=False)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport mlflow\nimport mlflow.sklearn\nfrom mlflow.models import infer_signature\n\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n\nimport mlflow\nprint(f'MLflow version:  {mlflow.__version__}')\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.dpi']       = 110\nplt.rcParams['axes.titlesize']   = 13\nplt.rcParams['axes.titleweight'] = 'bold'\n\nDATASET_URL  = 'https://raw.githubusercontent.com/timothy-watt/python-for-ai-ml/main/data/so_survey_2025_curated.csv'\nRANDOM_STATE = 42\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Load and clean SO 2025 -- standard pipeline\ndf_raw = pd.read_csv(DATASET_URL)\ndf = df_raw.copy()\ndf = df.dropna(subset=['ConvertedCompYearly'])\ndf['ConvertedCompYearly'] = pd.to_numeric(df['ConvertedCompYearly'], errors='coerce')\nQ1, Q3 = df['ConvertedCompYearly'].quantile([0.25, 0.75])\nIQR = Q3 - Q1\ndf = df[\n    (df['ConvertedCompYearly'] >= max(Q1 - 3*IQR, 5_000)) &\n    (df['ConvertedCompYearly'] <= min(Q3 + 3*IQR, 600_000))\n].copy()\nif 'YearsCodePro' in df.columns:\n    df['YearsCodePro'] = pd.to_numeric(df['YearsCodePro'], errors='coerce')\n    df['YearsCodePro'] = df['YearsCodePro'].fillna(df['YearsCodePro'].median())\ndf['uses_python'] = df.get('LanguageHaveWorkedWith', pd.Series(dtype=str)).str.contains('Python', na=False).astype(int)\ndf['uses_sql']    = df.get('LanguageHaveWorkedWith', pd.Series(dtype=str)).str.contains('SQL', na=False).astype(int)\ndf['uses_js']     = df.get('LanguageHaveWorkedWith', pd.Series(dtype=str)).str.contains('JavaScript', na=False).astype(int)\ndf['uses_ai']     = df.get('AIToolCurrently', pd.Series(dtype=str)).notna().astype(int)\ndf['log_salary']  = np.log(df['ConvertedCompYearly'])\ndf = df.reset_index(drop=True)\n\nFEATURE_COLS = [c for c in ['YearsCodePro','uses_python','uses_sql','uses_js','uses_ai']\n                if c in df.columns]\nX = df[FEATURE_COLS].copy()\nfor col in FEATURE_COLS:\n    med = X[col].median()\n    X[col] = X[col].fillna(med if pd.notna(med) else 0)\ny = df['log_salary']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=RANDOM_STATE\n)\nprint(f'Dataset ready: {len(df):,} rows')\nprint(f'Features: {FEATURE_COLS}')\nprint(f'Train: {len(X_train):,}  Test: {len(X_test):,}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 10.1 -- The ML Lifecycle\n\nA production ML system has six phases that repeat in a continuous loop:\n\n```\n  Define        Collect       Train &      Evaluate     Deploy       Monitor\n  Problem  -->  & Clean  -->  Experiment --> & Select --> & Serve --> & Retrain\n    |           Data          (MLflow)       Model       (FastAPI)    (Drift)\n    |_______________________________________________________________|  (loop)\n```\n\nChapters 3-9 covered the middle phases in depth. This chapter fills in\n**Experiment tracking**, **Deploy & Serve**, and **Monitor & Retrain** --\nthe phases most often skipped in tutorials but most important in practice.\n\n### The core MLOps problems\n\n**Reproducibility** -- can you recreate the exact model that went to production\nsix months ago? Without tracking, the answer is almost always no.\n\n**Collaboration** -- when three people are running experiments simultaneously,\nhow do you compare results and agree on which model to deploy?\n\n**Deployment gap** -- a model that works in a notebook often breaks when moved\nto production because of subtle differences in data preprocessing.\n\n**Model decay** -- the world changes. A model trained on 2024 data will gradually\nbecome less accurate as 2025 data arrives. Monitoring detects this before users do.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 10.2 -- Experiment Tracking with MLflow\n\nMLflow is the most widely used open-source MLOps platform. Its core concept:\nevery training run is a logged **experiment** with parameters, metrics,\nartefacts (model files, plots), and metadata. You can compare runs in a UI\nand promote the best model to a registry.\n\nThe four MLflow components we use:\n- **Tracking** -- log parameters and metrics during training\n- **Models** -- save models in a standard format with schema\n- **Model Registry** -- version and stage models (Staging → Production)\n- **Projects** -- package code for reproducible execution (covered in 10.5)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 10.2.1 -- Configure MLflow tracking\n\nimport os\n\n# In Colab we use a local SQLite tracking store\n# In production this would point to a remote server\nMLFLOW_DIR      = '/tmp/mlflow'\nos.makedirs(MLFLOW_DIR, exist_ok=True)\nmlflow.set_tracking_uri(f'sqlite:///{MLFLOW_DIR}/mlflow.db')\n\nEXPERIMENT_NAME = 'so2025_salary_regression'\nmlflow.set_experiment(EXPERIMENT_NAME)\n\nprint(f'MLflow tracking URI: {mlflow.get_tracking_uri()}')\nprint(f'Experiment: {EXPERIMENT_NAME}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 10.2.2 -- Log three model variants as separate MLflow runs\n#\n# Each run logs:\n#   - Parameters: model hyperparameters and feature list\n#   - Metrics:    CV R^2, test R^2, test MAE\n#   - Artefacts:  the trained model with input/output schema\n\nmodel_configs = [\n    ('Ridge',            Ridge(alpha=1.0),\n     {'alpha': 1.0}),\n    ('RandomForest',     RandomForestRegressor(n_estimators=100, max_depth=8,\n                                               random_state=RANDOM_STATE, n_jobs=-1),\n     {'n_estimators': 100, 'max_depth': 8}),\n    ('GradientBoosting', GradientBoostingRegressor(n_estimators=100, max_depth=4,\n                                                    learning_rate=0.1,\n                                                    random_state=RANDOM_STATE),\n     {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.1}),\n]\n\nrun_results = []\n\nfor model_name, model, params in model_configs:\n    pipe = Pipeline([\n        ('imputer', SimpleImputer(strategy='median')),\n        ('scaler',  StandardScaler()),\n        ('model',   model),\n    ])\n\n    with mlflow.start_run(run_name=model_name):\n        # Log parameters\n        mlflow.log_param('model_type',    model_name)\n        mlflow.log_param('features',      str(FEATURE_COLS))\n        mlflow.log_param('train_size',    len(X_train))\n        for k, v in params.items():\n            mlflow.log_param(k, v)\n\n        # Train\n        pipe.fit(X_train, y_train)\n\n        # Cross-validation on training set\n        cv_scores = cross_val_score(pipe, X_train, y_train, cv=5, scoring='r2')\n        mlflow.log_metric('cv_r2_mean', cv_scores.mean())\n        mlflow.log_metric('cv_r2_std',  cv_scores.std())\n\n        # Test set metrics\n        y_pred_log = pipe.predict(X_test)\n        y_pred_usd = np.exp(y_pred_log)\n        y_true_usd = np.exp(y_test)\n        test_r2  = r2_score(y_test, y_pred_log)\n        test_mae = mean_absolute_error(y_true_usd, y_pred_usd)\n        test_rmse = np.sqrt(mean_squared_error(y_true_usd, y_pred_usd))\n        mlflow.log_metric('test_r2',   test_r2)\n        mlflow.log_metric('test_mae',  test_mae)\n        mlflow.log_metric('test_rmse', test_rmse)\n\n        # Log the model with input/output schema\n        signature = infer_signature(X_train, pipe.predict(X_train))\n        mlflow.sklearn.log_model(\n            pipe, artifact_path='model',\n            signature=signature,\n            input_example=X_train.iloc[:3]\n        )\n\n        run_id = mlflow.active_run().info.run_id\n        run_results.append({\n            'model': model_name, 'run_id': run_id,\n            'cv_r2': cv_scores.mean(), 'test_r2': test_r2,\n            'test_mae': test_mae\n        })\n\n    print(f'{model_name:<20} CV R2={cv_scores.mean():.4f}  '\n          f'Test R2={test_r2:.4f}  MAE=${test_mae:,.0f}')\n\nresults_df = pd.DataFrame(run_results)\nbest_run   = results_df.loc[results_df['test_r2'].idxmax()]\nprint(f'Best model: {best_run[\"model\"]}  (run_id: {best_run[\"run_id\"][:8]}...)')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 10.2.3 -- Query MLflow programmatically and visualise run comparison\n\nclient = mlflow.tracking.MlflowClient()\nexperiment = client.get_experiment_by_name(EXPERIMENT_NAME)\nruns = client.search_runs(\n    experiment_ids=[experiment.experiment_id],\n    order_by=['metrics.test_r2 DESC']\n)\n\nprint(f'Runs logged: {len(runs)}')\nprint()\nprint(f'{\"Run name\":<22} {\"CV R2\":>8} {\"Test R2\":>9} {\"Test MAE\":>12}')\nprint('-' * 55)\nfor run in runs:\n    name = run.data.tags.get('mlflow.runName', run.info.run_id[:8])\n    cv   = run.data.metrics.get('cv_r2_mean', 0)\n    tr2  = run.data.metrics.get('test_r2', 0)\n    mae  = run.data.metrics.get('test_mae', 0)\n    print(f'{name:<22} {cv:>8.4f} {tr2:>9.4f} {mae:>12,.0f}')\n\n# Bar chart comparison\nfig, axes = plt.subplots(1, 2, figsize=(13, 4))\nnames = [r.data.tags.get('mlflow.runName', r.info.run_id[:8]) for r in runs]\nr2s   = [r.data.metrics.get('test_r2', 0) for r in runs]\nmaes  = [r.data.metrics.get('test_mae', 0) for r in runs]\n\naxes[0].bar(names, r2s, color='#2E75B6', edgecolor='white')\naxes[0].set_ylabel('Test R^2 (higher = better)')\naxes[0].set_title('Model Comparison: R^2')\nfor i, v in enumerate(r2s):\n    axes[0].text(i, v + 0.002, f'{v:.4f}', ha='center', fontsize=9)\n\naxes[1].bar(names, [m/1000 for m in maes], color='#E8722A', edgecolor='white')\naxes[1].set_ylabel('Test MAE ($k, lower = better)')\naxes[1].set_title('Model Comparison: MAE')\nfor i, v in enumerate(maes):\n    axes[1].text(i, v/1000 + 0.3, f'${v/1000:.1f}k', ha='center', fontsize=9)\n\nplt.suptitle('MLflow Experiment: SO 2025 Salary Regression',\n             fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 10.3 -- Model Registry: Versioning and Staging\n\nThe MLflow Model Registry is a centralised store for model versions.\nEach registered model can move through stages:\n`None → Staging → Production → Archived`.\n\nThis stage progression is the handoff point between data scientists\n(who produce models) and ML engineers (who deploy them). The registry\nrecords who promoted a model, when, and why -- a full audit trail.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 10.3.1 -- Register the best model and transition to Staging\n\nREGISTERED_MODEL_NAME = 'so2025_salary_predictor'\n\nbest_run_id = best_run['run_id']\nmodel_uri   = f'runs:/{best_run_id}/model'\n\n# Register the model\nregistered = mlflow.register_model(\n    model_uri=model_uri,\n    name=REGISTERED_MODEL_NAME\n)\n\nprint(f'Registered model: {registered.name}')\nprint(f'Version:          {registered.version}')\nprint(f'Status:           {registered.status}')\n\n# Transition to Staging\nimport time\ntime.sleep(2)   # allow registration to complete\n\nclient.transition_model_version_stage(\n    name=REGISTERED_MODEL_NAME,\n    version=registered.version,\n    stage='Staging',\n    archive_existing_versions=False\n)\nprint(f'Transitioned to:  Staging')\n\n# Retrieve model details from registry\nmodel_details = client.get_registered_model(REGISTERED_MODEL_NAME)\nprint(f'Latest versions:  {[(v.version, v.current_stage) for v in model_details.latest_versions]}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 10.3.2 -- Load the registered model and make predictions\n#\n# This is how a serving system loads a model from the registry\n# without needing to know the run ID or file path.\n\nstaging_uri = f'models:/{REGISTERED_MODEL_NAME}/Staging'\nloaded_model = mlflow.sklearn.load_model(staging_uri)\n\n# Make predictions with the loaded model\nsample = X_test.iloc[:5]\npreds_log = loaded_model.predict(sample)\npreds_usd = np.exp(preds_log)\n\nprint('Predictions from registry-loaded model:')\nprint(f'{\"Sample\":>8}  {\"Predicted\":>14}  {\"Actual\":>14}')\nprint('-' * 38)\nfor i, (pred, true) in enumerate(zip(preds_usd, np.exp(y_test.iloc[:5]))):\n    print(f'{i+1:>8}  ${pred:>13,.0f}  ${true:>13,.0f}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 10.4 -- Serving Predictions with FastAPI\n\nA trained model sitting in a file is not useful to anyone who cannot run Python.\nA REST API wraps the model so any application -- a web app, mobile app,\nor another service -- can send a request and receive a prediction.\n\n**FastAPI** is the modern standard for Python REST APIs: fast, type-safe,\nand auto-generates interactive documentation at `/docs`.\n\nWe simulate the API here by writing the app code and testing it in-process.\nIn production you would run `uvicorn app:app --host 0.0.0.0 --port 8000`.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 10.4.1 -- Write the FastAPI app\n\nAPI_CODE = '''\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel, Field\nimport numpy as np\nimport pandas as pd\nimport mlflow.sklearn\nimport os\n\napp = FastAPI(\n    title=\"SO 2025 Salary Predictor\",\n    description=\"Predicts annual developer salary from profile features.\",\n    version=\"1.0.0\"\n)\n\n# Load model at startup -- not on every request\nMODEL = None\n\n@app.on_event(\"startup\")\ndef load_model():\n    global MODEL\n    MODEL = mlflow.sklearn.load_model(os.environ[\"MODEL_URI\"])\n\nclass DeveloperProfile(BaseModel):\n    years_code_pro: float = Field(..., ge=0, le=50, description=\"Years of professional coding\")\n    uses_python:    int   = Field(..., ge=0, le=1,  description=\"1 if uses Python, else 0\")\n    uses_sql:       int   = Field(..., ge=0, le=1,  description=\"1 if uses SQL, else 0\")\n    uses_js:        int   = Field(..., ge=0, le=1,  description=\"1 if uses JavaScript, else 0\")\n    uses_ai:        int   = Field(..., ge=0, le=1,  description=\"1 if uses AI tools, else 0\")\n\nclass SalaryPrediction(BaseModel):\n    predicted_salary_usd: float\n    predicted_salary_log: float\n    model_version:        str\n\n@app.get(\"/health\")\ndef health():\n    return {\"status\": \"ok\", \"model_loaded\": MODEL is not None}\n\n@app.post(\"/predict\", response_model=SalaryPrediction)\ndef predict(profile: DeveloperProfile):\n    if MODEL is None:\n        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n    features = pd.DataFrame([{\n        \"YearsCodePro\": profile.years_code_pro,\n        \"uses_python\":  profile.uses_python,\n        \"uses_sql\":     profile.uses_sql,\n        \"uses_js\":      profile.uses_js,\n        \"uses_ai\":      profile.uses_ai,\n    }])\n    log_pred = float(MODEL.predict(features)[0])\n    return SalaryPrediction(\n        predicted_salary_usd=round(np.exp(log_pred), 2),\n        predicted_salary_log=round(log_pred, 6),\n        model_version=\"1.0.0\"\n    )\n'''\n\nwith open('/tmp/salary_api.py', 'w') as f:\n    f.write(API_CODE)\nprint('FastAPI app written to /tmp/salary_api.py')\nprint()\nprint('To run in production:')\nprint('  export MODEL_URI=\"models:/so2025_salary_predictor/Staging\"')\nprint('  uvicorn salary_api:app --host 0.0.0.0 --port 8000')\nprint()\nprint('Auto-generated docs available at: http://localhost:8000/docs')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 10.4.2 -- Test the API in-process using FastAPI's TestClient\n\nimport sys\nsys.path.insert(0, '/tmp')\n\nimport os\nos.environ['MODEL_URI'] = f'runs:/{best_run_id}/model'\n\n# Patch the startup event and load model directly for testing\nfrom fastapi.testclient import TestClient\nimport importlib.util\n\nspec = importlib.util.spec_from_file_location('salary_api', '/tmp/salary_api.py')\nsalary_api = importlib.util.load_from_spec = None\n\n# Instead of importing the module (which needs uvicorn startup),\n# we test the prediction logic directly using the loaded model\ntest_profiles = [\n    {'YearsCodePro': 10, 'uses_python': 1, 'uses_sql': 1, 'uses_js': 0, 'uses_ai': 1},\n    {'YearsCodePro': 2,  'uses_python': 0, 'uses_sql': 1, 'uses_js': 1, 'uses_ai': 0},\n    {'YearsCodePro': 20, 'uses_python': 1, 'uses_sql': 0, 'uses_js': 0, 'uses_ai': 1},\n]\n\nprint('Simulated API predictions:')\nprint(f'{\"Profile\":<45} {\"Predicted Salary\":>18}')\nprint('-' * 65)\nfor p in test_profiles:\n    feat_dict = {col: p.get(col, p.get('YearsCodePro', 0)) for col in FEATURE_COLS}\n    features = pd.DataFrame([feat_dict])\n    log_pred = float(loaded_model.predict(features)[0])\n    usd_pred = np.exp(log_pred)\n    desc = (f\"{p['YearsCodePro']}yrs, \"\n            f\"{'Python' if p['uses_python'] else 'no-Python'}, \"\n            f\"{'SQL' if p['uses_sql'] else 'no-SQL'}, \"\n            f\"{'AI' if p['uses_ai'] else 'no-AI'}\")\n    print(f'{desc:<45} ${usd_pred:>17,.0f}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 10.5 -- Unit Testing ML Code\n\nML code is harder to test than ordinary software because outputs are\nprobabilistic -- you cannot assert that `predict([5, 1, 0, 0, 1]) == 95000`.\nInstead, you test the things that are deterministic:\n\n- **Data contracts:** does the preprocessing produce the expected shape and dtype?\n- **Boundary conditions:** does the model return a finite positive number for valid input?\n- **Regression tests:** does the retrained model perform at least as well as the baseline?\n- **Data validation:** does the pipeline reject inputs that violate the schema?\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 10.5.1 -- Write and run ML unit tests without pytest infrastructure\n# (pytest runs from the command line; we simulate it here inline)\n\nimport traceback\n\ndef assert_equal(val, expected, msg=''):\n    assert val == expected, f'FAIL: {msg} -- got {val}, expected {expected}'\n\ndef assert_true(condition, msg=''):\n    assert condition, f'FAIL: {msg}'\n\ndef assert_close(val, expected, tol=0.01, msg=''):\n    assert abs(val - expected) <= tol, (\n        f'FAIL: {msg} -- got {val:.4f}, expected {expected:.4f} +/- {tol}'\n    )\n\ntests_passed = 0\ntests_failed = 0\n\ndef run_test(name, fn):\n    global tests_passed, tests_failed\n    try:\n        fn()\n        print(f'  PASS  {name}')\n        tests_passed += 1\n    except Exception as e:\n        print(f'  FAIL  {name}: {e}')\n        tests_failed += 1\n\n# -- Tests --\n\ndef test_feature_count():\n    assert_equal(X_train.shape[1], len(FEATURE_COLS),\n                 'Training feature count matches FEATURE_COLS')\n\ndef test_no_nulls_after_cleaning():\n    assert_equal(int(X_train.isnull().sum().sum()), 0,\n                 'No nulls in training features after cleaning')\n\ndef test_prediction_is_finite():\n    sample = X_test.iloc[:10]\n    preds  = loaded_model.predict(sample)\n    assert_true(np.all(np.isfinite(preds)),\n                'All predictions are finite numbers')\n\ndef test_prediction_in_plausible_range():\n    sample   = X_test.iloc[:50]\n    preds_usd = np.exp(loaded_model.predict(sample))\n    assert_true(preds_usd.min() > 1_000,  'No predictions below $1,000')\n    assert_true(preds_usd.max() < 2_000_000, 'No predictions above $2M')\n\ndef test_model_beats_mean_baseline():\n    # A model that always predicts the mean has R^2 = 0\n    # Our model must beat this significantly\n    test_r2 = r2_score(y_test, loaded_model.predict(X_test))\n    assert_true(test_r2 > 0.1, f'Model R^2={test_r2:.4f} exceeds mean baseline (0.0)')\n\ndef test_log_salary_target_range():\n    # log(5000) ~ 8.5, log(600000) ~ 13.3\n    assert_true(y_train.min() > 8.0,  'Min log salary above log(5000)')\n    assert_true(y_train.max() < 14.0, 'Max log salary below log(1.2M)')\n\nprint('Running ML unit tests...')\nfor name, fn in [\n    ('Feature count matches FEATURE_COLS',    test_feature_count),\n    ('No nulls in training data',             test_no_nulls_after_cleaning),\n    ('Predictions are finite',                test_prediction_is_finite),\n    ('Predictions in plausible salary range', test_prediction_in_plausible_range),\n    ('Model beats mean baseline',             test_model_beats_mean_baseline),\n    ('Log salary target in expected range',   test_log_salary_target_range),\n]:\n    run_test(name, fn)\n\nprint()\nprint(f'Results: {tests_passed} passed, {tests_failed} failed')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 10.6 -- Data Drift Detection\n\n**Data drift** occurs when the statistical distribution of production data\ndiverges from the training data. A salary model trained in 2024 on developers\nearning $50k-$200k will degrade if 2025 production data contains a different\nsalary range, different country mix, or different experience distribution.\n\nDrift detection answers: *Is the data the model is seeing today\nstill similar enough to the data it was trained on?*\n\nWe implement a simple but effective approach using the **Population Stability\nIndex (PSI)** and the **Kolmogorov-Smirnov test** -- both widely used in\nproduction monitoring systems.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 10.6.1 -- Simulate production drift and detect it\n\nfrom scipy import stats\n\n# Simulate 'production' data arriving 12 months after training\n# We inject drift: higher average experience and salary in production\nnp.random.seed(RANDOM_STATE)\nn_prod = 500\n\nprod_data = X_test.sample(n=n_prod, replace=True, random_state=RANDOM_STATE).copy()\n\n# Inject drift: shift YearsCodePro upward (more senior developers in production)\nif 'YearsCodePro' in prod_data.columns:\n    prod_data['YearsCodePro'] = prod_data['YearsCodePro'] + np.random.normal(3, 1, n_prod)\n    prod_data['YearsCodePro'] = prod_data['YearsCodePro'].clip(0, 50)\n\ndef psi(expected, actual, bins=10):\n    \"\"\"\n    Population Stability Index.\n    PSI < 0.1:  no significant drift\n    PSI 0.1-0.2: moderate drift -- investigate\n    PSI > 0.2:  significant drift -- retrain\n    \"\"\"\n    # Build histogram bins from training data\n    breakpoints = np.percentile(expected, np.linspace(0, 100, bins + 1))\n    breakpoints  = np.unique(breakpoints)   # remove duplicates\n    if len(breakpoints) < 3:\n        return 0.0\n    exp_counts = np.histogram(expected, bins=breakpoints)[0] + 1e-6\n    act_counts = np.histogram(actual,   bins=breakpoints)[0] + 1e-6\n    exp_pct = exp_counts / exp_counts.sum()\n    act_pct = act_counts / act_counts.sum()\n    return float(np.sum((act_pct - exp_pct) * np.log(act_pct / exp_pct)))\n\nprint('Data Drift Report')\nprint('=' * 55)\nprint(f'{\"Feature\":<20} {\"KS p-value\":>12} {\"PSI\":>8} {\"Status\"}')\nprint('-' * 55)\n\nfor col in FEATURE_COLS:\n    train_vals = X_train[col].dropna().values\n    prod_vals  = prod_data[col].dropna().values\n    ks_stat, ks_p  = stats.ks_2samp(train_vals, prod_vals)\n    psi_score = psi(train_vals, prod_vals)\n    if psi_score > 0.2 or ks_p < 0.05:\n        status = 'DRIFT DETECTED'\n    elif psi_score > 0.1:\n        status = 'Monitor'\n    else:\n        status = 'OK'\n    print(f'{col:<20} {ks_p:>12.4f} {psi_score:>8.4f} {status}')\n\nprint()\nprint('PSI thresholds: < 0.1 = stable, 0.1-0.2 = investigate, > 0.2 = retrain')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 10.6.2 -- Visualise drift: training vs production distributions\n\ndrift_cols = [c for c in FEATURE_COLS if c == 'YearsCodePro' or\n              X_train[c].nunique() > 2]\nif not drift_cols:\n    drift_cols = FEATURE_COLS[:2]\n\nn_cols = len(drift_cols)\nfig, axes = plt.subplots(1, n_cols, figsize=(6 * n_cols, 4))\nif n_cols == 1:\n    axes = [axes]\n\nfor ax, col in zip(axes, drift_cols):\n    ax.hist(X_train[col].dropna(), bins=30, alpha=0.5,\n            color='#2E75B6', density=True, label='Training')\n    ax.hist(prod_data[col].dropna(), bins=30, alpha=0.5,\n            color='#E8722A', density=True, label='Production (simulated)')\n    psi_val = psi(X_train[col].dropna().values, prod_data[col].dropna().values)\n    ax.set_title(f'{col}\\nPSI={psi_val:.3f}')\n    ax.set_xlabel(col)\n    ax.set_ylabel('Density')\n    ax.legend(fontsize=9)\n\nplt.suptitle('Drift Detection: Training vs Simulated Production Data',\n             fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Chapter 10 Summary\n\n### Key Takeaways\n\n- **MLflow** is the standard open-source experiment tracker. Log params and\n  metrics with `mlflow.log_param()` / `mlflow.log_metric()`. Always log inside\n  a `with mlflow.start_run():` context so runs are cleanly scoped.\n- **`infer_signature`** captures the input/output schema of your model.\n  This prevents silent failures when the serving environment has different\n  column names or dtypes.\n- **The Model Registry** decouples training from deployment. Data scientists\n  push to Staging; ML engineers promote to Production. The audit trail\n  shows who approved each version and when.\n- **FastAPI** wraps models as REST APIs with automatic validation (Pydantic)\n  and auto-generated docs at `/docs`. Load the model once at startup,\n  not on every request.\n- **ML unit tests** focus on data contracts, boundary conditions, and\n  regression baselines -- not exact output values. Run them in CI on\n  every commit that touches training code.\n- **PSI > 0.2** is the standard threshold for triggering a retrain.\n  The KS test gives a complementary p-value-based signal.\n  Monitor every feature that the model uses, not just the target.\n\n### Project Thread Status\n\n| Task | Status |\n|------|--------|\n| Three model variants tracked in MLflow | Done |\n| Best model registered and staged | Done |\n| FastAPI prediction endpoint written | Done |\n| Six ML unit tests written and passing | Done |\n| Drift detection with PSI and KS test | Done |\n\n---\n\n### What's Next: Chapter 11 -- Computer Vision with PyTorch\n\nChapter 11 applies the PyTorch training loop from Chapter 7 to image data:\nCNNs, transfer learning with a pre-trained ResNet, and feature map visualisation.\nImages are the third major data modality after tabular (Ch 3-6) and text (Ch 8).\n\nAfter Chapter 11, the appendices cover:\n- **Appendix D** -- Reinforcement learning: Q-learning and DQN on CartPole\n- **Appendix E** -- SQL for data scientists: sqlite3, pandas.read_sql, window functions\n- **Appendix F** -- Git and GitHub for ML: branching, nbstripout, DVC\n\n---\n\n*End of Chapter 10 -- Python for AI/ML*  \n[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n"
    }
  ]
}