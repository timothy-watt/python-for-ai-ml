{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Chapter 3 -- NumPy and Pandas\n## *Python for AI/ML: A Complete Learning Journey*\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/CH03_NumPy_and_Pandas.ipynb)\n&nbsp;&nbsp;[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n\n---\n\n**Part:** 2 -- Data Science Foundations  \n**Prerequisites:** Chapter 2 (Intermediate Python)  \n**Estimated time:** 5-6 hours\n\n---\n\n### Learning Objectives\n\nBy the end of this chapter you will be able to:\n\n- Create and manipulate NumPy arrays with indexing, slicing, and broadcasting\n- Use NumPy universal functions (ufuncs) and random number generation\n- Understand why NumPy is 100x faster than pure Python for numerical work\n- Create Pandas Series and DataFrames from multiple sources\n- Select data with `.loc`, `.iloc`, and boolean indexing\n- Clean real-world data: missing values, duplicates, dtype conversion\n- Aggregate and group data with `groupby`, `agg`, and `pivot_table`\n- Apply functions efficiently with `apply()` vs vectorised operations\n- Merge and join DataFrames\n\n---\n\n### Project Thread -- Chapter 3\n\nThis is the chapter where the SO 2025 dataset becomes real.\nWe load the full 15,000-row curated subset, clean it end-to-end,\nand build the analysis-ready DataFrame that every subsequent chapter uses.\nBy the end of this chapter you will have handled salary outliers, exploded\nmulti-value language columns, and produced a cleaned dataset ready for\nvisualisation in Chapter 4 and modelling in Chapter 6.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Setup -- Imports and Version Check\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')   # suppress minor deprecation warnings\n\nprint(f'NumPy:  {np.__version__}')\nprint(f'Pandas: {pd.__version__}')\n\nDATASET_URL = 'https://raw.githubusercontent.com/timothy-watt/python-for-ai-ml/main/data/so_survey_2025_curated.csv'\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 3.1 -- NumPy: Fast Numerical Computing\n\nNumPy (Numerical Python) is the foundation of the entire Python data science stack.\nPandas, scikit-learn, PyTorch, and TensorFlow all store data as NumPy arrays\ninternally -- or arrays with a compatible interface.\n\nThe key insight: a NumPy array stores all its data in a single contiguous block\nof memory with a fixed type. This lets C code loop over it at full CPU speed --\n100-1000x faster than a Python list, which stores pointers to individual Python objects.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 3.1.1 -- Creating arrays and understanding dtypes\n\n# From a Python list -- NumPy infers the dtype automatically\nsalaries_list = [72_000, 88_000, 105_000, 135_000, 155_000, 195_000, 210_000]\na = np.array(salaries_list)\nprint(f'Array:  {a}')\nprint(f'dtype:  {a.dtype}')    # int64 -- 64-bit integer\nprint(f'shape:  {a.shape}')    # (7,) -- 1D array with 7 elements\nprint(f'ndim:   {a.ndim}')     # 1 -- number of dimensions\nprint(f'size:   {a.size}')     # 7 -- total number of elements\n\n# Force a specific dtype\na_float = np.array(salaries_list, dtype=np.float64)\nprint(f'float64 array: {a_float}')\n\n# 2D array -- a matrix\nfeatures = np.array([\n    [7,  135_000, 1],   # [years_exp, salary, uses_python]\n    [3,   88_000, 0],\n    [9,  105_000, 1],\n    [12, 210_000, 1],\n], dtype=np.float64)\nprint(f'2D shape: {features.shape}')   # (4, 3) -- 4 rows, 3 columns\n\n# Array creation shortcuts\nzeros  = np.zeros((3, 4))          # 3x4 matrix of 0.0\nones   = np.ones((2, 5))           # 2x5 matrix of 1.0\neye    = np.eye(4)                 # 4x4 identity matrix\nrng    = np.arange(0, 100, 10)     # like range() but returns an array\nlinsp  = np.linspace(0, 1, 11)     # 11 evenly spaced values from 0 to 1\n\nprint(f'zeros shape:  {zeros.shape}')\nprint(f'arange:       {rng}')\nprint(f'linspace:     {linsp}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 3.1.2 -- Indexing, slicing, and boolean masking\n\na = np.array([72_000, 88_000, 105_000, 135_000, 155_000, 195_000, 210_000])\n\n# 1D indexing -- identical to Python lists\nprint(f'First:    {a[0]}')\nprint(f'Last:     {a[-1]}')\nprint(f'Slice:    {a[2:5]}')\nprint(f'Reversed: {a[::-1]}')\n\n# 2D indexing -- [row, column]\nm = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint(f'Element [1,2]: {m[1, 2]}')    # row 1, col 2 -> 6\nprint(f'Row 0:         {m[0, :]}')    # all columns of row 0\nprint(f'Col 1:         {m[:, 1]}')    # all rows of column 1\nprint(f'Submatrix:     {m[0:2, 1:3]}')  # rows 0-1, cols 1-2\n\n# Boolean masking -- the backbone of data filtering\n# Step 1: create a boolean array where condition is True\nmask = a > 100_000\nprint(f'Mask:          {mask}')        # [False False  True  True  True  True  True]\n\n# Step 2: use the mask to select elements\nhigh_salaries = a[mask]\nprint(f'High salaries: {high_salaries}')\n\n# Combine conditions with & (and) and | (or) -- NOT Python's 'and'/'or'\nmiddle = a[(a >= 100_000) & (a <= 160_000)]\nprint(f'Middle band:   {middle}')\n\n# np.where -- like a vectorised if/else\nlabels = np.where(a >= 150_000, 'Senior', 'Junior')\nprint(f'Labels:        {labels}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 3.1.3 -- Broadcasting\n#\n# Broadcasting is NumPy's rule for performing arithmetic between arrays\n# of different shapes. It eliminates most explicit loops over array elements.\n# Rule: dimensions are compatible if they are equal OR one of them is 1.\n\nsalaries = np.array([72_000, 88_000, 105_000, 135_000, 210_000], dtype=float)\n\n# Scalar broadcast -- the scalar is applied to every element\nprint('--- Scalar operations ---')\nprint(f'Original:     {salaries}')\nprint(f'+10% raise:   {salaries * 1.1}')       # multiplies every element\nprint(f'- $5k:        {salaries - 5_000}')      # subtracts from every element\nprint(f'Normalised:   {(salaries - salaries.mean()) / salaries.std()}')\n\n# Why print .shape before operations -- a debugging habit\na = np.array([[1, 2, 3], [4, 5, 6]])     # shape (2, 3)\nb = np.array([10, 20, 30])               # shape (3,) -- broadcast across rows\nprint(f'a shape: {a.shape},  b shape: {b.shape}')\nprint(f'a + b:\\n{a + b}')   # b is added to each row of a\n\n# Column broadcast -- reshape b to (2,1) to broadcast across columns\nc = np.array([[100], [200]])             # shape (2, 1)\nprint(f'a shape: {a.shape},  c shape: {c.shape}')\nprint(f'a + c:\\n{a + c}')   # c is added to each column of a\n\n# Real use: subtract the column mean from each column (feature centering)\ndata = np.array([[72_000, 7], [88_000, 3], [105_000, 9], [135_000, 12]], dtype=float)\ncol_means = data.mean(axis=0)   # axis=0 = compute along rows (per column)\ncentered  = data - col_means    # broadcasts col_means across all rows\nprint(f'Column means: {col_means}')\nprint(f'Centered data:\\n{centered}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 3.1.4 -- Universal functions (ufuncs) and aggregations\n\nsalaries = np.array([72_000, 88_000, 105_000, 135_000, 155_000, 195_000, 210_000], dtype=float)\n\n# Aggregation functions -- collapse an array to a scalar\nprint('--- Aggregations ---')\nprint(f'sum:    ${np.sum(salaries):>12,.0f}')\nprint(f'mean:   ${np.mean(salaries):>12,.2f}')\nprint(f'median: ${np.median(salaries):>12,.2f}')\nprint(f'std:    ${np.std(salaries):>12,.2f}')\nprint(f'min:    ${np.min(salaries):>12,.0f}')\nprint(f'max:    ${np.max(salaries):>12,.0f}')\nprint(f'argmax: index {np.argmax(salaries)} (highest earner)')\n\n# Math ufuncs -- element-wise operations\nprint()\nprint('--- Math ufuncs ---')\nlog_sals = np.log(salaries)    # natural log -- common for skewed salary distributions\nprint(f'log(salaries): {np.round(log_sals, 2)}')\n\n# 2D aggregations -- axis parameter controls direction\nmatrix = np.array([[72_000, 7, 1],\n                   [88_000, 3, 0],\n                   [105_000, 9, 1],\n                   [135_000, 12, 1]], dtype=float)\n\nprint()\nprint('--- 2D aggregations ---')\nprint(f'Column means (axis=0): {matrix.mean(axis=0)}')   # mean of each column\nprint(f'Row means    (axis=1): {matrix.mean(axis=1)}')   # mean of each row\n\n# Random number generation -- essential for ML (weight initialisation, train/test splits)\nrng = np.random.default_rng(seed=42)   # new-style: reproducible random generator\nprint()\nprint('--- Random ---')\nprint(f'Uniform [0,1]:    {rng.random(5).round(4)}')\nprint(f'Normal (mu=0):    {rng.standard_normal(5).round(4)}')\nprint(f'Integers 0-99:    {rng.integers(0, 100, size=5)}')\nrandom_salaries = rng.normal(loc=120_000, scale=30_000, size=1000)\nprint(f'Simulated salaries: mean=${random_salaries.mean():,.0f}, std=${random_salaries.std():,.0f}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 3.1.5 -- Speed comparison: NumPy vs pure Python\nimport time\n\nn = 1_000_000   # one million elements\n\n# Pure Python: list of numbers\npy_list = list(range(n))\nstart = time.perf_counter()\npy_result = sum(x * 1.1 for x in py_list)\npy_time = time.perf_counter() - start\n\n# NumPy: array of numbers\nnp_array = np.arange(n, dtype=float)\nstart = time.perf_counter()\nnp_result = (np_array * 1.1).sum()\nnp_time = time.perf_counter() - start\n\nprint(f'Pure Python: {py_time*1000:>8.2f} ms')\nprint(f'NumPy:       {np_time*1000:>8.2f} ms')\nprint(f'Speedup:     {py_time/np_time:.0f}x faster with NumPy')\nprint(f'Results match: {abs(py_result - np_result) < 1}')\n\n# Memory comparison\nimport sys\npy_mem = sys.getsizeof(py_list) + sum(sys.getsizeof(x) for x in py_list[:100]) * 10\nnp_mem = np_array.nbytes\nprint(f'Python list memory: ~{py_mem/1e6:.0f} MB (estimated)')\nprint(f'NumPy array memory:  {np_mem/1e6:.1f} MB')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 3.1.6 -- Linear algebra basics (preview for Chapter 7)\n#\n# Neural networks are fundamentally matrix multiplications.\n# Here we cover the operations Chapter 7 will rely on.\n\n# Matrix multiplication -- the @ operator\n# In ML: output = inputs @ weights + bias\ninputs  = np.array([[1.0, 2.0, 3.0],\n                    [4.0, 5.0, 6.0]])   # shape (2, 3): 2 samples, 3 features\nweights = np.array([[0.1, 0.2],\n                    [0.3, 0.4],\n                    [0.5, 0.6]])         # shape (3, 2): 3 inputs, 2 outputs\nbias    = np.array([0.01, 0.02])         # shape (2,): one bias per output\n\noutput = inputs @ weights + bias         # shape (2, 2)\nprint(f'inputs shape:  {inputs.shape}')\nprint(f'weights shape: {weights.shape}')\nprint(f'output shape:  {output.shape}')\nprint(f'output:\\n{output}')\n\n# Transpose\nprint(f'weights.T shape: {weights.T.shape}')\n\n# Dot product -- similarity between two vectors\nv1 = np.array([1.0, 0.0, 0.0])   # unit vector\nv2 = np.array([0.8, 0.6, 0.0])   # similar direction\nv3 = np.array([0.0, 1.0, 0.0])   # orthogonal\nprint(f'dot(v1, v2) = {np.dot(v1, v2):.2f}  (similar: high)')\nprint(f'dot(v1, v3) = {np.dot(v1, v3):.2f}  (orthogonal: zero)')\n\n# Norm -- magnitude of a vector\nprint(f'norm(v2) = {np.linalg.norm(v2):.4f}')   # should be 1.0 for unit vector\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 3.2 -- Pandas: DataFrames for Data Science\n\nPandas is to tabular data what NumPy is to numerical arrays.\nA **DataFrame** is a 2D table where each column can have its own dtype --\nlike a spreadsheet that lives in memory and has hundreds of analysis methods.\n\nThe two core Pandas data structures:\n- **Series** -- a 1D labelled array (one column, or one row)\n- **DataFrame** -- a 2D labelled table (rows and columns)\n\nEvery column in a DataFrame is a Series. Every row is also accessible as a Series.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 3.2.1 -- Series: the building block\n\n# Create a Series from a list -- index is auto-assigned 0, 1, 2...\nsalaries = pd.Series([72_000, 88_000, 105_000, 135_000, 210_000])\nprint('Basic Series:')\nprint(salaries)\nprint(f'dtype: {salaries.dtype}')\n\n# Create with a custom index -- index can be any hashable values\nsal_named = pd.Series(\n    [72_000, 88_000, 105_000, 135_000, 210_000],\n    index=['R001', 'R002', 'R003', 'R004', 'R005'],\n    name='salary'\n)\nprint()\nprint('Named Series with custom index:')\nprint(sal_named)\n\n# Series arithmetic -- broadcasts like NumPy\nprint(f'Mean:   ${sal_named.mean():,.0f}')\nprint(f'Median: ${sal_named.median():,.0f}')\nprint(f'Above mean: {(sal_named > sal_named.mean()).sum()} respondents')\n\n# Series from dict -- keys become the index\ncountry_counts = pd.Series({'USA': 4521, 'Germany': 1832, 'UK': 2104, 'India': 3210})\nprint()\nprint('Country counts:')\nprint(country_counts.sort_values(ascending=False))\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 3.2.2 -- Creating DataFrames\n\n# From a list of dicts -- the most common way in practice\ndata = [\n    {'id': 'R001', 'country': 'USA',     'salary': 135_000, 'years_exp': 7,  'lang': 'Python'},\n    {'id': 'R002', 'country': 'UK',      'salary':  88_000, 'years_exp': 3,  'lang': 'JavaScript'},\n    {'id': 'R003', 'country': 'Germany', 'salary': 105_000, 'years_exp': 9,  'lang': 'Python'},\n    {'id': 'R004', 'country': 'India',   'salary':  42_000, 'years_exp': 2,  'lang': 'Java'},\n    {'id': 'R005', 'country': 'Canada',  'salary': 118_000, 'years_exp': 11, 'lang': 'Python'},\n]\ndf = pd.DataFrame(data)\nprint('DataFrame from list of dicts:')\nprint(df)\nprint(f'Shape: {df.shape}   dtype per column:')\nprint(df.dtypes)\n\n# From a dict of lists -- columns as keys\ndf2 = pd.DataFrame({\n    'salary':    [72_000, 88_000, 135_000],\n    'years_exp': [3, 5, 9],\n    'python':    [True, False, True],\n})\nprint()\nprint('DataFrame from dict of lists:')\nprint(df2)\n\n# From a NumPy array\narr = np.array([[72_000, 7], [88_000, 3], [135_000, 9]])\ndf3 = pd.DataFrame(arr, columns=['salary', 'years_exp'])\nprint()\nprint('DataFrame from NumPy array:')\nprint(df3)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 3.2.3 -- Selection: .loc, .iloc, and boolean indexing\n#\n# This is the most important section to get right -- confusing loc/iloc\n# causes silent data errors that are hard to debug.\n#\n#   df[col]          -- select a column by name (returns Series)\n#   df[[c1, c2]]     -- select multiple columns (returns DataFrame)\n#   df.loc[row, col] -- select by LABEL (index value and column name)\n#   df.iloc[r, c]    -- select by INTEGER POSITION (0-based)\n\ndf = pd.DataFrame(data)   # reuse the 5-row DataFrame from above\ndf = df.set_index('id')   # use 'id' as the row index\n\nprint('--- Column selection ---')\nprint(df['salary'])                    # single column -> Series\nprint()\nprint(df[['salary', 'country']])       # multiple columns -> DataFrame\n\nprint()\nprint('--- .loc (label-based) ---')\nprint(df.loc['R001'])                  # one row by index label\nprint()\nprint(df.loc['R001', 'salary'])        # specific cell: row label, col name\nprint(df.loc[['R001', 'R003'], ['salary', 'years_exp']])  # multiple rows and cols\n\nprint()\nprint('--- .iloc (position-based) ---')\nprint(df.iloc[0])                      # first row by position\nprint(df.iloc[0, 1])                   # row 0, col 1\nprint(df.iloc[0:3, 0:2])              # first 3 rows, first 2 cols\n\nprint()\nprint('--- Boolean indexing ---')\nhigh_earners = df[df['salary'] > 100_000]\nprint(high_earners)\n\npython_devs = df[(df['lang'] == 'Python') & (df['years_exp'] >= 5)]\nprint()\nprint('Python devs with 5+ years:')\nprint(python_devs)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3.2.4 -- Project: Loading the Full SO 2025 Dataset\n\nThis is the moment the book's dataset becomes fully operational.\nWe load all 15,000 rows, run the three essential first commands on every\nnew dataset, and understand the shape of what we are working with.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Load the full SO 2025 curated dataset\nprint(f'Loading: {DATASET_URL}')\ndf = pd.read_csv(DATASET_URL)\n\n# The three commands you run on every new dataset -- in this order\nprint(f'Shape: {df.shape[0]:,} rows x {df.shape[1]} columns')\nprint()\nprint('First 3 rows:')\nprint(df.head(3).to_string())\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# .info() -- the single most useful first look at any DataFrame\n# Shows: column names, non-null counts, dtypes, memory usage\nprint('DataFrame Info:')\ndf.info()\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# .describe() -- statistical summary of numeric columns\nprint('Numeric column statistics:')\ndf.describe().round(0)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 3.3 -- Data Cleaning\n\nReal-world data is always messy. The SO 2025 dataset is no exception:\nit has missing values, salary outliers spanning from $1 to $20M+,\nmulti-value columns where one cell contains a semicolon-separated list,\nand columns that need type conversion.\n\nCleaning is not glamorous, but it is the difference between a model that\ntrains on corrupted data and one that produces trustworthy predictions.\n> **Rule:** Never modify `df` in-place during cleaning without keeping a backup.\n> Use `df.copy()` to create a clean working copy.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 3.3.1 -- Auditing missing values\n\n# df.isnull() returns a boolean DataFrame: True where data is missing\nmissing = df.isnull().sum().sort_values(ascending=False)\nmissing_pct = (missing / len(df) * 100).round(1)\n\nmissing_report = pd.DataFrame({\n    'missing_count': missing,\n    'missing_pct': missing_pct\n}).query('missing_count > 0')   # only rows with at least one missing value\n\nprint(f'Columns with missing values: {len(missing_report)} of {df.shape[1]}')\nprint()\nprint(missing_report.to_string())\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 3.3.2 -- Handling missing values: drop vs fill\n\ndf_clean = df.copy()   # always work on a copy\n\n# Strategy 1: drop rows where the target column (salary) is missing\n# We CANNOT impute the target variable -- those rows must go\nbefore = len(df_clean)\ndf_clean = df_clean.dropna(subset=['ConvertedCompYearly'])\nprint(f'Dropped {before - len(df_clean):,} rows with missing salary')\nprint(f'Remaining: {len(df_clean):,} rows')\n\n# Strategy 2: fill missing categorical columns with a placeholder\ncat_cols = ['Country', 'DevType', 'EdLevel', 'Employment', 'RemoteWork', 'OrgSize']\nfor col in cat_cols:\n    if col in df_clean.columns:\n        n_filled = df_clean[col].isnull().sum()\n        df_clean[col] = df_clean[col].fillna('Unknown')\n        if n_filled > 0:\n            print(f'Filled {n_filled:,} missing values in {col} with \"Unknown\"')\n\n# Strategy 3: fill numeric columns with median (robust to outliers)\nif 'YearsCodePro' in df_clean.columns:\n    # YearsCodePro has entries like 'Less than 1 year' and 'More than 50 years'\n    # Convert to numeric first, coercing non-numeric strings to NaN\n    df_clean['YearsCodePro'] = pd.to_numeric(df_clean['YearsCodePro'], errors='coerce')\n    median_yrs = df_clean['YearsCodePro'].median()\n    n_filled   = df_clean['YearsCodePro'].isnull().sum()\n    df_clean['YearsCodePro'] = df_clean['YearsCodePro'].fillna(median_yrs)\n    print(f'Filled {n_filled:,} missing YearsCodePro with median ({median_yrs:.0f} years)')\n\nprint(f'Missing values remaining: {df_clean.isnull().sum().sum()}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 3.3.3 -- Salary outlier handling\n#\n# ConvertedCompYearly ranges from $1 (data entry error) to $20M+ (noise).\n# We use the IQR (interquartile range) method to identify outlier bounds,\n# then apply a sensible hard cap that preserves real high earners.\n\nsal = df_clean['ConvertedCompYearly']\n\nprint('Salary distribution BEFORE cleaning:')\nprint(f'  Count:  {sal.count():,}')\nprint(f'  Min:    ${sal.min():,.0f}')\nprint(f'  p1:     ${sal.quantile(0.01):,.0f}')\nprint(f'  Median: ${sal.median():,.0f}')\nprint(f'  Mean:   ${sal.mean():,.0f}')\nprint(f'  p99:    ${sal.quantile(0.99):,.0f}')\nprint(f'  Max:    ${sal.max():,.0f}')\n\n# IQR bounds\nQ1  = sal.quantile(0.25)\nQ3  = sal.quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = max(Q1 - 3 * IQR, 5_000)      # at least $5k\nupper_bound = min(Q3 + 3 * IQR, 600_000)    # cap at $600k\n\nprint(f'IQR bounds: ${lower_bound:,.0f} to ${upper_bound:,.0f}')\n\nbefore = len(df_clean)\ndf_clean = df_clean[\n    (df_clean['ConvertedCompYearly'] >= lower_bound) &\n    (df_clean['ConvertedCompYearly'] <= upper_bound)\n]\nprint(f'Removed {before - len(df_clean):,} outlier salary rows')\n\nprint()\nprint('Salary distribution AFTER cleaning:')\nsal_clean = df_clean['ConvertedCompYearly']\nprint(f'  Count:  {sal_clean.count():,}')\nprint(f'  Min:    ${sal_clean.min():,.0f}')\nprint(f'  Median: ${sal_clean.median():,.0f}')\nprint(f'  Mean:   ${sal_clean.mean():,.0f}')\nprint(f'  Max:    ${sal_clean.max():,.0f}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 3.3.4 -- Handling multi-value columns\n#\n# LanguageHaveWorkedWith stores multiple values per row: 'Python;SQL;JavaScript'\n# To analyse language popularity, we need to 'explode' this column.\n# This converts one row with 3 languages into 3 rows with 1 language each.\n\nlang_col = 'LanguageHaveWorkedWith'\n\nif lang_col in df_clean.columns:\n    # Step 1: split the semicolon-separated string into a list\n    # str accessor: vectorised string methods for the whole column\n    lang_lists = df_clean[lang_col].str.split(';')\n    print(f'Example before split: {df_clean[lang_col].iloc[0]}')\n    print(f'Example after split:  {lang_lists.iloc[0]}')\n\n    # Step 2: explode -- one row per language\n    lang_exploded = lang_lists.explode()   # one language per row\n    lang_exploded = lang_exploded.str.strip()   # remove any whitespace\n    lang_exploded = lang_exploded[lang_exploded.notna() & (lang_exploded != '')]  # drop empties\n\n    # Step 3: count and rank\n    lang_counts = lang_exploded.value_counts()\n\n    print(f'Unique languages in dataset: {len(lang_counts)}')\n    print()\n    print('Top 15 languages by respondent count:')\n    top15 = lang_counts.head(15)\n    for rank, (lang, count) in enumerate(top15.items(), 1):\n        pct = count / len(df_clean) * 100\n        bar = '#' * int(pct / 1.5)\n        print(f'  #{rank:>2}: {lang:<25} {count:>5,}  ({pct:>5.1f}%)  {bar}')\n\n    # Step 4: add a convenience column to df_clean\n    # Boolean column: does this respondent use Python?\n    df_clean['uses_python'] = df_clean[lang_col].str.contains('Python', na=False)\n    python_pct = df_clean['uses_python'].mean() * 100\n    print(f'Respondents using Python: {python_pct:.1f}%')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 3.3.5 -- Dtype conversion and final clean-up\n\n# Check current dtypes\nprint('Current dtypes:')\nprint(df_clean.dtypes)\nprint()\n\n# Convert object columns that should be categories\n# 'category' dtype uses less memory for columns with few unique values\ncat_cols = ['Country', 'EdLevel', 'Employment', 'RemoteWork', 'OrgSize']\nfor col in cat_cols:\n    if col in df_clean.columns:\n        n_unique = df_clean[col].nunique()\n        df_clean[col] = df_clean[col].astype('category')\n        print(f'  {col:<20} -> category  ({n_unique} unique values)')\n\n# Reset index after all the row-dropping operations\ndf_clean = df_clean.reset_index(drop=True)\n\n# Final shape\nprint()\nprint('=== Final cleaned dataset ===')\nprint(f'Shape: {df_clean.shape[0]:,} rows x {df_clean.shape[1]} columns')\nprint(f'Memory: {df_clean.memory_usage(deep=True).sum() / 1e6:.1f} MB')\nprint(f'Missing values: {df_clean.isnull().sum().sum()}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 3.4 -- Exploration and Aggregation\n\nWith a clean dataset, we can now ask real questions about it.\nPandas `groupby` and `agg` are the primary tools for computing\ngroup-level statistics -- the equivalent of SQL's `GROUP BY`.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 3.4.1 -- groupby basics\n\n# groupby splits the DataFrame into groups, applies a function to each,\n# then combines the results -- the Split-Apply-Combine pattern.\n\nif 'RemoteWork' in df_clean.columns:\n    # Mean salary by remote work status\n    remote_salary = (\n        df_clean.groupby('RemoteWork')['ConvertedCompYearly']\n        .agg(['mean', 'median', 'count'])\n        .round(0)\n        .sort_values('median', ascending=False)\n    )\n    remote_salary.columns = ['mean_salary', 'median_salary', 'count']\n    print('Salary by remote work status:')\n    print(remote_salary.to_string())\n\n# Multi-column groupby\nif 'EdLevel' in df_clean.columns and 'RemoteWork' in df_clean.columns:\n    top_ed = df_clean['EdLevel'].value_counts().head(3).index.tolist()\n    top_remote = df_clean['RemoteWork'].value_counts().head(3).index.tolist()\n    subset = df_clean[\n        df_clean['EdLevel'].isin(top_ed) & df_clean['RemoteWork'].isin(top_remote)\n    ]\n    cross = (\n        subset.groupby(['EdLevel', 'RemoteWork'])['ConvertedCompYearly']\n        .median()\n        .unstack()     # pivot RemoteWork values into columns\n        .round(0)\n    )\n    print()\n    print('Median salary: Education x Remote Work')\n    print(cross.to_string())\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 3.4.2 -- .agg() with multiple functions and custom aggregations\n\n# Top 8 countries by respondent count\ntop_countries = df_clean['Country'].value_counts().head(8).index.tolist()\ndf_top = df_clean[df_clean['Country'].isin(top_countries)]\n\n# Apply multiple aggregation functions at once with a dict\ncountry_stats = df_top.groupby('Country')['ConvertedCompYearly'].agg(\n    count    = 'count',\n    mean     = 'mean',\n    median   = 'median',\n    std      = 'std',\n    p25      = lambda x: x.quantile(0.25),\n    p75      = lambda x: x.quantile(0.75),\n).round(0).sort_values('median', ascending=False)\n\nprint('Salary statistics by country (top 8):')\nprint(country_stats.to_string())\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 3.4.3 -- apply() vs vectorised operations: when to use each\n#\n# apply() calls a Python function on each row or column.\n# Vectorised operations use Pandas/NumPy built-ins on the whole column.\n# Vectorised is ALWAYS faster -- use apply() only when no vectorised option exists.\n\nimport time\n\nsal = df_clean['ConvertedCompYearly']\n\n# Task: add a salary band label column\n\n# Method 1: apply() with a Python function\ndef band_label(s):\n    if s >= 200_000: return 'Principal'\n    if s >= 150_000: return 'Senior+'\n    if s >= 100_000: return 'Senior'\n    if s >=  60_000: return 'Mid-level'\n    return 'Junior'\n\nstart = time.perf_counter()\nbands_apply = sal.apply(band_label)\nt_apply = time.perf_counter() - start\n\n# Method 2: pd.cut() -- vectorised binning (no Python loop)\nstart = time.perf_counter()\nbands_cut = pd.cut(\n    sal,\n    bins=[0, 60_000, 100_000, 150_000, 200_000, float('inf')],\n    labels=['Junior', 'Mid-level', 'Senior', 'Senior+', 'Principal'],\n    right=False\n)\nt_cut = time.perf_counter() - start\n\nprint(f'apply() time:   {t_apply*1000:.2f} ms')\nprint(f'pd.cut() time:  {t_cut*1000:.2f} ms')\nprint(f'Speedup:        {t_apply/t_cut:.1f}x faster with vectorised pd.cut()')\n\n# Use pd.cut() result going forward\ndf_clean['salary_band'] = bands_cut\nprint()\nprint('Salary band distribution:')\nprint(df_clean['salary_band'].value_counts().sort_index())\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 3.4.4 -- Merging and joining DataFrames\n#\n# pd.merge() combines two DataFrames based on shared key columns.\n# Analogous to SQL JOIN operations.\n\n# Build a country metadata lookup table\ncountry_meta = pd.DataFrame({\n    'Country': ['United States', 'United Kingdom', 'Germany', 'India', 'Canada',\n                'France', 'Australia', 'Brazil'],\n    'region':  ['North America', 'Europe', 'Europe', 'Asia', 'North America',\n                'Europe', 'Oceania', 'South America'],\n    'currency': ['USD', 'GBP', 'EUR', 'INR', 'CAD', 'EUR', 'AUD', 'BRL'],\n})\n\n# Inner join -- only rows where Country exists in BOTH tables\ndf_merged = pd.merge(df_clean, country_meta, on='Country', how='left')\n# 'left' join keeps all rows from df_clean; unmatched get NaN in meta columns\n\nprint(f'Original shape:  {df_clean.shape}')\nprint(f'Merged shape:    {df_merged.shape}')\nprint(f'New columns:     {[c for c in df_merged.columns if c not in df_clean.columns]}')\n\n# Salary by region\nif 'region' in df_merged.columns:\n    region_stats = (\n        df_merged.dropna(subset=['region'])\n        .groupby('region')['ConvertedCompYearly']\n        .agg(count='count', median='median')\n        .sort_values('median', ascending=False)\n        .round(0)\n    )\n    print()\n    print('Salary by region:')\n    print(region_stats.to_string())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 3.4.5 -- Project: Complete SO 2025 Cleaning Summary\n\nWe now have a fully cleaned, analysis-ready DataFrame.\nThis cell produces the definitive summary that carries into Chapter 4.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "print('=' * 60)\nprint('  SO 2025 Developer Survey -- Cleaned Dataset Summary')\nprint('=' * 60)\n\nprint(f'Rows:                    {len(df_clean):,}')\nprint(f'Columns:                 {df_clean.shape[1]}')\nprint(f'Missing values:          {df_clean.isnull().sum().sum()}')\n\nsal = df_clean['ConvertedCompYearly']\nprint(f'Salary range:            ${sal.min():,.0f} to ${sal.max():,.0f}')\nprint(f'Salary median:           ${sal.median():,.0f}')\nprint(f'Salary mean:             ${sal.mean():,.0f}')\n\nif 'Country' in df_clean.columns:\n    print(f'Countries represented:   {df_clean[\"Country\"].nunique()}')\n\nif 'uses_python' in df_clean.columns:\n    print(f'Python adoption:         {df_clean[\"uses_python\"].mean()*100:.1f}%')\n\nif 'salary_band' in df_clean.columns:\n    print()\n    print('Salary band breakdown:')\n    for band, count in df_clean['salary_band'].value_counts().sort_index().items():\n        pct = count / len(df_clean) * 100\n        print(f'  {str(band):<12} {count:>5,}  ({pct:.1f}%)')\n\nprint()\nprint('This cleaned DataFrame is the foundation for:')\nprint('  Chapter 4: visualisation and EDA')\nprint('  Chapter 5: statistical analysis')\nprint('  Chapter 6: machine learning pipeline')\nprint('  Chapter 7: deep learning')\nprint('  Chapter 8: NLP on text columns')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Concept Check Questions\n\n> Test your understanding before moving on. Answer each question without referring back to the notebook, then expand to check.\n\n**Q1.** What does NumPy **broadcasting** mean? Give an example with shapes `(3,)` and `(4, 3)`.\n\n<details><summary>Show answer</summary>\n\nBroadcasting is NumPy's rule for operating on arrays of different shapes without copying data. A `(3,)` array is treated as `(1, 3)` and replicated across 4 rows to match `(4, 3)`. ```python\na = np.array([10, 20, 30])  # shape (3,)\nb = np.ones((4, 3))         # shape (4, 3)\nresult = a + b              # shape (4, 3)\n```\n\n</details>\n\n**Q2.** What is the difference between `df.loc[0]` and `df.iloc[0]`?\n\n<details><summary>Show answer</summary>\n\n`df.loc[0]` selects by **label** â€” returns the row whose *index label* is `0`. `df.iloc[0]` selects by **integer position** â€” always returns the first row regardless of index labels. The difference matters when the index is not a default 0-based integer range.\n\n</details>\n\n**Q3.** Why use `fillna(median)` rather than `fillna(mean)` for skewed salary data?\n\n<details><summary>Show answer</summary>\n\nThe mean is pulled upward by outliers, making it unrepresentative. The median is robust to outliers and better represents the typical value. Always check the distribution's skewness before choosing an imputation strategy.\n\n</details>\n\n**Q4.** What does `df.groupby('Country')['Salary'].agg(['mean', 'count'])` return, and what would you check before trusting the means?\n\n<details><summary>Show answer</summary>\n\nA DataFrame with one row per country with `mean` and `count` columns. Check `count` â€” countries with n < 30 will have unreliable means. Filter to countries with sufficient sample size before drawing conclusions.\n\n</details>\n\n**Q5.** Explain the difference between `pd.merge(..., how='inner')` and `how='left'`.\n\n<details><summary>Show answer</summary>\n\n`'inner'` keeps only rows where the key exists in **both** DataFrames. `'left'` keeps all rows from the left DataFrame and fills with `NaN` where no match exists in the right. Use `'left'` when the left table is authoritative and you want to enrich it with optional right-table data.\n\n</details>\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Coding Exercises\n\n> Three exercises per chapter: **ðŸ”§ Guided** (fill-in-the-blanks) Â· **ðŸ”¨ Applied** (write from scratch) Â· **ðŸ—ï¸ Extension** (go beyond the chapter)\n\nExercises use the SO 2025 developer survey dataset.\nExpand each **Solution** block only after attempting the exercise.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exercise 1 ðŸ”§ Guided â€” Vectorised salary normalisation\n\nComplete `normalise_salaries(arr: np.ndarray, method: str) -> np.ndarray`\nsupporting three methods:\n- `'minmax'`: scale to [0, 1]\n- `'zscore'`: zero mean, unit variance\n- `'log'`: natural log transform (handle zeros with +1)\n\nNo loops allowed â€” use only NumPy vectorised operations.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\n\ndef normalise_salaries(arr: np.ndarray, method: str) -> np.ndarray:\n    if method == 'minmax':\n        return  # YOUR CODE\n    elif method == 'zscore':\n        return  # YOUR CODE\n    elif method == 'log':\n        return  # YOUR CODE\n    else:\n        raise ValueError(f'Unknown method: {method}')\n\nsalaries = np.array([30000, 55000, 90000, 120000, 200000, 0], dtype=float)\nfor m in ['minmax', 'zscore', 'log']:\n    print(f'{m:>8}: {normalise_salaries(salaries, m).round(3)}')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<details><summary>ðŸ’¡ Hint</summary>\n\nminmax: `(arr - arr.min()) / (arr.max() - arr.min())`\nzscore: `(arr - arr.mean()) / arr.std()`\nlog: `np.log1p(arr)` â€” equivalent to `log(arr + 1)`, safe for zeros\n\n</details>\n\n<details><summary>âœ… Solution</summary>\n\n```python\ndef normalise_salaries(arr, method):\n    if method == 'minmax': return (arr - arr.min()) / (arr.max() - arr.min())\n    elif method == 'zscore': return (arr - arr.mean()) / arr.std()\n    elif method == 'log': return np.log1p(arr)\n    else: raise ValueError(f'Unknown: {method}')\n```\n\n</details>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exercise 2 ðŸ”¨ Applied â€” SO 2025 salary cohort analysis\n\nUsing the synthetic SO 2025 DataFrame, compute a cohort table showing,\nfor each (`uses_python`, `uses_ai`) combination (4 groups):\n- count, median salary, 25th/75th percentile, and fraction earning > $100k\n\nUse only pandas `groupby`, `agg`, and `apply` â€” no loops.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import pandas as pd\nimport numpy as np\n\nrng = np.random.default_rng(42)\nn = 3000\ndf = pd.DataFrame({\n    'uses_python': rng.integers(0,2,n),\n    'uses_ai':     rng.integers(0,2,n),\n    'salary':      np.exp(10.8 + 0.3*rng.integers(0,2,n) + rng.normal(0,0.5,n))\n})\n\n# YOUR CODE: build cohort_table\ncohort_table = None\nprint(cohort_table)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<details><summary>ðŸ’¡ Hint</summary>\n\nUse `groupby(['uses_python','uses_ai'])['salary'].agg(...)` with a dict of\nnamed aggregations. For the `>100k` fraction, use `lambda s: (s > 100000).mean()`.\n\n</details>\n\n<details><summary>âœ… Solution</summary>\n\n```python\ncohort_table = df.groupby(['uses_python','uses_ai'])['salary'].agg(\n    count='count',\n    median='median',\n    p25=lambda s: s.quantile(0.25),\n    p75=lambda s: s.quantile(0.75),\n    pct_high_earner=lambda s: (s > 100000).mean()\n).round(2)\nprint(cohort_table)\n```\n\n</details>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exercise 3 ðŸ—ï¸ Extension â€” Rolling salary percentile tracker\n\nWrite a function that, given a DataFrame with a `survey_year` column,\ncomputes a *rolling* 3-year window percentile table showing how the\n25th, 50th, and 75th salary percentiles changed year over year.\n\nThen plot the result as a band chart (shaded area between p25 and p75,\nline for p50) using matplotlib.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng(42)\nyears = list(range(2018, 2026))\nrecords = []\nfor yr in years:\n    trend = (yr - 2018) * 3000\n    n = 300\n    records.append(pd.DataFrame({'year': yr,\n        'salary': np.exp(10.8 + rng.normal(0, 0.5, n)) + trend}))\ndf_time = pd.concat(records, ignore_index=True)\n\ndef rolling_percentiles(df: pd.DataFrame, window: int = 3) -> pd.DataFrame:\n    # YOUR CODE: return DataFrame with columns year, p25, p50, p75\n    pass\n\nresult = rolling_percentiles(df_time)\nprint(result)\n# Add your band chart here"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<details><summary>ðŸ’¡ Hint</summary>\n\nUse `df.groupby('year')['salary'].quantile([0.25,0.50,0.75]).unstack()` first,\nthen `df_pct.rolling(window).mean()` on the result.\nFor the band chart: `ax.fill_between(years, p25, p75, alpha=0.3)` + `ax.plot(years, p50)`\n\n</details>\n\n<details><summary>âœ… Solution</summary>\n\n```python\ndef rolling_percentiles(df, window=3):\n    pcts = df.groupby('year')['salary'].quantile([0.25,0.5,0.75]).unstack()\n    pcts.columns = ['p25','p50','p75']\n    return pcts.rolling(window, min_periods=1).mean().reset_index()\n```\n\n</details>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Chapter 3 Summary\n\nChapter 3 is the data science foundation every subsequent chapter builds on.\n\n### Key Takeaways\n\n- **NumPy arrays** store data in contiguous memory with a fixed dtype -- this is why\n  operations are 100x faster than Python lists. Always check `.shape` before arithmetic.\n- **Broadcasting** eliminates explicit loops: `array - array.mean()` centres every element\n  without writing a single for-loop.\n- **DataFrames** are labelled 2D tables. Each column is a Series with its own dtype.\n- **`.loc` vs `.iloc`**: loc uses labels, iloc uses integer positions. Mixing them up\n  causes silent errors. When in doubt, use `.loc`.\n- **Boolean indexing** is the standard way to filter rows:\n  `df[df['salary'] > 100_000]` is idiomatic Pandas.\n- **Missing values** come in three forms: drop (target column), fill with constant\n  (categorical), fill with median (numeric).\n- **Outlier handling** with IQR bounds is more robust than mean +/- 3 std.\n- **Multi-value columns** (semicolon-separated) need `.str.split().explode()` to analyse.\n- **Vectorised operations** (`pd.cut`, `.str.contains`) are always faster than `.apply()`.\n- **`groupby().agg()`** is the Split-Apply-Combine pattern -- equivalent to SQL GROUP BY.\n\n### Project Thread Status\n\n| Task | Status |\n|------|--------|\n| Loaded full 15,000-row SO 2025 dataset with `pd.read_csv()` | Done |\n| Audited and handled missing values | Done |\n| Removed salary outliers with IQR method | Done |\n| Exploded multi-value language column | Done |\n| Added `uses_python` and `salary_band` convenience columns | Done |\n| Computed salary statistics by country, education, remote work | Done |\n| Produced `df_clean` -- the analysis-ready DataFrame | Done |\n\n---\n\n### What's Next: Chapter 4 -- Data Visualisation\n\nChapter 4 takes `df_clean` and builds a full exploratory visualisation suite:\nsalary histograms with log scale, box plots by developer type, top-15 language\nbar charts, AI tool adoption heatmaps, and interactive Plotly charts.\nEvery chart tells a story about the 2025 developer landscape.\n\n---\n\n*End of Chapter 3 -- Python for AI/ML*  \n[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n"
    }
  ]
}