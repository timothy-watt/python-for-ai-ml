# Curriculum Map â€” Python for AI/ML
### *A Complete Learning Journey: From Absolute Beginner to AI/ML Practitioner*

> **How to use this map:**
> Each chapter lists its learning objectives, prerequisites, key concepts, estimated time,
> and what it unlocks. Use it to plan your learning path, skip chapters you already know,
> or identify gaps before jumping in.

---

## Quick Reference

| Chapter | Title | Part | Time | GPU? | Depends on |
|---------|-------|------|------|------|------------|
| 0  | Orientation & Setup | 1 | 1â€“2 hrs | No | â€” |
| 1  | Python Fundamentals | 1 | 4â€“5 hrs | No | Ch 0 |
| 2  | Intermediate Python | 1 | 5â€“6 hrs | No | Ch 1 |
| 3  | NumPy and Pandas | 2 | 5â€“6 hrs | No | Ch 2 |
| 4  | Data Visualisation | 2 | 3â€“4 hrs | No | Ch 3 |
| 5  | SciPy & Statistics | 3 | 4â€“5 hrs | No | Ch 4 |
| 6  | scikit-learn | 3 | 6â€“7 hrs | No | Ch 5 |
| 7  | Deep Learning (PyTorch) | 3 | 5â€“6 hrs | **Yes** | Ch 6 |
| 8  | NLP & Transformers | 3 | 5â€“6 hrs | **Yes** | Ch 7 |
| 9  | Computer Vision | 3 | 5â€“6 hrs | **Yes** | Ch 7 |
| 10 | Ethics & Responsible AI | 3 | 4â€“5 hrs | No | Ch 6 |
| 11 | MLOps & Production | 4 | 5â€“6 hrs | No | Ch 6 |

**Total estimated time:** 53â€“68 hours of active work
**GPU chapters:** Enable T4 GPU in Colab â€” `Runtime â†’ Change Runtime Type â†’ T4 GPU`

---

## Part 1 â€” Core Python Fundamentals

### Chapter 0 â€” Orientation & Setup

| Property | Detail |
|----------|--------|
| **File** | `CH00_Orientation_and_Setup.ipynb` |
| **Time** | 1â€“2 hours |
| **Prerequisites** | None |
| **Background assumed** | None â€” this is the starting point |

**Learning Objectives**
- Navigate the Google Colab interface confidently
- Mount Google Drive and manage notebook persistence
- Enable GPU and TPU runtimes
- Install packages with `pip` and verify installations
- Understand the Stack Overflow 2025 dataset that runs through the entire book
- Set up a local Python environment (optional: conda, venv, VS Code)

**Key Concepts Introduced**
`!pip install` Â· Colab cells Â· Drive mounting Â· `import` Â· dataset preview

**Unlocks:** Everything â€” complete this before any other chapter

---

### Chapter 1 â€” Python Fundamentals

| Property | Detail |
|----------|--------|
| **File** | `CH01_Python_Fundamentals.ipynb` |
| **Time** | 4â€“5 hours |
| **Prerequisites** | Chapter 0 |
| **Background assumed** | None â€” starts from scratch |

**Learning Objectives**
- Declare variables and understand Python's dynamic typing
- Manipulate strings with slicing, formatting, and f-strings
- Control program flow with `if/elif/else`, `for`, `while`, `break`, `continue`
- Work with lists, tuples, sets, and dictionaries
- Write reusable functions with positional args, keyword args, `*args`, `**kwargs`, and lambda
- Apply list comprehensions for concise transformations

**Key Concepts Introduced**
Variables Â· data types Â· control flow Â· data structures Â· functions Â· comprehensions

**Project:** Pure-Python salary analysis on 20 SO 2025 rows â€” no libraries, just logic

**Unlocks:** Chapter 2

---

### Chapter 2 â€” Intermediate Python

| Property | Detail |
|----------|--------|
| **File** | `CH02_Intermediate_Python.ipynb` |
| **Time** | 5â€“6 hours |
| **Prerequisites** | Chapter 1 |
| **Background assumed** | Basic Python syntax from Chapter 1 |

**Learning Objectives**
- Import from the standard library and third-party packages
- Read and write files using `with` statements and `pathlib`
- Handle exceptions with `try/except/finally`
- Design classes with `__init__`, properties, and dunder methods
- Understand how OOP maps directly to the scikit-learn API (`fit`, `transform`, `predict`)
- Write generator functions and use comprehensions for all four collection types
- Build and apply decorators and context managers

**Key Concepts Introduced**
Modules Â· OOP Â· classes Â· inheritance Â· generators Â· decorators Â· context managers

**Bridge:** The `SurveyAnalyser` class deliberately mirrors how scikit-learn estimators work â€”
this is the conceptual link between pure Python and ML frameworks.

**Project:** `SurveyLoader` + `SurveyAnalyser` classes operating on 500 SO 2025 rows

**Unlocks:** Chapter 3; the scikit-learn API in Chapter 6 will feel familiar

---

## Part 2 â€” Data Science Foundations

### Chapter 3 â€” NumPy and Pandas

| Property | Detail |
|----------|--------|
| **File** | `CH03_NumPy_and_Pandas.ipynb` |
| **Time** | 5â€“6 hours |
| **Prerequisites** | Chapter 2 |
| **Background assumed** | Python OOP and file I/O |

**Learning Objectives**
- Create and manipulate NumPy arrays; understand shape, dtype, and broadcasting
- Apply universal functions (ufuncs) for vectorised computation
- Build and index Pandas Series and DataFrames with `.loc` and `.iloc`
- Clean real data: handle missing values, fix dtypes, detect and remove outliers
- Aggregate with `groupby`, reshape with `pivot_table`, combine with `merge`
- Produce `df_clean` â€” the cleaned dataset used in every subsequent chapter

**Key Concepts Introduced**
Arrays Â· broadcasting Â· vectorisation Â· DataFrames Â· `.loc`/`.iloc` Â· `groupby` Â· `merge`

**Mental Model:** NumPy arrays are the underlying storage for every ML framework tensor.
Understanding shape `(rows, cols)` here prevents debugging pain in Chapters 7â€“9.

**Project:** Full SO 2025 cleaning pipeline â€” 15,000 rows ready for ML

**Unlocks:** Chapters 4, 5, 6 (and indirectly everything after)

---

### Chapter 4 â€” Data Visualisation

| Property | Detail |
|----------|--------|
| **File** | `CH04_Data_Visualization.ipynb` |
| **Time** | 3â€“4 hours |
| **Prerequisites** | Chapter 3 |
| **Background assumed** | Pandas DataFrames |

**Learning Objectives**
- Use Matplotlib's Figure/Axes model for precise plot control
- Choose the right chart type: line, scatter, bar, histogram, box, violin
- Build multi-panel layouts with `plt.subplots`
- Apply Seaborn for statistical visualisations: `histplot`, `boxplot`, `heatmap`, `pairplot`
- Build interactive charts with Plotly Express
- Design charts for communication: titles, labels, colour, annotation

**Key Concepts Introduced**
Figure/Axes Â· `subplots` Â· Seaborn themes Â· FacetGrid Â· Plotly Express

**Project:** Full EDA suite on SO 2025 â€” salary distributions, language rankings, AI tool adoption

**Unlocks:** Chapter 5; visualisation skills used in every subsequent chapter

---

## Part 3 â€” Machine Learning and Artificial Intelligence

> Covers three data modalities in sequence: **tabular** (Ch 5â€“7), **text** (Ch 8),
> **images** (Ch 9). Chapter 10 audits all three for fairness.

### Chapter 5 â€” SciPy and Statistical Computing

| Property | Detail |
|----------|--------|
| **File** | `CH05_SciPy_Statistical_Computing.ipynb` |
| **Time** | 4â€“5 hours |
| **Prerequisites** | Chapter 4 |
| **Background assumed** | Basic statistics concepts (mean, variance) helpful but not required |

**Learning Objectives**
- Understand probability distributions and test for normality
- Run two-sample t-tests and Mann-Whitney U tests correctly
- Apply one-way ANOVA and chi-squared tests
- Compute and interpret effect sizes: Cohen's d, eta-squared, CramÃ©r's V
- Fit custom curves and find roots with `scipy.optimize`
- Distinguish statistical significance from practical significance

**Key Concepts Introduced**
p-values Â· effect sizes Â· hypothesis testing Â· normality Â· confidence intervals Â· curve fitting

**Mental Model:** Statistical tests answer "is this difference real or noise?" â€” the same
question a learning curve answers in ML, just with different machinery.

**Project:** Is the Python salary premium statistically significant, and how large is the effect?

**Unlocks:** Chapter 6; statistical thinking informs every model evaluation decision

---

### Chapter 6 â€” Machine Learning with scikit-learn

| Property | Detail |
|----------|--------|
| **File** | `CH06_Machine_Learning_Sklearn.ipynb` |
| **Time** | 6â€“7 hours |
| **Prerequisites** | Chapter 5 |
| **Background assumed** | NumPy, Pandas, basic statistics |

**Learning Objectives**
- Explain the scikit-learn API contract: `fit`, `transform`, `predict`, `score`
- Build preprocessing pipelines that prevent data leakage
- Apply `ColumnTransformer` for mixed numeric/categorical data
- Train and evaluate regression models (Ridge, Lasso, Random Forest)
- Train and evaluate classification models (Logistic Regression, Gradient Boosting)
- Diagnose bias vs variance using learning curves
- Tune hyperparameters with `RandomizedSearchCV`
- Handle class imbalance with SMOTE and class weights
- Optimise decision thresholds using precision-recall curves
- Calibrate predicted probabilities with `CalibratedClassifierCV`
- Combine feature sets with `FeatureUnion`
- Cluster developers with KMeans; visualise with PCA

**Key Concepts Introduced**
Pipeline Â· ColumnTransformer Â· cross-validation Â· bias-variance tradeoff Â·
data leakage Â· SMOTE Â· calibration Â· FeatureUnion Â· KMeans Â· PCA

**Mental Model:** A scikit-learn `Pipeline` is just a sequence of `fit/transform` steps
followed by a final `fit/predict` step â€” exactly the `SurveyAnalyser` pattern from Chapter 2.

**Project:** Salary regression Â· Python-usage classifier Â· developer clustering

**Unlocks:** Chapters 7, 10, 11 (all three depend on Chapter 6 concepts)

---

### Chapter 7 â€” Deep Learning with PyTorch

| Property | Detail |
|----------|--------|
| **File** | `CH07_Deep_Learning_PyTorch.ipynb` |
| **Time** | 5â€“6 hours |
| **Prerequisites** | Chapter 6 |
| **Background assumed** | scikit-learn API; linear algebra basics helpful |
| **GPU** | **Required â€” enable T4 in Colab** |

**Learning Objectives**
- Create tensors and understand shape, dtype, and device placement
- Explain autograd: how PyTorch tracks operations for automatic differentiation
- Build neural networks with `nn.Module`, `nn.Linear`, `nn.BatchNorm1d`, `nn.Dropout`
- Write the five-step training loop: `zero_grad` â†’ forward â†’ loss â†’ `backward` â†’ `step`
- Use `model.train()` / `model.eval()` correctly
- Apply `ReduceLROnPlateau` and checkpoint best weights
- Save and load models with `torch.save` / `torch.load`
- Find optimal learning rates with the LR Range Test
- Accelerate training with mixed precision (`torch.cuda.amp`)
- Compile models for GPU efficiency with `torch.compile`

**Key Concepts Introduced**
Tensors Â· autograd Â· `nn.Module` Â· BatchNorm Â· Dropout Â· training loop Â·
`BCEWithLogitsLoss` Â· `state_dict` Â· LR Range Test Â· AMP Â· `torch.compile`

**Mental Model:** The PyTorch training loop is the scikit-learn `fit()` method written out
explicitly â€” forward pass = predict, loss = score, backward = learn, step = update weights.

**Project:** Salary regression MLP Â· Python-usage binary classifier MLP

**Unlocks:** Chapters 8 and 9 (both use the PyTorch training loop)

---

### Chapter 8 â€” NLP and Transformers

| Property | Detail |
|----------|--------|
| **File** | `CH08_NLP_and_Transformers.ipynb` |
| **Time** | 5â€“6 hours |
| **Prerequisites** | Chapter 7 |
| **Background assumed** | PyTorch training loop; basic text familiarity |
| **GPU** | **Recommended â€” T4 in Colab** |

**Learning Objectives**
- Build a classical NLP pipeline: clean â†’ tokenise â†’ TF-IDF â†’ classify
- Explain subword tokenisation and why it handles unknown words
- Run zero-shot inference with HuggingFace `pipeline()`
- Fine-tune DistilBERT on a custom classification task
- Visualise attention weights to interpret model focus
- Build a full RAG pipeline: embed documents, index with FAISS, retrieve, generate
- Integrate API-based LLMs (OpenAI / Anthropic) in a provider-agnostic client
- Prompt for structured JSON output and parse responses safely

**Key Concepts Introduced**
TF-IDF Â· subword tokenisation Â· `[CLS]` token Â· attention Â· fine-tuning Â·
sentence embeddings Â· FAISS Â· RAG Â· `LLMClient` Â· structured prompting

**Mental Model:** RAG separates *knowledge* (the document store, easily updated) from
*reasoning* (the LLM, fixed). Updating knowledge requires no retraining â€” just re-indexing.

**Project:** Developer role classifier (TF-IDF vs DistilBERT) Â· end-to-end RAG over SO 2025

**Unlocks:** Chapter 10 audits NLP models for bias; Chapter 11 deploys them

---

### Chapter 9 â€” Computer Vision with PyTorch

| Property | Detail |
|----------|--------|
| **File** | `CH09_Computer_Vision_PyTorch.ipynb` |
| **Time** | 5â€“6 hours |
| **Prerequisites** | Chapter 7 |
| **Background assumed** | PyTorch training loop; Chapters 8 and 9 are independent |
| **GPU** | **Required â€” T4 in Colab** |

**Learning Objectives**
- Explain how convolutional layers detect spatial features via weight sharing
- Build a CNN from scratch using `nn.Conv2d`, `nn.MaxPool2d`, `nn.BatchNorm2d`
- Build an augmentation pipeline with `torchvision.transforms`
- Visualise what a CNN learns by inspecting feature maps with forward hooks
- Apply transfer learning: freeze a ResNet-18 backbone, replace the classification head
- Run object detection with a pre-trained Faster R-CNN and draw bounding boxes
- Run semantic segmentation with DeepLabV3 and visualise per-pixel label maps

**Key Concepts Introduced**
Convolution Â· weight sharing Â· receptive field Â· MaxPool Â· `torchvision` Â·
data augmentation Â· transfer learning Â· feature maps Â· Faster R-CNN Â· DeepLabV3

**Mental Model:** CNN layers learn a hierarchy of features â€” edges â†’ textures â†’ shapes â†’
objects. Pre-trained models give you that hierarchy for free; you only teach the final categories.

**Project:** Custom CNN vs ResNet-18 on CIFAR-10 Â· bounding box detection Â· segmentation masks

**Unlocks:** Chapter 10 (ethics applies to CV models); Chapter 11 (deploy any of these)

---

### Chapter 10 â€” Ethics, Bias, and Responsible AI

| Property | Detail |
|----------|--------|
| **File** | `CH10_Ethics_Bias_Responsible_AI.ipynb` |
| **Time** | 4â€“5 hours |
| **Prerequisites** | Chapter 6 |
| **Background assumed** | scikit-learn models; basic statistics |
| **GPU** | Not required |

**Learning Objectives**
- Name and identify five sources of bias in ML systems
- Audit a dataset for demographic representation gaps
- Compute per-group fairness metrics: MAE by group, bias direction
- Explain and compare fairness criteria: demographic parity, equalised odds, calibration
- Generate global and local SHAP explanations for any sklearn model
- Apply sample reweighting as a bias mitigation technique
- Write a model card documenting intended use, limitations, and fairness metrics

**Key Concepts Introduced**
Representation bias Â· measurement bias Â· aggregation bias Â·
demographic parity Â· equalised odds Â· SHAP Â· `shap.Explainer` Â· model cards

**Mental Model:** A model card is to an ML model what a nutrition label is to food â€”
it doesn't make a bad model good, but it gives users the information to make informed decisions.

**Project:** Full fairness audit of the Chapter 6 salary regression model

**Unlocks:** Chapter 11 (deploying responsibly requires the ethics vocabulary from Ch 10)

---

## Part 4 â€” Production and Deployment

### Chapter 11 â€” MLOps and Production ML

| Property | Detail |
|----------|--------|
| **File** | `CH12_MLOps_Production_ML.ipynb` |
| **Time** | 5â€“6 hours |
| **Prerequisites** | Chapter 6 |
| **Background assumed** | scikit-learn models; basic REST API familiarity helpful |
| **GPU** | Not required |

**Learning Objectives**
- Explain the six-phase ML lifecycle and where MLOps fits
- Track experiments with MLflow: log parameters, metrics, and artefacts
- Register and stage models in the MLflow Model Registry
- Serve predictions as a REST API with FastAPI and Pydantic validation
- Write ML unit tests targeting data contracts and performance regression
- Detect data drift using PSI and the Kolmogorov-Smirnov test
- Generate a GitHub Actions CI/CD workflow that blocks degraded models from merging

**Key Concepts Introduced**
ML lifecycle Â· experiment tracking Â· model registry Â· `infer_signature` Â·
FastAPI Â· Pydantic Â· PSI Â· KS test Â· GitHub Actions Â· performance regression tests

**Mental Model:** Training a model is ~10% of the work in a production ML system.
The other 90% is reproducibility, serving, monitoring, and keeping it working as the world changes.

**Project:** Full MLOps pipeline for the SO 2025 salary model â€” tracked, registered,
served, monitored, and CI-tested

**Unlocks:** Appendix G (Docker packages what you built here for deployment)

---

## Appendices

| Appendix | Title | File | Best read after |
|----------|-------|------|-----------------|
| A | Python Environment Setup | `APP_A_Environment_Setup.ipynb` | Chapter 0 |
| B | Keras 3 Companion | `APP_B_Keras3_Companion.ipynb` | Chapter 7 |
| C | Project Ideas & Further Reading | `APP_C_Projects_and_Further_Reading.ipynb` | Any chapter |
| D | Reinforcement Learning Foundations | `APP_D_Reinforcement_Learning.ipynb` | Chapter 7 |
| E | SQL for Data Scientists | `APP_E_SQL_for_Data_Scientists.ipynb` | Chapter 3 |
| F | Git & GitHub for ML | `APP_F_Git_GitHub_for_ML.ipynb` | Chapter 0 |
| G | Docker & Containerisation | `APP_G_Docker_Containerisation.ipynb` | Chapter 11 |

---

## Concept Dependency Graph

```
Ch 0 (Setup)
  â””â”€â–º Ch 1 (Fundamentals)
        â””â”€â–º Ch 2 (Intermediate Python)
              â””â”€â–º Ch 3 (NumPy & Pandas)
                    â””â”€â–º Ch 4 (Visualisation)
                          â””â”€â–º Ch 5 (Statistics)
                                â””â”€â–º Ch 6 (scikit-learn) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                      â”œâ”€â–º Ch 7 (PyTorch) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                                      â”‚     â”œâ”€â–º Ch 8 (NLP)                 â”‚  â”‚
                                      â”‚     â””â”€â–º Ch 9 (Computer Vision)     â”‚  â”‚
                                      â”œâ”€â–º Ch 10 (Ethics) â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                      â””â”€â–º Ch 12 (MLOps)  â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Learning Path Recommendations

| Your goal | Recommended path |
|-----------|-----------------|
| Complete beginner | Ch 0 â†’ 1 â†’ 2 â†’ 3 â†’ 4 â†’ 5 â†’ 6 â†’ 7 â†’ 8 â†’ 9 â†’ 10 â†’ 11 |
| Python-literate, new to ML | Ch 3 â†’ 4 â†’ 5 â†’ 6 â†’ 7 â†’ 8 â†’ 9 â†’ 10 â†’ 11 |
| ML background, learn deep learning | Ch 7 â†’ 8 â†’ 9 |
| Want NLP / RAG | Ch 7 (prereq) â†’ Ch 8 |
| Want computer vision | Ch 7 (prereq) â†’ Ch 9 |
| Want production skills | Ch 6 (prereq) â†’ Ch 11 â†’ App G |
| Interview prep | Ch 6 + Ch 7 + App B (Keras) |
| Responsible AI focus | Ch 6 (prereq) â†’ Ch 10 |

---

## Key Skills by Industry Role

| Role | Most relevant chapters |
|------|----------------------|
| Data Analyst | 0â€“4, App E (SQL) |
| ML Engineer | 6, 7, 10, 11, App F (Git), App G (Docker) |
| Data Scientist | 3â€“10 |
| NLP Engineer | 7, 8, App D |
| Computer Vision Engineer | 7, 9 |
| ML Platform / MLOps Engineer | 11, App F, App G |
| AI Researcher | 7, 8, 9, App D (RL) |

---

*Python for AI/ML â€” A Complete Learning Journey*
*Dataset: Stack Overflow 2025 Developer Survey*
*Platform: Google Colab (free) Â· Compatible with any Jupyter environment*

---

### Chapter 12 â€” Adversarial ML and Model Security

| Property | Detail |
|----------|--------|
| **File** | `CH11_Adversarial_ML_Security.ipynb` |
| **Time** | 5â€“6 hours |
| **Prerequisites** | Chapter 7 (PyTorch), Chapter 9 (CV), Chapter 8 (RAG) |
| **Background assumed** | PyTorch training loop; ML model deployment |
| **GPU** | **Recommended â€” T4 in Colab** |

**Learning Objectives**
- Map the ML attack surface: evasion, poisoning, extraction, and LLM-specific attacks
- Implement FGSM and PGD evasion attacks and measure accuracy degradation
- Apply adversarial training to harden a model; understand the robustness-accuracy tradeoff
- Simulate a label-flipping poisoning attack and detect it with kNN consistency scoring
- Execute a model extraction attack and apply rate limiting and output noise defences
- Apply a structured red team framework to an LLM RAG system

**Key Concepts Introduced**
FGSM Â· PGD Â· epsilon ball Â· adversarial training Â· label-flipping Â· model extraction Â·
fidelity Â· prompt injection Â· indirect injection Â· red teaming Â· `RedTeamReport`

**Mental Model:** Security is a property of the *system*, not just the model weights.
An attacker has multiple entry points: the input, the training data, the API, and the
LLM's context. Each requires a different class of defence.

**Project:** Attack and defend the Chapter 9 ResNet-18 (evasion) and Chapter 8 RAG system (red team)

**Unlocks:** Appendix H (pipeline-level security)

---

### Appendix H â€” MLSecOps: Securing the ML Pipeline

| Appendix | Title | File | Best read after |
|----------|-------|------|-----------------|
| H | MLSecOps | `APP_H_MLSecOps.ipynb` | Ch 11 + Ch 12 |

**What's covered:** Supply chain risks Â· The pickle exploit and safetensors Â· FastAPI auth and rate limiting Â· `nbstripout` and `detect-secrets` Â· Distinguishing drift from adversarial probing Â· MLSecOps audit checklist

---

### Appendix I â€” Failure Modes and Troubleshooting

| Appendix | Title | File | Best read after |
|----------|-------|------|-----------------|
| I | Failure Modes | `APP_I_Failure_Modes.ipynb` | Any chapter where results seem wrong |

**What's covered:**
Silent data leakage (temporal, target, row) Â· Train/test contamination and the Pipeline fix Â·
NaN propagation patterns by model type Â· Learning curves for fit diagnosis Â·
Class imbalance and PR-AUC vs ROC-AUC Â· Gradient problems (vanishing, exploding, dead ReLUs) Â·
Tokenisation gotchas (truncation, attention mask) Â· Training-serving skew Â·
20-point diagnostic checklist + 'model performs worse than expected' decision tree

### Coding Exercises

All 13 chapters now include a **Coding Exercises** section with 3 exercises each:

| Tier | Symbol | Description |
|------|--------|-------------|
| Guided | ğŸ”§ | Fill-in-the-blanks scaffold â€” practice the core pattern |
| Applied | ğŸ”¨ | Write from scratch on a new slice of the SO 2025 dataset |
| Extension | ğŸ—ï¸ | Go beyond the chapter â€” combine techniques, investigate hypotheses |

**Total: 39 exercises** across 13 chapters. All use the SO 2025 dataset for consistency.
Collapsible hints and solutions in every exercise.

---

### Appendix J â€” Python Foundations: A Deep Dive

| Appendix | Title | File | Best read before |
|----------|-------|------|-----------------|
| J | Python Foundations | `APP_J_Python_Foundations.ipynb` | Chapter 0 (if new to programming) |

**What's covered:**
Variables, types, and the object model Â· Numbers and operators Â· String methods and f-strings Â·
Control flow (if/elif/else) Â· Loops (for, while, break, continue) Â·
Functions (parameters, defaults, *args, **kwargs, scope, lambda) Â·
Lists in depth (methods, comprehensions, mutability) Â· Tuples and named tuples Â·
Dictionaries in depth (iteration, comprehensions, common patterns) Â· Sets and set operations Â·
Mutability and the reference model (the alias trap, shallow vs deep copy) Â·
Error handling (try/except/else/finally, raising exceptions) Â·
File I/O (reading, writing, CSV module) Â· Mini project (pure-Python salary analysis)

**Deliberately omits:** Type hints, dataclasses, generators, decorators, OOP â€”
those are covered in Chapters 1 and 2 once the foundations are solid.
