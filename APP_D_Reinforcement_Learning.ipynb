{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Appendix D -- Reinforcement Learning Foundations\n## *Python for AI/ML: A Complete Learning Journey*\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/APP_D_Reinforcement_Learning.ipynb)\n&nbsp;&nbsp;[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n\n---\n\n**Prerequisites:** Chapter 7 (Deep Learning with PyTorch)  \n\n---\n\n### Learning Objectives\n\n- Explain the RL framework: agent, environment, state, action, reward\n- Implement Q-learning from scratch on a grid world\n- Understand the Bellman equation and temporal difference learning\n- Build a Deep Q-Network (DQN) agent that solves CartPole-v1\n- Explain experience replay and the target network trick\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Setup\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import subprocess\nsubprocess.run(['pip', 'install', 'gymnasium', '-q'], check=False)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom collections import deque, namedtuple\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport gymnasium as gym\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Gymnasium: {gym.__version__}')\nprint(f'Device:    {DEVICE}')\n\nRANDOM_STATE = 42\ntorch.manual_seed(RANDOM_STATE)\nnp.random.seed(RANDOM_STATE)\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.dpi'] = 110\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## D.1 -- The Reinforcement Learning Framework\n\nRL is fundamentally different from supervised and unsupervised learning.\nThere is no labelled dataset. Instead:\n\n- An **agent** takes **actions** in an **environment**\n- The environment returns a new **state** and a **reward** signal\n- The agent's goal is to maximise **cumulative reward** over time\n\n```\n         action\n  Agent --------> Environment\n    ^                  |\n    |   state, reward  |\n    +------------------+\n```\n\n**Key concepts:**\n\n**Policy (π):** the agent's strategy -- given a state, which action to take.  \n**Value function V(s):** expected cumulative reward from state s.  \n**Q-function Q(s,a):** expected cumulative reward from state s taking action a.  \n**Discount factor γ:** how much to value future rewards vs immediate ones.  \n**The Bellman equation:** `Q(s,a) = r + γ * max_a' Q(s', a')`  \n(the value of an action equals its immediate reward plus the discounted\nbest future value from the resulting state)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# D.1.1 -- Q-learning from scratch on a simple grid world\n\nclass GridWorld:\n    \"\"\"\n    4x4 grid. Agent starts at (0,0), goal at (3,3), holes at (1,1) and (2,3).\n    Actions: 0=up, 1=down, 2=left, 3=right\n    Rewards: +10 goal, -10 hole, -0.1 each step (encourages efficiency)\n    \"\"\"\n    GOAL  = (3, 3)\n    HOLES = {(1, 1), (2, 3)}\n    SIZE  = 4\n\n    def reset(self):\n        self.pos = (0, 0)\n        return self.pos\n\n    def step(self, action):\n        r, c = self.pos\n        if   action == 0: r = max(r - 1, 0)\n        elif action == 1: r = min(r + 1, self.SIZE - 1)\n        elif action == 2: c = max(c - 1, 0)\n        elif action == 3: c = min(c + 1, self.SIZE - 1)\n        self.pos = (r, c)\n        if self.pos == self.GOAL:       return self.pos, +10.0, True\n        if self.pos in self.HOLES:      return self.pos, -10.0, True\n        return self.pos, -0.1, False\n\n\n# Q-learning: learn Q(s, a) for every state-action pair\nn_states  = GridWorld.SIZE ** 2   # 16 states (row*SIZE + col)\nn_actions = 4\nQ = np.zeros((n_states, n_actions))\n\nenv     = GridWorld()\nalpha   = 0.1    # learning rate\ngamma   = 0.95   # discount factor\nepsilon = 1.0    # exploration rate (decays over time)\neps_min = 0.05\neps_decay = 0.995\n\nepisode_rewards = []\n\nfor episode in range(1000):\n    state = env.reset()\n    s     = state[0] * GridWorld.SIZE + state[1]   # flatten to int\n    total_reward = 0\n\n    for _ in range(50):   # max steps per episode\n        # Epsilon-greedy: explore randomly or exploit Q table\n        if random.random() < epsilon:\n            action = random.randint(0, n_actions - 1)\n        else:\n            action = np.argmax(Q[s])\n\n        next_state, reward, done = env.step(action)\n        s_next = next_state[0] * GridWorld.SIZE + next_state[1]\n\n        # Bellman update: Q(s,a) <- Q(s,a) + alpha * (r + gamma*max Q(s') - Q(s,a))\n        Q[s, action] += alpha * (\n            reward + gamma * np.max(Q[s_next]) - Q[s, action]\n        )\n        s = s_next\n        total_reward += reward\n        if done:\n            break\n\n    epsilon = max(eps_min, epsilon * eps_decay)\n    episode_rewards.append(total_reward)\n\n# Plot learning curve\nwindow = 50\nsmoothed = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')\nfig, ax = plt.subplots(figsize=(10, 4))\nax.plot(episode_rewards, alpha=0.3, color='#2E75B6', linewidth=0.8)\nax.plot(range(window-1, len(episode_rewards)), smoothed, '#E8722A', linewidth=2)\nax.set_xlabel('Episode')\nax.set_ylabel('Total Reward')\nax.set_title('Q-Learning on Grid World: Learning Curve\\n(orange = 50-episode moving average)')\nplt.tight_layout()\nplt.show()\n\n# Show learned policy\naction_symbols = ['↑', '↓', '←', '→']\nprint('Learned policy (greedy from Q table):')\nfor r in range(GridWorld.SIZE):\n    row = ''\n    for c in range(GridWorld.SIZE):\n        if   (r, c) == GridWorld.GOAL:       row += ' G '\n        elif (r, c) in GridWorld.HOLES:      row += ' X '\n        else:\n            s   = r * GridWorld.SIZE + c\n            row += f' {action_symbols[np.argmax(Q[s])]} '\n    print(row)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## D.2 -- Deep Q-Network (DQN) for CartPole\n\nQ-learning with a table works only for small, discrete state spaces.\n**DQN** replaces the Q-table with a neural network that approximates Q(s, a)\nfor continuous or high-dimensional state spaces.\n\nTwo tricks make DQN stable:\n\n**Experience replay:** store transitions (s, a, r, s') in a buffer and\nsample random mini-batches. This breaks the temporal correlation between\nconsecutive training samples that would otherwise make training unstable.\n\n**Target network:** maintain a separate copy of the Q-network whose weights\nare updated slowly. Using the same network for both predictions and targets\ncreates a moving target that causes divergence.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# D.2.1 -- DQN agent for CartPole-v1\n# CartPole: balance a pole on a cart by pushing left or right\n# State: 4 floats (cart pos, cart vel, pole angle, pole vel)\n# Actions: 0=push left, 1=push right\n# Episode ends when pole falls or cart moves too far\n# Solved when average reward >= 475 over 100 episodes\n\nTransition = namedtuple('Transition', ['state','action','reward','next_state','done'])\n\nclass ReplayBuffer:\n    def __init__(self, capacity=10_000):\n        self.buffer = deque(maxlen=capacity)\n    def push(self, *args):\n        self.buffer.append(Transition(*args))\n    def sample(self, batch_size):\n        return random.sample(self.buffer, batch_size)\n    def __len__(self):\n        return len(self.buffer)\n\n\nclass DQN(nn.Module):\n    def __init__(self, n_obs, n_actions):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_obs, 128), nn.ReLU(),\n            nn.Linear(128, 128),   nn.ReLU(),\n            nn.Linear(128, n_actions)\n        )\n    def forward(self, x):\n        return self.net(x)\n\n\nenv_cp      = gym.make('CartPole-v1')\nn_obs       = env_cp.observation_space.shape[0]   # 4\nn_actions_cp = env_cp.action_space.n              # 2\n\npolicy_net = DQN(n_obs, n_actions_cp).to(DEVICE)\ntarget_net = DQN(n_obs, n_actions_cp).to(DEVICE)\ntarget_net.load_state_dict(policy_net.state_dict())\ntarget_net.eval()\n\noptimizer_dqn = optim.AdamW(policy_net.parameters(), lr=1e-3)\nbuffer    = ReplayBuffer(10_000)\n\n# Hyperparameters\nBATCH_SIZE    = 128\nGAMMA_DQN     = 0.99\nEPS_START     = 1.0\nEPS_END       = 0.05\nEPS_DECAY     = 500     # steps to decay over\nTARGET_UPDATE = 20      # update target network every N episodes\nN_EPISODES_DQN = 400\n\nepisode_durations = []\nsteps_done = 0\n\ndef select_action(state, steps_done):\n    eps = EPS_END + (EPS_START - EPS_END) * np.exp(-steps_done / EPS_DECAY)\n    if random.random() < eps:\n        return random.randint(0, n_actions_cp - 1)\n    with torch.no_grad():\n        s = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n        return policy_net(s).argmax(1).item()\n\ndef optimise_model():\n    if len(buffer) < BATCH_SIZE:\n        return\n    batch      = buffer.sample(BATCH_SIZE)\n    states     = torch.tensor(np.array([t.state      for t in batch]), dtype=torch.float32).to(DEVICE)\n    actions    = torch.tensor([t.action    for t in batch], dtype=torch.long).unsqueeze(1).to(DEVICE)\n    rewards    = torch.tensor([t.reward    for t in batch], dtype=torch.float32).to(DEVICE)\n    next_states= torch.tensor(np.array([t.next_state for t in batch]), dtype=torch.float32).to(DEVICE)\n    dones      = torch.tensor([t.done      for t in batch], dtype=torch.float32).to(DEVICE)\n\n    # Current Q values\n    q_values = policy_net(states).gather(1, actions).squeeze(1)\n\n    # Target Q values (Bellman equation)\n    with torch.no_grad():\n        next_q = target_net(next_states).max(1).values\n        targets = rewards + GAMMA_DQN * next_q * (1 - dones)\n\n    loss = nn.SmoothL1Loss()(q_values, targets)\n    optimizer_dqn.zero_grad()\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 100)\n    optimizer_dqn.step()\n\nprint(f'Training DQN on CartPole-v1 for {N_EPISODES_DQN} episodes...')\n\nfor episode in range(N_EPISODES_DQN):\n    state, _ = env_cp.reset(seed=episode)\n    for t in range(500):\n        action = select_action(state, steps_done)\n        next_state, reward, terminated, truncated, _ = env_cp.step(action)\n        done = terminated or truncated\n        buffer.push(state, action, reward, next_state, float(done))\n        state = next_state\n        steps_done += 1\n        optimise_model()\n        if done:\n            episode_durations.append(t + 1)\n            break\n    if episode % TARGET_UPDATE == 0:\n        target_net.load_state_dict(policy_net.state_dict())\n\nenv_cp.close()\n\nwindow = 50\nsmoothed_dqn = np.convolve(episode_durations, np.ones(window)/window, mode='valid')\nfig, ax = plt.subplots(figsize=(10, 4))\nax.plot(episode_durations, alpha=0.3, color='#2E75B6', linewidth=0.8)\nax.plot(range(window-1, len(episode_durations)), smoothed_dqn, '#E8722A', linewidth=2)\nax.axhline(475, color='green', linestyle='--', linewidth=1.5, label='Solved threshold (475)')\nax.set_xlabel('Episode')\nax.set_ylabel('Duration (steps survived)')\nax.set_title('DQN on CartPole-v1\\n(agent learns to balance the pole)')\nax.legend()\nplt.tight_layout()\nplt.show()\n\nfinal_avg = np.mean(episode_durations[-100:])\nsolved    = 'SOLVED' if final_avg >= 475 else f'avg={final_avg:.0f} (not yet solved -- try more episodes)'\nprint(f'Last 100 episodes average duration: {final_avg:.0f}  --  {solved}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## D.3 -- RL Summary and Where to Go Next\n\n### Key Takeaways\n\n- RL learns from interaction, not labelled data. The reward signal is the only supervision.\n- The **Bellman equation** is the mathematical foundation: the value of an action\n  equals its immediate reward plus the discounted value of the best next action.\n- **Epsilon-greedy** balances exploration (try new things) and exploitation (use what you know).\n- **Experience replay** breaks temporal correlations; **target networks** stabilise training.\n  Both are necessary for DQN to converge.\n- CartPole is solved when the agent keeps the pole balanced for 475+ steps consistently.\n\n### Beyond DQN\n\n- **Policy gradient methods** (REINFORCE, PPO, A3C) -- optimise the policy directly\n  rather than learning a Q-function. Better for continuous action spaces.\n- **Actor-Critic (A2C/A3C)** -- combines value-based and policy-based methods.\n- **Proximal Policy Optimisation (PPO)** -- the most widely used RL algorithm\n  in production (used by OpenAI for ChatGPT's RLHF training).\n- **Stable-Baselines3** -- the standard Python RL library;\n  provides PPO, SAC, TD3 with a scikit-learn-style API.\n\n---\n\n*End of Appendix D -- Python for AI/ML*  \n[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n"
    }
  ]
}