{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Appendix C -- Project Ideas and Further Reading\n## *Python for AI/ML: A Complete Learning Journey*\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/APP_C_Projects_and_Further_Reading.ipynb)\n&nbsp;&nbsp;[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n\n---\n\nThis appendix provides ten capstone project ideas using the SO 2025 dataset,\nfollowed by curated further reading and course recommendations for each\nmajor topic in the book.\n\nAll ten projects use data you already have. Each is scoped to be completable\nin a weekend for a motivated reader who has finished the main chapters.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## C.1 -- Ten Capstone Projects\n\n### Project 1 -- Salary Predictor Web App\n\n**Difficulty:** Intermediate  \n**Chapters used:** 3, 6  \n**Goal:** Deploy the Chapter 6 salary regression model as an interactive web app\nwhere a user enters their profile (years experience, country, languages) and\nreceives a salary prediction with a confidence interval.\n\n**Key steps:**\n1. Train the best Chapter 6 model and save it with `joblib`\n2. Build a `Gradio` or `Streamlit` front-end with input widgets\n3. Display prediction, confidence interval, and SHAP explanation\n4. Deploy to Hugging Face Spaces (free)\n\n**Stretch goal:** Add a 'compare yourself to similar developers' feature\nshowing the user's predicted salary vs the median for their country and role.\n\n---\n\n### Project 2 -- Developer Trend Analysis (2019-2025)\n\n**Difficulty:** Beginner-Intermediate  \n**Chapters used:** 3, 4, 5  \n**Goal:** Download multiple years of SO survey data and track how language\npopularity, salary, and AI tool adoption have changed over time.\n\n**Key steps:**\n1. Download SO surveys from 2019, 2021, 2023, and 2025\n2. Harmonise column names across years (they change frequently)\n3. Build time-series charts for Python vs JavaScript vs SQL adoption\n4. Test whether salary trends differ significantly by year (Chapter 5 ANOVA)\n\n**Stretch goal:** Forecast 2026 language adoption using scipy curve fitting.\n\n---\n\n### Project 3 -- AI Tool Adoption Classifier\n\n**Difficulty:** Intermediate  \n**Chapters used:** 6, 8  \n**Goal:** Build a model that predicts which AI tools a developer is likely\nto adopt based on their role, experience, and current language stack.\n\n**Key steps:**\n1. Explode the `AIToolCurrently` column into binary flags per tool\n2. Build a multi-label classifier (one binary classifier per tool)\n3. Evaluate with precision/recall per tool\n4. Identify which features most predict AI tool adoption (SHAP)\n\n**Stretch goal:** Use Chapter 8 zero-shot classification to tag free-text\njob descriptions with likely AI tool affinity.\n\n---\n\n### Project 4 -- Compensation Equity Audit Tool\n\n**Difficulty:** Intermediate-Advanced  \n**Chapters used:** 5, 6, 9  \n**Goal:** Build a reusable audit tool that takes any tabular dataset with\na salary column and a group column, and outputs a structured fairness report.\n\n**Key steps:**\n1. Generalise the Chapter 9 audit code into a `FairnessAuditor` class\n2. Compute demographic parity, equalised odds, and calibration automatically\n3. Generate a PDF report with charts and statistical test results\n4. Apply to the SO 2025 dataset with Country, EdLevel, and RemoteWork as groups\n\n**Stretch goal:** Add a mitigation recommendation engine that suggests\nreweighting, resampling, or threshold adjustment based on the disparity type.\n\n---\n\n### Project 5 -- Developer Archetype Deep-Dive\n\n**Difficulty:** Intermediate  \n**Chapters used:** 3, 4, 6  \n**Goal:** Extend the Chapter 6 KMeans clustering into a full developer\narchetype analysis with richer features and interpretable profiles.\n\n**Key steps:**\n1. Include 15+ binary language and tool flags as clustering features\n2. Try k=5 and k=8 clusters; use silhouette score to choose\n3. Name each cluster based on its top features\n4. Build an interactive Plotly dashboard showing cluster profiles\n\n**Stretch goal:** Assign new respondents to clusters using `model.predict()`\nand build a 'which developer archetype are you?' quiz.\n\n---\n\n### Project 6 -- Salary Regression with Deep Learning\n\n**Difficulty:** Advanced  \n**Chapters used:** 6, 7  \n**Goal:** Push the Chapter 7 MLP further with richer features, architecture\nsearch, and a rigorous comparison against the Chapter 6 Random Forest.\n\n**Key steps:**\n1. One-hot encode Country and EdLevel; concatenate with numeric features\n2. Try embedding layers for high-cardinality categoricals (Country has 100+ values)\n3. Implement manual early stopping and learning rate warm-up\n4. Compare Random Forest vs MLP at equal feature sets\n\n**Stretch goal:** Implement a simple neural architecture search over\nhidden layer sizes and depths using RandomizedSearchCV-style random sampling.\n\n---\n\n### Project 7 -- NLP: Mining Developer Sentiment at Scale\n\n**Difficulty:** Advanced  \n**Chapters used:** 8  \n**Goal:** Apply the Chapter 8 sentiment pipeline to a larger corpus of\ndeveloper text (Stack Overflow questions, GitHub issues, or Reddit r/programming)\nand track sentiment trends by topic.\n\n**Key steps:**\n1. Collect data using the Stack Overflow API or Reddit API\n2. Run the Chapter 8 sentiment pipeline in batches\n3. Aggregate sentiment by topic (Kubernetes, React, Python, etc.)\n4. Visualise sentiment trends over time\n\n**Stretch goal:** Fine-tune a model on domain-specific developer sentiment\nrather than the general SST-2 model.\n\n---\n\n### Project 8 -- Reproducible ML Pipeline with MLflow\n\n**Difficulty:** Intermediate  \n**Chapters used:** 6  \n**Goal:** Add experiment tracking to the Chapter 6 pipeline using MLflow,\nso every training run is logged with parameters, metrics, and artifacts.\n\n**Key steps:**\n1. `pip install mlflow`\n2. Wrap the Chapter 6 training loop with `mlflow.start_run()`\n3. Log hyperparameters with `mlflow.log_param()`, metrics with `mlflow.log_metric()`\n4. Save the model with `mlflow.sklearn.log_model()`\n5. Open the MLflow UI to compare runs\n\n**Stretch goal:** Implement a hyperparameter sweep with 50 runs and\nidentify the Pareto frontier of accuracy vs training time.\n\n---\n\n### Project 9 -- Time-to-First-Job Predictor\n\n**Difficulty:** Intermediate  \n**Chapters used:** 3, 5, 6  \n**Goal:** Use the SO 2025 `YearsCode` vs `YearsCodePro` columns to estimate\ntime from first coding to first professional role, and model what factors\npredict a faster transition.\n\n**Key steps:**\n1. Engineer `time_to_pro = YearsCode - YearsCodePro` (careful with edge cases)\n2. Analyse distribution by education level, country, and primary language\n3. Build a regression model predicting `time_to_pro`\n4. Identify the features most associated with faster professional entry\n\n**Stretch goal:** Use survival analysis (`lifelines` library) to model\ntime-to-employment as a censored event.\n\n---\n\n### Project 10 -- End-to-End Book Capstone\n\n**Difficulty:** Advanced  \n**Chapters used:** All  \n**Goal:** Build a complete, deployed ML product that uses techniques from\nevery chapter of the book.\n\n**Suggested product:** A 'Developer Profile Analyser' that:\n- Loads and cleans SO 2025 data (Ch 3)\n- Produces an EDA report with charts (Ch 4)\n- Runs statistical tests on group differences (Ch 5)\n- Predicts salary with a tuned sklearn pipeline (Ch 6)\n- Compares with a PyTorch MLP (Ch 7)\n- Classifies developer type from free-text role description (Ch 8)\n- Audits predictions for fairness and generates a model card (Ch 9)\n- Deploys as a Gradio app on HuggingFace Spaces\n\nThis project is your portfolio centrepiece. It demonstrates every skill\ncovered in the book in an integrated, working product.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## C.2 -- Further Reading and Resources\n\n### Foundational Books\n\n**Python and Data Science:**\n- *Python for Data Analysis* -- Wes McKinney (the Pandas creator; essential reference)\n- *Python Data Science Handbook* -- Jake VanderPlas (free online at jakevdp.github.io)\n- *Fluent Python* -- Luciano Ramalho (deep Python internals; read after finishing this book)\n\n**Machine Learning:**\n- *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow* -- Aurélien Géron\n- *The Elements of Statistical Learning* -- Hastie, Tibshirani, Friedman (free PDF; rigorous)\n- *Pattern Recognition and Machine Learning* -- Bishop (Bayesian perspective)\n\n**Deep Learning:**\n- *Deep Learning* -- Goodfellow, Bengio, Courville (free at deeplearningbook.org)\n- *Dive into Deep Learning* -- Zhang et al. (free at d2l.ai; PyTorch-native)\n\n**NLP and Transformers:**\n- *Natural Language Processing with Transformers* -- Lewis Tunstall et al. (HuggingFace team)\n- *Speech and Language Processing* -- Jurafsky and Martin (free draft at web.stanford.edu/~jurafsky)\n\n**Ethics and Responsible AI:**\n- *Weapons of Math Destruction* -- Cathy O'Neil (accessible; real-world cases)\n- *The Alignment Problem* -- Brian Christian (AI safety and values)\n- *Data Feminism* -- Catherine D'Ignazio and Lauren Klein (power and data)\n\n---\n\n### Online Courses\n\n**Free:**\n- fast.ai Practical Deep Learning for Coders -- practical-first, PyTorch\n- CS231n (Stanford) -- Convolutional Neural Networks; lectures on YouTube\n- CS224n (Stanford) -- NLP with Deep Learning; lectures on YouTube\n- HuggingFace NLP Course -- huggingface.co/learn/nlp-course\n- Kaggle Learn -- short, practical courses on ML, feature engineering, deep learning\n\n**Paid:**\n- deeplearning.ai Specialisations (Coursera) -- Andrew Ng; systematic and rigorous\n- Full Stack Deep Learning (fullstackdeeplearning.com) -- production ML systems\n\n---\n\n### Key Papers\n\n**Transformers:**\n- *Attention Is All You Need* -- Vaswani et al. 2017 (the transformer paper)\n- *BERT: Pre-training of Deep Bidirectional Transformers* -- Devlin et al. 2019\n- *Language Models are Few-Shot Learners* -- Brown et al. 2020 (GPT-3)\n\n**Fairness and Ethics:**\n- *Model Cards for Model Reporting* -- Mitchell et al. 2019 (Google)\n- *A Framework for Understanding Unintended Consequences of ML* -- Suresh and Guttag 2021\n- *Fairness and Abstraction in Sociotechnical Systems* -- Selbst et al. 2019\n\n**Explainability:**\n- *A Unified Approach to Interpreting Model Predictions* -- Lundberg and Lee 2017 (SHAP)\n- *Why Should I Trust You? Explaining the Predictions of Any Classifier* -- Ribeiro et al. 2016 (LIME)\n\n---\n\n### Communities and Practice\n\n- **Kaggle** -- competitions, notebooks, and datasets; essential for building intuition fast\n- **Papers With Code** -- every paper linked to its implementation; great for staying current\n- **HuggingFace Hub** -- 500k+ models and datasets; browse what is possible\n- **r/MachineLearning** -- research discussion; high signal-to-noise\n- **Stack Overflow** -- specific implementation questions; the dataset source for this book\n\n---\n\n## C.3 -- Your Learning Path Forward\n\nYou have completed a book that takes you from Python basics to fine-tuning\na transformer. That puts you in the top tier of what most online courses cover.\n\nThe honest next steps, in order of impact:\n\n**1. Build something.** Pick one of the ten projects above and complete it.\n   A deployed, working project teaches more than three more courses.\n\n**2. Enter a Kaggle competition.** The structured feedback loop of a leaderboard\n   reveals gaps in your knowledge faster than any other mechanism.\n\n**3. Read one paper per week.** Start with the papers listed above.\n   Reading the original source builds understanding that tutorials cannot.\n\n**4. Contribute to open source.** Fix a bug in a library you use.\n   Reading production-quality code accelerates your own code quality.\n\n**5. Teach someone else.** Writing a blog post or explaining a concept to a\n   colleague exposes every gap in your own understanding.\n\n---\n\n*End of Appendix C -- Python for AI/ML*  \n[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n"
    }
  ]
}