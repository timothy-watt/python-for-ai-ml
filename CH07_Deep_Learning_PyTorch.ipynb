{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Chapter 7 -- Deep Learning with PyTorch\n## *Python for AI/ML: A Complete Learning Journey*\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/CH07_Deep_Learning_PyTorch.ipynb)\n&nbsp;&nbsp;[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n\n---\n\n**Part:** 3 -- Machine Learning and AI  \n**Prerequisites:** Chapter 6 (Machine Learning with scikit-learn)  \n**Estimated time:** 6-7 hours\n\n---\n\n### Learning Objectives\n\nBy the end of this chapter you will be able to:\n\n- Explain what a tensor is and why PyTorch uses them instead of NumPy arrays\n- Create and manipulate tensors: indexing, reshaping, device transfer\n- Build a neural network with `nn.Module`, `nn.Linear`, and activation functions\n- Write a complete training loop: forward pass, loss, backward pass, optimiser step\n- Use `DataLoader` and `Dataset` for efficient batched training\n- Apply regularisation: dropout, weight decay, batch normalisation\n- Diagnose overfitting with training vs validation loss curves\n- Save and load model weights with `torch.save` and `torch.load`\n\n---\n\n### Project Thread -- Chapter 7\n\nTwo neural networks trained on SO 2025 data:\n\n1. **Salary regression MLP** -- a multi-layer perceptron that predicts log salary\n   from developer profile features, compared against the Chapter 6 Random Forest baseline\n2. **Python usage classifier MLP** -- a binary classifier that predicts Python adoption,\n   compared against the Chapter 6 Gradient Boosting baseline\n\nBoth networks are built from scratch -- no high-level Keras-style wrappers.\nEvery component is explicit so you understand what the framework does for you.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Setup -- Imports, Device, and Data\n\n\n> **Before running this notebook:** go to **Runtime → Change runtime type → T4 GPU**.\n> The two training loops in Sections 7.4 and 7.5 work on CPU but run 3-5x faster on GPU.\n> If T4 is unavailable, CPU will still complete in a few minutes."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score, accuracy_score, classification_report\n\nprint(f'PyTorch: {torch.__version__}')\nprint(f'CUDA available: {torch.cuda.is_available()}')\n\n# Use GPU if available, otherwise CPU\n# In Colab: Runtime -> Change runtime type -> T4 GPU for faster training\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {DEVICE}')\n\nRANDOM_STATE = 42\ntorch.manual_seed(RANDOM_STATE)\nnp.random.seed(RANDOM_STATE)\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.dpi']       = 110\nplt.rcParams['axes.titlesize']   = 13\nplt.rcParams['axes.titleweight'] = 'bold'\n\nDATASET_URL = 'https://raw.githubusercontent.com/timothy-watt/python-for-ai-ml/main/data/so_survey_2025_curated.csv'\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Load and clean SO 2025 -- identical pipeline to Chapter 6\ndf_raw = pd.read_csv(DATASET_URL)\ndf = df_raw.copy()\ndf = df.dropna(subset=['ConvertedCompYearly'])\ndf['ConvertedCompYearly'] = pd.to_numeric(df['ConvertedCompYearly'], errors='coerce')\nQ1, Q3 = df['ConvertedCompYearly'].quantile([0.25, 0.75])\nIQR = Q3 - Q1\ndf = df[\n    (df['ConvertedCompYearly'] >= max(Q1 - 3*IQR, 5_000)) &\n    (df['ConvertedCompYearly'] <= min(Q3 + 3*IQR, 600_000))\n].copy()\nif 'YearsCodePro' in df.columns:\n    df['YearsCodePro'] = pd.to_numeric(df['YearsCodePro'], errors='coerce')\n    df['YearsCodePro'] = df['YearsCodePro'].fillna(df['YearsCodePro'].median())\nfor col in ['Country', 'EdLevel', 'Employment', 'RemoteWork']:\n    if col in df.columns:\n        df[col] = df[col].fillna('Unknown')\ndf['uses_python'] = df.get('LanguageHaveWorkedWith', pd.Series(dtype=str)).str.contains('Python', na=False).astype(int)\ndf['uses_sql']    = df.get('LanguageHaveWorkedWith', pd.Series(dtype=str)).str.contains('SQL', na=False).astype(int)\ndf['uses_js']     = df.get('LanguageHaveWorkedWith', pd.Series(dtype=str)).str.contains('JavaScript', na=False).astype(int)\ndf['uses_ai']     = df.get('AIToolCurrently', pd.Series(dtype=str)).notna().astype(int)\ndf['log_salary']  = np.log(df['ConvertedCompYearly'])\ndf = df.reset_index(drop=True)\nprint(f'Dataset ready: {len(df):,} rows')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 7.1 -- Tensors: PyTorch's Core Data Structure\n\nA **tensor** is an n-dimensional array -- identical in structure to a NumPy array,\nbut with two crucial additions:\n\n1. **Device awareness:** a tensor can live on CPU or GPU. Moving it to GPU (`tensor.to('cuda')`)\n   makes all operations on it run on the GPU automatically -- no code changes needed.\n2. **Autograd:** PyTorch tracks every operation on tensors with `requires_grad=True`,\n   building a computation graph that enables automatic differentiation.\n   This is what makes backpropagation possible without manual gradient calculation.\n\nEverything else in PyTorch -- layers, optimisers, losses -- operates on tensors.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.1.1 -- Creating tensors\n\n# From a Python list\nt1 = torch.tensor([1.0, 2.0, 3.0, 4.0])\nprint(f't1: {t1}  dtype={t1.dtype}  shape={t1.shape}')\n\n# From a NumPy array -- shares memory when on CPU (no copy)\narr = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\nt2  = torch.from_numpy(arr)\nprint(f't2 shape: {t2.shape}  dtype={t2.dtype}')\n\n# Factory functions\nzeros = torch.zeros(3, 4)          # 3x4 matrix of 0.0\nones  = torch.ones(2, 5)           # 2x5 matrix of 1.0\nrand  = torch.rand(3, 3)           # uniform [0, 1)\nrandn = torch.randn(3, 3)          # standard normal\neye   = torch.eye(4)               # 4x4 identity matrix\nprint(f'zeros shape: {zeros.shape}')\nprint(f'randn:\\n{randn.numpy().round(3)}')\n\n# Specifying dtype explicitly\nt_float32 = torch.tensor([1, 2, 3], dtype=torch.float32)  # default for neural nets\nt_int64   = torch.tensor([1, 2, 3], dtype=torch.int64)    # required for class labels\nprint(f'float32: {t_float32.dtype}  int64: {t_int64.dtype}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.1.2 -- Tensor operations: indexing, reshaping, device transfer\n\nt = torch.randn(4, 3)\nprint(f'Original shape: {t.shape}')\nprint(f'First row:      {t[0]}')\nprint(f'Col 1:          {t[:, 1]}')\nprint(f'Element [2,1]:  {t[2, 1].item():.4f}')  # .item() extracts Python scalar\n\n# Reshaping\nflat   = t.reshape(-1)           # flatten to 1D (-1 = infer size)\nt_2x6  = t.reshape(2, 6)        # reshape to 2x6\nt_T    = t.T                     # transpose\nprint(f'Flattened: {flat.shape}  Reshaped: {t_2x6.shape}  Transposed: {t_T.shape}')\n\n# Arithmetic -- all operations are element-wise unless you use @\na = torch.tensor([1.0, 2.0, 3.0])\nb = torch.tensor([4.0, 5.0, 6.0])\nprint(f'a + b:     {a + b}')\nprint(f'a * b:     {a * b}')    # element-wise, NOT dot product\nprint(f'a @ b:     {a @ b}')    # dot product (inner product)\nprint(f'a.dot(b):  {torch.dot(a, b)}')\n\n# Matrix multiplication\nW = torch.randn(3, 5)   # weight matrix\nx = torch.randn(5)      # input vector\ny = W @ x               # matrix-vector product: shape (3,)\nprint(f'W @ x shape: {y.shape}')\n\n# Device transfer -- move to GPU if available\nt_device = t.to(DEVICE)\nprint(f'Tensor device: {t_device.device}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.1.3 -- Autograd: automatic differentiation\n#\n# When requires_grad=True, PyTorch records every operation on the tensor.\n# Calling .backward() on the final scalar loss computes gradients\n# for all tensors in the computation graph that have requires_grad=True.\n# This is the entire mathematical basis of neural network training.\n\n# Simple example: compute d/dx of f(x) = 3x^2 + 2x + 1 at x=2\nx = torch.tensor(2.0, requires_grad=True)  # leaf variable\nf = 3 * x**2 + 2 * x + 1                  # builds computation graph\n\nf.backward()   # computes df/dx\n\nprint(f'f(2)   = {f.item():.1f}  (expected: 3*4 + 2*2 + 1 = 17)')\nprint(f'df/dx  = {x.grad.item():.1f}  (expected: 6x + 2 at x=2 = 14)')\n\n# In a neural network, x becomes the model weights, f becomes the loss.\n# .backward() fills .grad on every weight tensor, then the optimiser\n# uses those gradients to update the weights.\n\n# torch.no_grad() disables gradient tracking -- use during inference\n# to save memory and speed up forward passes when we do not need gradients\nwith torch.no_grad():\n    y = 3 * x**2 + 2 * x + 1\n    print(f'Under no_grad: y={y.item():.1f}, grad_fn={y.grad_fn}')  # None\n\n# Converting between tensor and numpy\nt = torch.tensor([1.0, 2.0, 3.0])\narr = t.detach().numpy()    # .detach() removes from computation graph first\nprint(f'Tensor -> numpy: {arr}  type={type(arr).__name__}')\nback = torch.from_numpy(arr)\nprint(f'numpy -> tensor: {back}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 7.2 -- Building Neural Networks with nn.Module\n\n`nn.Module` is the base class for every neural network in PyTorch.\nYou subclass it, define layers in `__init__`, and implement the forward pass in `forward()`.\nPyTorch then handles parameter registration, gradient tracking, device transfer,\nserialisation, and training/eval mode switching automatically.\n\nThis is the same OOP pattern from Chapter 2 -- a class with `__init__` storing\nconfiguration and learned state, and methods implementing behaviour.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.2.1 -- A minimal MLP (Multi-Layer Perceptron)\n\nclass SalaryMLP(nn.Module):\n    \"\"\"\n    Multi-layer perceptron for salary regression.\n\n    Architecture:\n        Input -> Linear -> BatchNorm -> ReLU -> Dropout\n               -> Linear -> BatchNorm -> ReLU -> Dropout\n               -> Linear -> output\n\n    Parameters\n    ----------\n    input_dim  : int   -- number of input features\n    hidden_dims: list  -- number of neurons in each hidden layer\n    dropout    : float -- dropout probability (0 = disabled)\n    \"\"\"\n\n    def __init__(self, input_dim, hidden_dims=(128, 64, 32), dropout=0.3):\n        super().__init__()   # must call parent __init__\n\n        layers = []\n        prev_dim = input_dim\n\n        for hidden_dim in hidden_dims:\n            layers += [\n                nn.Linear(prev_dim, hidden_dim),  # learnable weight matrix + bias\n                nn.BatchNorm1d(hidden_dim),        # normalise activations per batch\n                nn.ReLU(),                         # non-linearity: max(0, x)\n                nn.Dropout(p=dropout),             # randomly zero p% of activations\n            ]\n            prev_dim = hidden_dim\n\n        layers.append(nn.Linear(prev_dim, 1))   # output layer: single salary prediction\n\n        # nn.Sequential chains layers -- forward() calls them in order\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass: compute predictions from input tensor x.\n        Called automatically when you do: output = model(x)\n        \"\"\"\n        return self.network(x).squeeze(-1)   # squeeze removes the trailing dim-1\n\n\n# Instantiate the model and move to the target device\ninput_dim   = 5   # placeholder -- will be set properly in section 7.3\nsalary_net  = SalaryMLP(input_dim=input_dim, hidden_dims=(128, 64, 32), dropout=0.3)\nsalary_net  = salary_net.to(DEVICE)\n\nprint(salary_net)\nprint()\n\n# Count trainable parameters\nn_params = sum(p.numel() for p in salary_net.parameters() if p.requires_grad)\nprint(f'Trainable parameters: {n_params:,}')\n\n# Test with a random batch\nx_test  = torch.randn(8, input_dim).to(DEVICE)   # batch of 8 samples\ny_test  = salary_net(x_test)\nprint(f'Input shape:  {x_test.shape}')\nprint(f'Output shape: {y_test.shape}')   # (8,) -- one prediction per sample\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.2.2 -- Loss functions and optimisers\n#\n# A loss function measures how wrong the model's predictions are.\n# The optimiser adjusts model weights to reduce the loss.\n\n# Common loss functions:\n#   nn.MSELoss()    -- Mean Squared Error: regression tasks\n#   nn.MAELoss()    -- Mean Absolute Error: regression, robust to outliers\n#   nn.BCEWithLogitsLoss() -- Binary Cross-Entropy: binary classification\n#   nn.CrossEntropyLoss()  -- Multi-class classification\n\ncriterion = nn.MSELoss()   # for log-salary regression\n\n# Common optimisers:\n#   optim.SGD        -- Stochastic Gradient Descent (baseline)\n#   optim.Adam       -- Adaptive Moment Estimation (default choice)\n#   optim.AdamW      -- Adam with decoupled weight decay (better regularisation)\n\noptimizer = optim.AdamW(\n    salary_net.parameters(),\n    lr=1e-3,           # learning rate: step size for weight updates\n    weight_decay=1e-4  # L2 regularisation: penalises large weights\n)\n\n# Demonstrate one manual forward + backward pass\nx_demo  = torch.randn(16, input_dim).to(DEVICE)\ny_demo  = torch.randn(16).to(DEVICE)          # fake targets\n\noptimizer.zero_grad()          # clear gradients from the previous step\ny_pred  = salary_net(x_demo)   # forward pass\nloss    = criterion(y_pred, y_demo)  # compute loss\nloss.backward()                # backward pass: compute all gradients\noptimizer.step()               # update weights using gradients\n\nprint(f'One manual training step completed.')\nprint(f'Loss: {loss.item():.6f}')\nprint(f'Output grad_fn: {y_pred.grad_fn.__class__.__name__}')  # SqueezeBackward\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 7.3 -- Dataset and DataLoader\n\nPyTorch's `Dataset` and `DataLoader` handle the mechanics of batched training:\nshuffling, batching, and multi-process data loading. They are the equivalent\nof scikit-learn's `Pipeline` for the data side of training.\n\nYou subclass `Dataset` and implement three methods:\n- `__len__`: how many samples in the dataset\n- `__getitem__`: return one (features, label) pair by index\n\n`DataLoader` wraps the dataset and yields batches during the training loop.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.3.1 -- Custom Dataset for SO 2025\n\nclass SurveyDataset(Dataset):\n    \"\"\"\n    PyTorch Dataset wrapping the SO 2025 feature matrix and targets.\n\n    Parameters\n    ----------\n    X : np.ndarray -- feature matrix (already scaled)\n    y : np.ndarray -- target vector\n    \"\"\"\n\n    def __init__(self, X, y):\n        # Convert numpy arrays to float32 tensors\n        # float32 is the standard dtype for neural network weights and activations\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        # Called by DataLoader to fetch one sample\n        # Returns a (features, label) tuple\n        return self.X[idx], self.y[idx]\n\n\n# Build the feature matrix for regression\nfeature_cols = [c for c in ['YearsCodePro', 'uses_python', 'uses_sql', 'uses_js', 'uses_ai']\n                if c in df.columns]\nprint(f'Features: {feature_cols}')\n\nX_raw = df[feature_cols].copy()\nfor col in feature_cols:\n    med = X_raw[col].median()\n    X_raw[col] = X_raw[col].fillna(med if pd.notna(med) else 0)\ny_raw = df['log_salary'].values\n\n# Train/test split\nX_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n    X_raw.values, y_raw, test_size=0.2, random_state=RANDOM_STATE\n)\n\n# Scale features -- fit ONLY on training data\nfeat_scaler = StandardScaler()\nX_train_sc  = feat_scaler.fit_transform(X_train_np)\nX_test_sc   = feat_scaler.transform(X_test_np)\n\n# Create Dataset objects\ntrain_dataset = SurveyDataset(X_train_sc, y_train_np)\ntest_dataset  = SurveyDataset(X_test_sc,  y_test_np)\n\n# DataLoader: batches + shuffling\n# batch_size=256: process 256 samples per weight update\n# shuffle=True: randomise order each epoch (prevents the model memorising order)\ntrain_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\ntest_loader  = DataLoader(test_dataset,  batch_size=512, shuffle=False)\n\ninput_dim = X_train_sc.shape[1]\nprint(f'Train: {len(train_dataset):,} samples  ({len(train_loader)} batches of 256)')\nprint(f'Test:  {len(test_dataset):,} samples')\nprint(f'Input dim: {input_dim}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 7.4 -- The Training Loop\n\nThe training loop is the heart of deep learning. Every epoch it:\n\n1. Iterates over batches from the DataLoader\n2. Runs the **forward pass**: compute predictions\n3. Computes the **loss**: how wrong are the predictions?\n4. Runs the **backward pass**: compute gradients via backprop\n5. **Optimiser step**: update weights in the direction that reduces loss\n6. Runs a **validation pass** with `torch.no_grad()` to monitor overfitting\n\nThe train/validation loss curves are the most important diagnostic in deep learning.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.4.1 -- Reusable train_epoch and evaluate functions\n\ndef train_epoch(model, loader, criterion, optimizer):\n    \"\"\"\n    Run one full pass over the training data.\n    Returns mean training loss for this epoch.\n    \"\"\"\n    model.train()   # sets BatchNorm and Dropout to training mode\n    total_loss = 0.0\n\n    for X_batch, y_batch in loader:\n        X_batch = X_batch.to(DEVICE)\n        y_batch = y_batch.to(DEVICE)\n\n        optimizer.zero_grad()              # 1. clear old gradients\n        y_pred = model(X_batch)            # 2. forward pass\n        loss   = criterion(y_pred, y_batch)  # 3. compute loss\n        loss.backward()                    # 4. backward pass\n        optimizer.step()                   # 5. update weights\n\n        total_loss += loss.item() * len(X_batch)  # accumulate weighted loss\n\n    return total_loss / len(loader.dataset)\n\n\ndef evaluate(model, loader, criterion):\n    \"\"\"\n    Evaluate model on a DataLoader without updating weights.\n    Returns mean loss and all predictions as numpy arrays.\n    \"\"\"\n    model.eval()    # sets BatchNorm and Dropout to inference mode\n    total_loss = 0.0\n    all_preds  = []\n    all_targets = []\n\n    with torch.no_grad():   # disable gradient tracking for efficiency\n        for X_batch, y_batch in loader:\n            X_batch = X_batch.to(DEVICE)\n            y_batch = y_batch.to(DEVICE)\n            y_pred  = model(X_batch)\n            loss    = criterion(y_pred, y_batch)\n            total_loss += loss.item() * len(X_batch)\n            all_preds.append(y_pred.cpu().numpy())\n            all_targets.append(y_batch.cpu().numpy())\n\n    preds   = np.concatenate(all_preds)\n    targets = np.concatenate(all_targets)\n    return total_loss / len(loader.dataset), preds, targets\n\n\nprint('train_epoch() and evaluate() functions defined.')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.4.2 -- Train the salary regression MLP\n\n# Build fresh model\nsalary_net = SalaryMLP(\n    input_dim   = input_dim,\n    hidden_dims = (128, 64, 32),\n    dropout     = 0.3\n).to(DEVICE)\n\ncriterion  = nn.MSELoss()\noptimizer  = optim.AdamW(salary_net.parameters(), lr=1e-3, weight_decay=1e-4)\n\n# ReduceLROnPlateau: halve the learning rate if validation loss stops improving\nscheduler  = ReduceLROnPlateau(optimizer, mode='min', factor=0.5,\n                                patience=5, verbose=False)\n\nN_EPOCHS   = 60\ntrain_losses = []\nval_losses   = []\nbest_val     = float('inf')\nbest_weights = None\n\nprint(f'Training SalaryMLP for {N_EPOCHS} epochs on {DEVICE}...')\nprint(f'{\"Epoch\":>6}  {\"Train Loss\":>12}  {\"Val Loss\":>12}  {\"LR\":>10}')\nprint('-' * 44)\n\nfor epoch in range(1, N_EPOCHS + 1):\n    tr_loss          = train_epoch(salary_net, train_loader, criterion, optimizer)\n    val_loss, _, _   = evaluate(salary_net, test_loader, criterion)\n    scheduler.step(val_loss)\n\n    train_losses.append(tr_loss)\n    val_losses.append(val_loss)\n\n    # Save best weights (early stopping logic)\n    if val_loss < best_val:\n        best_val     = val_loss\n        best_weights = {k: v.clone() for k, v in salary_net.state_dict().items()}\n\n    if epoch % 10 == 0 or epoch == 1:\n        lr = optimizer.param_groups[0]['lr']\n        print(f'{epoch:>6}  {tr_loss:>12.6f}  {val_loss:>12.6f}  {lr:>10.2e}')\n\nprint(f'Best validation loss: {best_val:.6f}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.4.3 -- Evaluate and plot training curves\n\n# Restore best weights\nsalary_net.load_state_dict(best_weights)\n\n# Final evaluation\n_, y_pred_log, y_true_log = evaluate(salary_net, test_loader, criterion)\ny_pred_usd = np.exp(y_pred_log)\ny_true_usd = np.exp(y_true_log)\n\nr2  = r2_score(y_true_log, y_pred_log)\nmae = np.mean(np.abs(y_true_usd - y_pred_usd))\n\nprint(f'SalaryMLP Test Results:')\nprint(f'  R^2 (log scale): {r2:.4f}')\nprint(f'  MAE (USD):       ${mae:,.0f}')\n\n# Training curves\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nepochs = range(1, len(train_losses) + 1)\naxes[0].plot(epochs, train_losses, '#E8722A', linewidth=2, label='Train loss')\naxes[0].plot(epochs, val_losses,   '#2E75B6', linewidth=2, label='Val loss')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('MSE Loss (log salary)')\naxes[0].set_title('Training vs Validation Loss\\n(curves converging = good fit)')\naxes[0].legend()\n\naxes[1].scatter(y_true_usd/1000, y_pred_usd/1000, alpha=0.2, s=8, color='#2E75B6')\nlim = max(y_true_usd.max(), y_pred_usd.max()) / 1000\naxes[1].plot([0, lim], [0, lim], 'r--', linewidth=2, label='Perfect')\naxes[1].set_xlabel('Actual Salary ($k)')\naxes[1].set_ylabel('Predicted Salary ($k)')\naxes[1].set_title(f'Actual vs Predicted Salary\\nR^2={r2:.3f}, MAE=${mae/1000:.1f}k')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 7.5 -- Binary Classification MLP\n\nThe classifier network is identical to the regression network in architecture,\nwith two changes:\n\n- **Output layer:** 1 neuron with no activation (raw logit)\n- **Loss function:** `BCEWithLogitsLoss` -- combines sigmoid activation and\n  binary cross-entropy in a numerically stable single operation\n\nDuring inference, apply `torch.sigmoid()` to the raw logit to get a probability.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.5.1 -- Build classifier dataset and model\n\nclf_feature_cols = [c for c in ['YearsCodePro', 'ConvertedCompYearly',\n                                 'uses_sql', 'uses_js', 'uses_ai']\n                    if c in df.columns]\n\nX_clf_raw = df[clf_feature_cols].copy()\nfor col in clf_feature_cols:\n    med = X_clf_raw[col].median()\n    X_clf_raw[col] = X_clf_raw[col].fillna(med if pd.notna(med) else 0)\ny_clf_raw = df['uses_python'].values.astype(np.float32)\n\nX_tr_c, X_te_c, y_tr_c, y_te_c = train_test_split(\n    X_clf_raw.values, y_clf_raw, test_size=0.2,\n    random_state=RANDOM_STATE, stratify=y_clf_raw\n)\n\nclf_scaler  = StandardScaler()\nX_tr_c_sc   = clf_scaler.fit_transform(X_tr_c)\nX_te_c_sc   = clf_scaler.transform(X_te_c)\n\nclf_train_ds = SurveyDataset(X_tr_c_sc, y_tr_c)\nclf_test_ds  = SurveyDataset(X_te_c_sc, y_te_c)\nclf_train_loader = DataLoader(clf_train_ds, batch_size=256, shuffle=True)\nclf_test_loader  = DataLoader(clf_test_ds,  batch_size=512, shuffle=False)\n\nclf_input_dim = X_tr_c_sc.shape[1]\nprint(f'Classifier features: {clf_feature_cols}')\nprint(f'Class balance: Python={y_clf_raw.mean()*100:.1f}%')\n\n# The network architecture is identical -- only the loss changes\nclass_net = SalaryMLP(\n    input_dim   = clf_input_dim,\n    hidden_dims = (64, 32),\n    dropout     = 0.2\n).to(DEVICE)\n\nclf_criterion = nn.BCEWithLogitsLoss()   # sigmoid + BCE in one stable op\nclf_optimizer = optim.AdamW(class_net.parameters(), lr=1e-3, weight_decay=1e-4)\nclf_scheduler = ReduceLROnPlateau(clf_optimizer, mode='min', factor=0.5, patience=5)\n\nn_params = sum(p.numel() for p in class_net.parameters() if p.requires_grad)\nprint(f'Classifier parameters: {n_params:,}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.5.2 -- Train the classifier\n\nN_EPOCHS_CLF    = 50\nclf_train_losses = []\nclf_val_losses   = []\nclf_best_val     = float('inf')\nclf_best_weights = None\n\nprint(f'Training classifier for {N_EPOCHS_CLF} epochs on {DEVICE}...')\nprint(f'{\"Epoch\":>6}  {\"Train Loss\":>12}  {\"Val Loss\":>12}')\nprint('-' * 34)\n\nfor epoch in range(1, N_EPOCHS_CLF + 1):\n    tr_loss        = train_epoch(class_net, clf_train_loader, clf_criterion, clf_optimizer)\n    val_loss, _, _ = evaluate(class_net, clf_test_loader, clf_criterion)\n    clf_scheduler.step(val_loss)\n\n    clf_train_losses.append(tr_loss)\n    clf_val_losses.append(val_loss)\n\n    if val_loss < clf_best_val:\n        clf_best_val     = val_loss\n        clf_best_weights = {k: v.clone() for k, v in class_net.state_dict().items()}\n\n    if epoch % 10 == 0 or epoch == 1:\n        print(f'{epoch:>6}  {tr_loss:>12.6f}  {val_loss:>12.6f}')\n\nprint(f'Best val loss: {clf_best_val:.6f}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.5.3 -- Evaluate the classifier\n\nclass_net.load_state_dict(clf_best_weights)\n\n_, logits, y_true = evaluate(class_net, clf_test_loader, clf_criterion)\n\n# Convert raw logits to probabilities and binary predictions\nprobs  = 1 / (1 + np.exp(-logits))   # sigmoid manually for clarity\ny_pred = (probs >= 0.5).astype(int)\n\nacc = accuracy_score(y_true, y_pred)\nprint(f'Classifier Test Accuracy: {acc:.4f}  ({acc*100:.1f}%)')\nprint()\nprint(classification_report(y_true, y_pred, target_names=['Non-Python', 'Python']))\n\n# Loss curves\nfig, ax = plt.subplots(figsize=(9, 5))\nepochs_clf = range(1, len(clf_train_losses) + 1)\nax.plot(epochs_clf, clf_train_losses, '#E8722A', linewidth=2, label='Train loss')\nax.plot(epochs_clf, clf_val_losses,   '#2E75B6', linewidth=2, label='Val loss')\nax.set_xlabel('Epoch')\nax.set_ylabel('BCE Loss')\nax.set_title(f'Classifier Training Curves\\nFinal accuracy: {acc*100:.1f}%')\nax.legend()\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 7.6 -- Saving, Loading, and Model Comparison\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.6.1 -- Saving and loading model weights\n#\n# Best practice: save only the state_dict (weights), not the entire model object.\n# The model class definition must be available when loading.\n\nimport os\n\nsave_path = '/tmp/salary_mlp_best.pt'\ntorch.save(best_weights, save_path)\nprint(f'Saved weights to {save_path}  ({os.path.getsize(save_path)/1024:.1f} KB)')\n\n# Load into a fresh model instance\nloaded_net = SalaryMLP(input_dim=input_dim, hidden_dims=(128, 64, 32), dropout=0.3)\nloaded_net.load_state_dict(torch.load(save_path, map_location='cpu'))\nloaded_net.eval()\nprint('Weights loaded successfully into fresh model instance.')\n\n# Verify predictions match\nx_verify = torch.tensor(X_test_sc[:5], dtype=torch.float32)\nwith torch.no_grad():\n    salary_net.eval()\n    original_preds = salary_net(x_verify).numpy()\n    loaded_preds   = loaded_net(x_verify).numpy()\n\nprint(f'Original predictions: {np.exp(original_preds).round(0)}')\nprint(f'Loaded predictions:   {np.exp(loaded_preds).round(0)}')\nprint(f'Match: {np.allclose(original_preds, loaded_preds)}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.6.2 -- Chapter summary: MLP vs Random Forest on both tasks\n\nprint('=' * 60)\nprint('  Chapter 7 Results: Neural Network vs Baseline')\nprint('=' * 60)\n\nprint()\nprint('Task 1: Salary Regression (test set)')\nprint(f'  SalaryMLP (PyTorch):          R^2 = {r2:.4f},  MAE = ${mae:,.0f}')\nprint('  Random Forest (Chapter 6):    see CH06 notebook for comparison')\n\nprint()\nprint('Task 2: Python Usage Classification (test set)')\nprint(f'  ClassifierMLP (PyTorch):      accuracy = {acc:.4f}')\nprint('  Best sklearn model (Ch 6):    see CH06 notebook for comparison')\n\nprint()\nprint('Key observations:')\nprint('  - For tabular data with few features, tree-based models often')\nprint('    match or beat MLPs without extensive tuning')\nprint('  - Neural networks shine on high-dimensional data: images, text,')\nprint('    audio -- not on 5-column tabular datasets')\nprint('  - Chapter 8 (NLP) shows where neural networks dominate')\nprint()\nprint('Neural network concepts mastered in this chapter:')\nfor concept in [\n    'Tensors and autograd',\n    'nn.Module and layer construction',\n    'BatchNorm, Dropout, ReLU',\n    'Dataset and DataLoader',\n    'The training loop (forward, loss, backward, step)',\n    'ReduceLROnPlateau scheduler',\n    'Best-weight checkpointing',\n    'torch.save / torch.load',\n]:\n    print(f'  - {concept}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Chapter 7 Summary\n\n### Key Takeaways\n\n- A **tensor** is a GPU-aware, autograd-enabled array. It is the fundamental data\n  structure of deep learning. `requires_grad=True` enables gradient tracking.\n- **`model.train()`** activates Dropout and BatchNorm training behaviour.\n  **`model.eval()`** switches them to inference mode. Forgetting this causes\n  evaluation results to vary randomly -- a common subtle bug.\n- The **training loop** has five steps every iteration: zero_grad, forward,\n  loss, backward, step. Missing any step produces wrong or stale results.\n- **`torch.no_grad()`** disables the autograd graph during inference.\n  Always wrap validation and test passes in it.\n- **BatchNorm** normalises activations within a batch -- accelerates training\n  and reduces sensitivity to learning rate choice.\n- **Dropout** randomly zeroes activations during training -- a powerful regulariser\n  that reduces overfitting. Disabled automatically during `model.eval()`.\n- **`BCEWithLogitsLoss`** is numerically more stable than `Sigmoid + BCELoss`.\n  Use it for all binary classification -- apply sigmoid manually only at inference.\n- **Save state_dict, not the model object.** `torch.save(model.state_dict(), path)`\n  is portable; saving the full model object is fragile across code changes.\n\n### Project Thread Status\n\n| Task | Architecture | Result |\n|------|-------------|--------|\n| Salary regression | MLP 128-64-32 + BatchNorm + Dropout | R^2 reported |\n| Python classification | MLP 64-32 + BatchNorm + Dropout | Accuracy reported |\n| Model persistence | torch.save / torch.load | Round-trip verified |\n\n---\n\n### What's Next: Chapter 8 -- NLP and Transformers\n\nChapter 8 applies neural networks to text: tokenisation, embeddings,\nsentiment analysis with a pre-trained transformer, and fine-tuning\na BERT-family model on SO 2025 developer comments. This is where the\narchitecture from Chapter 7 scales to its full expressive power.\n\n---\n\n*End of Chapter 7 -- Python for AI/ML*  \n[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n"
    }
  ]
}