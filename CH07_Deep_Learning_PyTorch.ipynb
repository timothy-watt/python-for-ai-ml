{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Chapter 7 -- Deep Learning with PyTorch\n## *Python for AI/ML: A Complete Learning Journey*\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/CH07_Deep_Learning_PyTorch.ipynb)\n&nbsp;&nbsp;[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n\n---\n\n**Part:** 3 -- Machine Learning and AI  \n**Prerequisites:** Chapter 6 (Machine Learning with scikit-learn)  \n**Estimated time:** 6-7 hours\n\n---\n\n### Learning Objectives\n\nBy the end of this chapter you will be able to:\n\n- Explain what a tensor is and why PyTorch uses them instead of NumPy arrays\n- Create and manipulate tensors: indexing, reshaping, device transfer\n- Build a neural network with `nn.Module`, `nn.Linear`, and activation functions\n- Write a complete training loop: forward pass, loss, backward pass, optimiser step\n- Use `DataLoader` and `Dataset` for efficient batched training\n- Apply regularisation: dropout, weight decay, batch normalisation\n- Diagnose overfitting with training vs validation loss curves\n- Save and load model weights with `torch.save` and `torch.load`\n\n---\n\n### Project Thread -- Chapter 7\n\nTwo neural networks trained on SO 2025 data:\n\n1. **Salary regression MLP** -- a multi-layer perceptron that predicts log salary\n   from developer profile features, compared against the Chapter 6 Random Forest baseline\n2. **Python usage classifier MLP** -- a binary classifier that predicts Python adoption,\n   compared against the Chapter 6 Gradient Boosting baseline\n\nBoth networks are built from scratch -- no high-level Keras-style wrappers.\nEvery component is explicit so you understand what the framework does for you.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Setup -- Imports, Device, and Data\n\n\n> **Before running this notebook:** go to **Runtime → Change runtime type → T4 GPU**.\n> The two training loops in Sections 7.4 and 7.5 work on CPU but run 3-5x faster on GPU.\n> If T4 is unavailable, CPU will still complete in a few minutes."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score, accuracy_score, classification_report\n\nprint(f'PyTorch: {torch.__version__}')\nprint(f'CUDA available: {torch.cuda.is_available()}')\n\n# Use GPU if available, otherwise CPU\n# In Colab: Runtime -> Change runtime type -> T4 GPU for faster training\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {DEVICE}')\n\nRANDOM_STATE = 42\ntorch.manual_seed(RANDOM_STATE)\nnp.random.seed(RANDOM_STATE)\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.dpi']       = 110\nplt.rcParams['axes.titlesize']   = 13\nplt.rcParams['axes.titleweight'] = 'bold'\n\nDATASET_URL = 'https://raw.githubusercontent.com/timothy-watt/python-for-ai-ml/main/data/so_survey_2025_curated.csv'\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Load and clean SO 2025 -- identical pipeline to Chapter 6\ndf_raw = pd.read_csv(DATASET_URL)\ndf = df_raw.copy()\ndf = df.dropna(subset=['ConvertedCompYearly'])\ndf['ConvertedCompYearly'] = pd.to_numeric(df['ConvertedCompYearly'], errors='coerce')\nQ1, Q3 = df['ConvertedCompYearly'].quantile([0.25, 0.75])\nIQR = Q3 - Q1\ndf = df[\n    (df['ConvertedCompYearly'] >= max(Q1 - 3*IQR, 5_000)) &\n    (df['ConvertedCompYearly'] <= min(Q3 + 3*IQR, 600_000))\n].copy()\nif 'YearsCodePro' in df.columns:\n    df['YearsCodePro'] = pd.to_numeric(df['YearsCodePro'], errors='coerce')\n    df['YearsCodePro'] = df['YearsCodePro'].fillna(df['YearsCodePro'].median())\nfor col in ['Country', 'EdLevel', 'Employment', 'RemoteWork']:\n    if col in df.columns:\n        df[col] = df[col].fillna('Unknown')\ndf['uses_python'] = df.get('LanguageHaveWorkedWith', pd.Series(dtype=str)).str.contains('Python', na=False).astype(int)\ndf['uses_sql']    = df.get('LanguageHaveWorkedWith', pd.Series(dtype=str)).str.contains('SQL', na=False).astype(int)\ndf['uses_js']     = df.get('LanguageHaveWorkedWith', pd.Series(dtype=str)).str.contains('JavaScript', na=False).astype(int)\ndf['uses_ai']     = df.get('AIToolCurrently', pd.Series(dtype=str)).notna().astype(int)\ndf['log_salary']  = np.log(df['ConvertedCompYearly'])\ndf = df.reset_index(drop=True)\nprint(f'Dataset ready: {len(df):,} rows')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 7.1 -- Tensors: PyTorch's Core Data Structure\n\nA **tensor** is an n-dimensional array -- identical in structure to a NumPy array,\nbut with two crucial additions:\n\n1. **Device awareness:** a tensor can live on CPU or GPU. Moving it to GPU (`tensor.to('cuda')`)\n   makes all operations on it run on the GPU automatically -- no code changes needed.\n2. **Autograd:** PyTorch tracks every operation on tensors with `requires_grad=True`,\n   building a computation graph that enables automatic differentiation.\n   This is what makes backpropagation possible without manual gradient calculation.\n\nEverything else in PyTorch -- layers, optimisers, losses -- operates on tensors.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.1.1 -- Creating tensors\n\n# From a Python list\nt1 = torch.tensor([1.0, 2.0, 3.0, 4.0])\nprint(f't1: {t1}  dtype={t1.dtype}  shape={t1.shape}')\n\n# From a NumPy array -- shares memory when on CPU (no copy)\narr = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\nt2  = torch.from_numpy(arr)\nprint(f't2 shape: {t2.shape}  dtype={t2.dtype}')\n\n# Factory functions\nzeros = torch.zeros(3, 4)          # 3x4 matrix of 0.0\nones  = torch.ones(2, 5)           # 2x5 matrix of 1.0\nrand  = torch.rand(3, 3)           # uniform [0, 1)\nrandn = torch.randn(3, 3)          # standard normal\neye   = torch.eye(4)               # 4x4 identity matrix\nprint(f'zeros shape: {zeros.shape}')\nprint(f'randn:\\n{randn.numpy().round(3)}')\n\n# Specifying dtype explicitly\nt_float32 = torch.tensor([1, 2, 3], dtype=torch.float32)  # default for neural nets\nt_int64   = torch.tensor([1, 2, 3], dtype=torch.int64)    # required for class labels\nprint(f'float32: {t_float32.dtype}  int64: {t_int64.dtype}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.1.2 -- Tensor operations: indexing, reshaping, device transfer\n\nt = torch.randn(4, 3)\nprint(f'Original shape: {t.shape}')\nprint(f'First row:      {t[0]}')\nprint(f'Col 1:          {t[:, 1]}')\nprint(f'Element [2,1]:  {t[2, 1].item():.4f}')  # .item() extracts Python scalar\n\n# Reshaping\nflat   = t.reshape(-1)           # flatten to 1D (-1 = infer size)\nt_2x6  = t.reshape(2, 6)        # reshape to 2x6\nt_T    = t.T                     # transpose\nprint(f'Flattened: {flat.shape}  Reshaped: {t_2x6.shape}  Transposed: {t_T.shape}')\n\n# Arithmetic -- all operations are element-wise unless you use @\na = torch.tensor([1.0, 2.0, 3.0])\nb = torch.tensor([4.0, 5.0, 6.0])\nprint(f'a + b:     {a + b}')\nprint(f'a * b:     {a * b}')    # element-wise, NOT dot product\nprint(f'a @ b:     {a @ b}')    # dot product (inner product)\nprint(f'a.dot(b):  {torch.dot(a, b)}')\n\n# Matrix multiplication\nW = torch.randn(3, 5)   # weight matrix\nx = torch.randn(5)      # input vector\ny = W @ x               # matrix-vector product: shape (3,)\nprint(f'W @ x shape: {y.shape}')\n\n# Device transfer -- move to GPU if available\nt_device = t.to(DEVICE)\nprint(f'Tensor device: {t_device.device}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.1.3 -- Autograd: automatic differentiation\n#\n# When requires_grad=True, PyTorch records every operation on the tensor.\n# Calling .backward() on the final scalar loss computes gradients\n# for all tensors in the computation graph that have requires_grad=True.\n# This is the entire mathematical basis of neural network training.\n\n# Simple example: compute d/dx of f(x) = 3x^2 + 2x + 1 at x=2\nx = torch.tensor(2.0, requires_grad=True)  # leaf variable\nf = 3 * x**2 + 2 * x + 1                  # builds computation graph\n\nf.backward()   # computes df/dx\n\nprint(f'f(2)   = {f.item():.1f}  (expected: 3*4 + 2*2 + 1 = 17)')\nprint(f'df/dx  = {x.grad.item():.1f}  (expected: 6x + 2 at x=2 = 14)')\n\n# In a neural network, x becomes the model weights, f becomes the loss.\n# .backward() fills .grad on every weight tensor, then the optimiser\n# uses those gradients to update the weights.\n\n# torch.no_grad() disables gradient tracking -- use during inference\n# to save memory and speed up forward passes when we do not need gradients\nwith torch.no_grad():\n    y = 3 * x**2 + 2 * x + 1\n    print(f'Under no_grad: y={y.item():.1f}, grad_fn={y.grad_fn}')  # None\n\n# Converting between tensor and numpy\nt = torch.tensor([1.0, 2.0, 3.0])\narr = t.detach().numpy()    # .detach() removes from computation graph first\nprint(f'Tensor -> numpy: {arr}  type={type(arr).__name__}')\nback = torch.from_numpy(arr)\nprint(f'numpy -> tensor: {back}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 7.2 -- Building Neural Networks with nn.Module\n\n`nn.Module` is the base class for every neural network in PyTorch.\nYou subclass it, define layers in `__init__`, and implement the forward pass in `forward()`.\nPyTorch then handles parameter registration, gradient tracking, device transfer,\nserialisation, and training/eval mode switching automatically.\n\nThis is the same OOP pattern from Chapter 2 -- a class with `__init__` storing\nconfiguration and learned state, and methods implementing behaviour.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.2.1 -- A minimal MLP (Multi-Layer Perceptron)\n\nclass SalaryMLP(nn.Module):\n    \"\"\"\n    Multi-layer perceptron for salary regression.\n\n    Architecture:\n        Input -> Linear -> BatchNorm -> ReLU -> Dropout\n               -> Linear -> BatchNorm -> ReLU -> Dropout\n               -> Linear -> output\n\n    Parameters\n    ----------\n    input_dim  : int   -- number of input features\n    hidden_dims: list  -- number of neurons in each hidden layer\n    dropout    : float -- dropout probability (0 = disabled)\n    \"\"\"\n\n    def __init__(self, input_dim, hidden_dims=(128, 64, 32), dropout=0.3):\n        super().__init__()   # must call parent __init__\n\n        layers = []\n        prev_dim = input_dim\n\n        for hidden_dim in hidden_dims:\n            layers += [\n                nn.Linear(prev_dim, hidden_dim),  # learnable weight matrix + bias\n                nn.BatchNorm1d(hidden_dim),        # normalise activations per batch\n                nn.ReLU(),                         # non-linearity: max(0, x)\n                nn.Dropout(p=dropout),             # randomly zero p% of activations\n            ]\n            prev_dim = hidden_dim\n\n        layers.append(nn.Linear(prev_dim, 1))   # output layer: single salary prediction\n\n        # nn.Sequential chains layers -- forward() calls them in order\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass: compute predictions from input tensor x.\n        Called automatically when you do: output = model(x)\n        \"\"\"\n        return self.network(x).squeeze(-1)   # squeeze removes the trailing dim-1\n\n\n# Instantiate the model and move to the target device\ninput_dim   = 5   # placeholder -- will be set properly in section 7.3\nsalary_net  = SalaryMLP(input_dim=input_dim, hidden_dims=(128, 64, 32), dropout=0.3)\nsalary_net  = salary_net.to(DEVICE)\n\nprint(salary_net)\nprint()\n\n# Count trainable parameters\nn_params = sum(p.numel() for p in salary_net.parameters() if p.requires_grad)\nprint(f'Trainable parameters: {n_params:,}')\n\n# Test with a random batch\nx_test  = torch.randn(8, input_dim).to(DEVICE)   # batch of 8 samples\ny_test  = salary_net(x_test)\nprint(f'Input shape:  {x_test.shape}')\nprint(f'Output shape: {y_test.shape}')   # (8,) -- one prediction per sample\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.2.2 -- Loss functions and optimisers\n#\n# A loss function measures how wrong the model's predictions are.\n# The optimiser adjusts model weights to reduce the loss.\n\n# Common loss functions:\n#   nn.MSELoss()    -- Mean Squared Error: regression tasks\n#   nn.MAELoss()    -- Mean Absolute Error: regression, robust to outliers\n#   nn.BCEWithLogitsLoss() -- Binary Cross-Entropy: binary classification\n#   nn.CrossEntropyLoss()  -- Multi-class classification\n\ncriterion = nn.MSELoss()   # for log-salary regression\n\n# Common optimisers:\n#   optim.SGD        -- Stochastic Gradient Descent (baseline)\n#   optim.Adam       -- Adaptive Moment Estimation (default choice)\n#   optim.AdamW      -- Adam with decoupled weight decay (better regularisation)\n\noptimizer = optim.AdamW(\n    salary_net.parameters(),\n    lr=1e-3,           # learning rate: step size for weight updates\n    weight_decay=1e-4  # L2 regularisation: penalises large weights\n)\n\n# Demonstrate one manual forward + backward pass\nx_demo  = torch.randn(16, input_dim).to(DEVICE)\ny_demo  = torch.randn(16).to(DEVICE)          # fake targets\n\noptimizer.zero_grad()          # clear gradients from the previous step\ny_pred  = salary_net(x_demo)   # forward pass\nloss    = criterion(y_pred, y_demo)  # compute loss\nloss.backward()                # backward pass: compute all gradients\noptimizer.step()               # update weights using gradients\n\nprint(f'One manual training step completed.')\nprint(f'Loss: {loss.item():.6f}')\nprint(f'Output grad_fn: {y_pred.grad_fn.__class__.__name__}')  # SqueezeBackward\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 7.3 -- Dataset and DataLoader\n\nPyTorch's `Dataset` and `DataLoader` handle the mechanics of batched training:\nshuffling, batching, and multi-process data loading. They are the equivalent\nof scikit-learn's `Pipeline` for the data side of training.\n\nYou subclass `Dataset` and implement three methods:\n- `__len__`: how many samples in the dataset\n- `__getitem__`: return one (features, label) pair by index\n\n`DataLoader` wraps the dataset and yields batches during the training loop.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.3.1 -- Custom Dataset for SO 2025\n\nclass SurveyDataset(Dataset):\n    \"\"\"\n    PyTorch Dataset wrapping the SO 2025 feature matrix and targets.\n\n    Parameters\n    ----------\n    X : np.ndarray -- feature matrix (already scaled)\n    y : np.ndarray -- target vector\n    \"\"\"\n\n    def __init__(self, X, y):\n        # Convert numpy arrays to float32 tensors\n        # float32 is the standard dtype for neural network weights and activations\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        # Called by DataLoader to fetch one sample\n        # Returns a (features, label) tuple\n        return self.X[idx], self.y[idx]\n\n\n# Build the feature matrix for regression\nfeature_cols = [c for c in ['YearsCodePro', 'uses_python', 'uses_sql', 'uses_js', 'uses_ai']\n                if c in df.columns]\nprint(f'Features: {feature_cols}')\n\nX_raw = df[feature_cols].copy()\nfor col in feature_cols:\n    med = X_raw[col].median()\n    X_raw[col] = X_raw[col].fillna(med if pd.notna(med) else 0)\ny_raw = df['log_salary'].values\n\n# Train/test split\nX_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n    X_raw.values, y_raw, test_size=0.2, random_state=RANDOM_STATE\n)\n\n# Scale features -- fit ONLY on training data\nfeat_scaler = StandardScaler()\nX_train_sc  = feat_scaler.fit_transform(X_train_np)\nX_test_sc   = feat_scaler.transform(X_test_np)\n\n# Create Dataset objects\ntrain_dataset = SurveyDataset(X_train_sc, y_train_np)\ntest_dataset  = SurveyDataset(X_test_sc,  y_test_np)\n\n# DataLoader: batches + shuffling\n# batch_size=256: process 256 samples per weight update\n# shuffle=True: randomise order each epoch (prevents the model memorising order)\ntrain_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\ntest_loader  = DataLoader(test_dataset,  batch_size=512, shuffle=False)\n\ninput_dim = X_train_sc.shape[1]\nprint(f'Train: {len(train_dataset):,} samples  ({len(train_loader)} batches of 256)')\nprint(f'Test:  {len(test_dataset):,} samples')\nprint(f'Input dim: {input_dim}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 7.4 -- The Training Loop\n\nThe training loop is the heart of deep learning. Every epoch it:\n\n1. Iterates over batches from the DataLoader\n2. Runs the **forward pass**: compute predictions\n3. Computes the **loss**: how wrong are the predictions?\n4. Runs the **backward pass**: compute gradients via backprop\n5. **Optimiser step**: update weights in the direction that reduces loss\n6. Runs a **validation pass** with `torch.no_grad()` to monitor overfitting\n\nThe train/validation loss curves are the most important diagnostic in deep learning.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.4.1 -- Reusable train_epoch and evaluate functions\n\ndef train_epoch(model, loader, criterion, optimizer):\n    \"\"\"\n    Run one full pass over the training data.\n    Returns mean training loss for this epoch.\n    \"\"\"\n    model.train()   # sets BatchNorm and Dropout to training mode\n    total_loss = 0.0\n\n    for X_batch, y_batch in loader:\n        X_batch = X_batch.to(DEVICE)\n        y_batch = y_batch.to(DEVICE)\n\n        optimizer.zero_grad()              # 1. clear old gradients\n        y_pred = model(X_batch)            # 2. forward pass\n        loss   = criterion(y_pred, y_batch)  # 3. compute loss\n        loss.backward()                    # 4. backward pass\n        optimizer.step()                   # 5. update weights\n\n        total_loss += loss.item() * len(X_batch)  # accumulate weighted loss\n\n    return total_loss / len(loader.dataset)\n\n\ndef evaluate(model, loader, criterion):\n    \"\"\"\n    Evaluate model on a DataLoader without updating weights.\n    Returns mean loss and all predictions as numpy arrays.\n    \"\"\"\n    model.eval()    # sets BatchNorm and Dropout to inference mode\n    total_loss = 0.0\n    all_preds  = []\n    all_targets = []\n\n    with torch.no_grad():   # disable gradient tracking for efficiency\n        for X_batch, y_batch in loader:\n            X_batch = X_batch.to(DEVICE)\n            y_batch = y_batch.to(DEVICE)\n            y_pred  = model(X_batch)\n            loss    = criterion(y_pred, y_batch)\n            total_loss += loss.item() * len(X_batch)\n            all_preds.append(y_pred.cpu().numpy())\n            all_targets.append(y_batch.cpu().numpy())\n\n    preds   = np.concatenate(all_preds)\n    targets = np.concatenate(all_targets)\n    return total_loss / len(loader.dataset), preds, targets\n\n\nprint('train_epoch() and evaluate() functions defined.')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.4.2 -- Train the salary regression MLP\n\n# Build fresh model\nsalary_net = SalaryMLP(\n    input_dim   = input_dim,\n    hidden_dims = (128, 64, 32),\n    dropout     = 0.3\n).to(DEVICE)\n\ncriterion  = nn.MSELoss()\noptimizer  = optim.AdamW(salary_net.parameters(), lr=1e-3, weight_decay=1e-4)\n\n# ReduceLROnPlateau: halve the learning rate if validation loss stops improving\nscheduler  = ReduceLROnPlateau(optimizer, mode='min', factor=0.5,\n                                patience=5, verbose=False)\n\nN_EPOCHS   = 60\ntrain_losses = []\nval_losses   = []\nbest_val     = float('inf')\nbest_weights = None\n\nprint(f'Training SalaryMLP for {N_EPOCHS} epochs on {DEVICE}...')\nprint(f'{\"Epoch\":>6}  {\"Train Loss\":>12}  {\"Val Loss\":>12}  {\"LR\":>10}')\nprint('-' * 44)\n\nfor epoch in range(1, N_EPOCHS + 1):\n    tr_loss          = train_epoch(salary_net, train_loader, criterion, optimizer)\n    val_loss, _, _   = evaluate(salary_net, test_loader, criterion)\n    scheduler.step(val_loss)\n\n    train_losses.append(tr_loss)\n    val_losses.append(val_loss)\n\n    # Save best weights (early stopping logic)\n    if val_loss < best_val:\n        best_val     = val_loss\n        best_weights = {k: v.clone() for k, v in salary_net.state_dict().items()}\n\n    if epoch % 10 == 0 or epoch == 1:\n        lr = optimizer.param_groups[0]['lr']\n        print(f'{epoch:>6}  {tr_loss:>12.6f}  {val_loss:>12.6f}  {lr:>10.2e}')\n\nprint(f'Best validation loss: {best_val:.6f}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.4.3 -- Evaluate and plot training curves\n\n# Restore best weights\nsalary_net.load_state_dict(best_weights)\n\n# Final evaluation\n_, y_pred_log, y_true_log = evaluate(salary_net, test_loader, criterion)\ny_pred_usd = np.exp(y_pred_log)\ny_true_usd = np.exp(y_true_log)\n\nr2  = r2_score(y_true_log, y_pred_log)\nmae = np.mean(np.abs(y_true_usd - y_pred_usd))\n\nprint(f'SalaryMLP Test Results:')\nprint(f'  R^2 (log scale): {r2:.4f}')\nprint(f'  MAE (USD):       ${mae:,.0f}')\n\n# Training curves\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nepochs = range(1, len(train_losses) + 1)\naxes[0].plot(epochs, train_losses, '#E8722A', linewidth=2, label='Train loss')\naxes[0].plot(epochs, val_losses,   '#2E75B6', linewidth=2, label='Val loss')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('MSE Loss (log salary)')\naxes[0].set_title('Training vs Validation Loss\\n(curves converging = good fit)')\naxes[0].legend()\n\naxes[1].scatter(y_true_usd/1000, y_pred_usd/1000, alpha=0.2, s=8, color='#2E75B6')\nlim = max(y_true_usd.max(), y_pred_usd.max()) / 1000\naxes[1].plot([0, lim], [0, lim], 'r--', linewidth=2, label='Perfect')\naxes[1].set_xlabel('Actual Salary ($k)')\naxes[1].set_ylabel('Predicted Salary ($k)')\naxes[1].set_title(f'Actual vs Predicted Salary\\nR^2={r2:.3f}, MAE=${mae/1000:.1f}k')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 7.5 -- Binary Classification MLP\n\nThe classifier network is identical to the regression network in architecture,\nwith two changes:\n\n- **Output layer:** 1 neuron with no activation (raw logit)\n- **Loss function:** `BCEWithLogitsLoss` -- combines sigmoid activation and\n  binary cross-entropy in a numerically stable single operation\n\nDuring inference, apply `torch.sigmoid()` to the raw logit to get a probability.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.5.1 -- Build classifier dataset and model\n\nclf_feature_cols = [c for c in ['YearsCodePro', 'ConvertedCompYearly',\n                                 'uses_sql', 'uses_js', 'uses_ai']\n                    if c in df.columns]\n\nX_clf_raw = df[clf_feature_cols].copy()\nfor col in clf_feature_cols:\n    med = X_clf_raw[col].median()\n    X_clf_raw[col] = X_clf_raw[col].fillna(med if pd.notna(med) else 0)\ny_clf_raw = df['uses_python'].values.astype(np.float32)\n\nX_tr_c, X_te_c, y_tr_c, y_te_c = train_test_split(\n    X_clf_raw.values, y_clf_raw, test_size=0.2,\n    random_state=RANDOM_STATE, stratify=y_clf_raw\n)\n\nclf_scaler  = StandardScaler()\nX_tr_c_sc   = clf_scaler.fit_transform(X_tr_c)\nX_te_c_sc   = clf_scaler.transform(X_te_c)\n\nclf_train_ds = SurveyDataset(X_tr_c_sc, y_tr_c)\nclf_test_ds  = SurveyDataset(X_te_c_sc, y_te_c)\nclf_train_loader = DataLoader(clf_train_ds, batch_size=256, shuffle=True)\nclf_test_loader  = DataLoader(clf_test_ds,  batch_size=512, shuffle=False)\n\nclf_input_dim = X_tr_c_sc.shape[1]\nprint(f'Classifier features: {clf_feature_cols}')\nprint(f'Class balance: Python={y_clf_raw.mean()*100:.1f}%')\n\n# The network architecture is identical -- only the loss changes\nclass_net = SalaryMLP(\n    input_dim   = clf_input_dim,\n    hidden_dims = (64, 32),\n    dropout     = 0.2\n).to(DEVICE)\n\nclf_criterion = nn.BCEWithLogitsLoss()   # sigmoid + BCE in one stable op\nclf_optimizer = optim.AdamW(class_net.parameters(), lr=1e-3, weight_decay=1e-4)\nclf_scheduler = ReduceLROnPlateau(clf_optimizer, mode='min', factor=0.5, patience=5)\n\nn_params = sum(p.numel() for p in class_net.parameters() if p.requires_grad)\nprint(f'Classifier parameters: {n_params:,}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.5.2 -- Train the classifier\n\nN_EPOCHS_CLF    = 50\nclf_train_losses = []\nclf_val_losses   = []\nclf_best_val     = float('inf')\nclf_best_weights = None\n\nprint(f'Training classifier for {N_EPOCHS_CLF} epochs on {DEVICE}...')\nprint(f'{\"Epoch\":>6}  {\"Train Loss\":>12}  {\"Val Loss\":>12}')\nprint('-' * 34)\n\nfor epoch in range(1, N_EPOCHS_CLF + 1):\n    tr_loss        = train_epoch(class_net, clf_train_loader, clf_criterion, clf_optimizer)\n    val_loss, _, _ = evaluate(class_net, clf_test_loader, clf_criterion)\n    clf_scheduler.step(val_loss)\n\n    clf_train_losses.append(tr_loss)\n    clf_val_losses.append(val_loss)\n\n    if val_loss < clf_best_val:\n        clf_best_val     = val_loss\n        clf_best_weights = {k: v.clone() for k, v in class_net.state_dict().items()}\n\n    if epoch % 10 == 0 or epoch == 1:\n        print(f'{epoch:>6}  {tr_loss:>12.6f}  {val_loss:>12.6f}')\n\nprint(f'Best val loss: {clf_best_val:.6f}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.5.3 -- Evaluate the classifier\n\nclass_net.load_state_dict(clf_best_weights)\n\n_, logits, y_true = evaluate(class_net, clf_test_loader, clf_criterion)\n\n# Convert raw logits to probabilities and binary predictions\nprobs  = 1 / (1 + np.exp(-logits))   # sigmoid manually for clarity\ny_pred = (probs >= 0.5).astype(int)\n\nacc = accuracy_score(y_true, y_pred)\nprint(f'Classifier Test Accuracy: {acc:.4f}  ({acc*100:.1f}%)')\nprint()\nprint(classification_report(y_true, y_pred, target_names=['Non-Python', 'Python']))\n\n# Loss curves\nfig, ax = plt.subplots(figsize=(9, 5))\nepochs_clf = range(1, len(clf_train_losses) + 1)\nax.plot(epochs_clf, clf_train_losses, '#E8722A', linewidth=2, label='Train loss')\nax.plot(epochs_clf, clf_val_losses,   '#2E75B6', linewidth=2, label='Val loss')\nax.set_xlabel('Epoch')\nax.set_ylabel('BCE Loss')\nax.set_title(f'Classifier Training Curves\\nFinal accuracy: {acc*100:.1f}%')\nax.legend()\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 7.6 -- Saving, Loading, and Model Comparison\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.6.1 -- Saving and loading model weights\n#\n# Best practice: save only the state_dict (weights), not the entire model object.\n# The model class definition must be available when loading.\n\nimport os\n\nsave_path = '/tmp/salary_mlp_best.pt'\ntorch.save(best_weights, save_path)\nprint(f'Saved weights to {save_path}  ({os.path.getsize(save_path)/1024:.1f} KB)')\n\n# Load into a fresh model instance\nloaded_net = SalaryMLP(input_dim=input_dim, hidden_dims=(128, 64, 32), dropout=0.3)\nloaded_net.load_state_dict(torch.load(save_path, map_location='cpu'))\nloaded_net.eval()\nprint('Weights loaded successfully into fresh model instance.')\n\n# Verify predictions match\nx_verify = torch.tensor(X_test_sc[:5], dtype=torch.float32)\nwith torch.no_grad():\n    salary_net.eval()\n    original_preds = salary_net(x_verify).numpy()\n    loaded_preds   = loaded_net(x_verify).numpy()\n\nprint(f'Original predictions: {np.exp(original_preds).round(0)}')\nprint(f'Loaded predictions:   {np.exp(loaded_preds).round(0)}')\nprint(f'Match: {np.allclose(original_preds, loaded_preds)}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.6.2 -- Chapter summary: MLP vs Random Forest on both tasks\n\nprint('=' * 60)\nprint('  Chapter 7 Results: Neural Network vs Baseline')\nprint('=' * 60)\n\nprint()\nprint('Task 1: Salary Regression (test set)')\nprint(f'  SalaryMLP (PyTorch):          R^2 = {r2:.4f},  MAE = ${mae:,.0f}')\nprint('  Random Forest (Chapter 6):    see CH06 notebook for comparison')\n\nprint()\nprint('Task 2: Python Usage Classification (test set)')\nprint(f'  ClassifierMLP (PyTorch):      accuracy = {acc:.4f}')\nprint('  Best sklearn model (Ch 6):    see CH06 notebook for comparison')\n\nprint()\nprint('Key observations:')\nprint('  - For tabular data with few features, tree-based models often')\nprint('    match or beat MLPs without extensive tuning')\nprint('  - Neural networks shine on high-dimensional data: images, text,')\nprint('    audio -- not on 5-column tabular datasets')\nprint('  - Chapter 8 (NLP) shows where neural networks dominate')\nprint()\nprint('Neural network concepts mastered in this chapter:')\nfor concept in [\n    'Tensors and autograd',\n    'nn.Module and layer construction',\n    'BatchNorm, Dropout, ReLU',\n    'Dataset and DataLoader',\n    'The training loop (forward, loss, backward, step)',\n    'ReduceLROnPlateau scheduler',\n    'Best-weight checkpointing',\n    'torch.save / torch.load',\n]:\n    print(f'  - {concept}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 7.7 -- Finding the Right Learning Rate\n\nThe learning rate is the single most important hyperparameter in neural network\ntraining. Too high and training diverges; too low and it converges glacially.\n\nThe **LR Range Test** (Leslie Smith, 2015) finds a good learning rate in one\nshort run: start very low, increase exponentially each batch, and plot the loss.\nThe optimal learning rate sits just before the loss starts rising sharply --\ntypically one order of magnitude below the minimum loss point.\n\nThis eliminates the need for expensive grid searches over learning rates.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.7.1 -- LR Range Test implementation\n\nimport math\n\ndef lr_range_test(model_fn, train_loader, criterion,\n                  start_lr=1e-7, end_lr=10.0, num_iter=100):\n    \"\"\"\n    Run the LR range test.\n    model_fn: callable that returns a freshly initialised model\n    Returns: (lrs, losses) lists for plotting\n    \"\"\"\n    model     = model_fn().to(DEVICE)\n    optimiser = torch.optim.SGD(model.parameters(), lr=start_lr)\n    mult      = (end_lr / start_lr) ** (1 / num_iter)\n\n    lrs, losses = [], []\n    best_loss   = float('inf')\n    avg_loss    = 0.0\n    beta        = 0.98   # smoothing factor\n\n    data_iter = iter(train_loader)\n\n    for i in range(num_iter):\n        try:\n            X_batch, y_batch = next(data_iter)\n        except StopIteration:\n            data_iter = iter(train_loader)\n            X_batch, y_batch = next(data_iter)\n\n        X_batch = X_batch.to(DEVICE)\n        y_batch = y_batch.to(DEVICE)\n\n        model.train()\n        optimiser.zero_grad()\n        out  = model(X_batch)\n        loss = criterion(out.squeeze(), y_batch.float())\n        loss.backward()\n        optimiser.step()\n\n        # Exponentially weighted moving average of loss\n        avg_loss = beta * avg_loss + (1 - beta) * loss.item()\n        smooth   = avg_loss / (1 - beta ** (i + 1))\n\n        current_lr = optimiser.param_groups[0]['lr']\n        lrs.append(current_lr)\n        losses.append(smooth)\n\n        if smooth < best_loss:\n            best_loss = smooth\n        # Stop if loss explodes\n        if smooth > 4 * best_loss:\n            break\n\n        # Increase LR for next step\n        for pg in optimiser.param_groups:\n            pg['lr'] *= mult\n\n    return lrs, losses\n\n\n# Run the test using the binary classification dataset from section 7.5\n# Recreate the DataLoaders used in section 7.5\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler as SkScaler\nfrom sklearn.model_selection import train_test_split as sk_split\n\nfeat_cols_7 = [c for c in ['YearsCodePro','uses_python','uses_sql','uses_js','uses_ai']\n               if c in df.columns]\nX7 = df[feat_cols_7].copy()\nfor col in feat_cols_7:\n    med = X7[col].median()\n    X7[col] = X7[col].fillna(med if pd.notna(med) else 0)\ny7 = df['uses_python'].values.astype('float32')\n\nX7tr, X7te, y7tr, y7te = sk_split(X7.values, y7, test_size=0.2,\n                                    random_state=RANDOM_STATE, stratify=y7)\nsc7 = SkScaler()\nX7tr_s = sc7.fit_transform(X7tr).astype('float32')\n\nlr_ds     = TensorDataset(torch.tensor(X7tr_s), torch.tensor(y7tr))\nlr_loader = DataLoader(lr_ds, batch_size=64, shuffle=True)\n\ndef fresh_clf():\n    return nn.Sequential(\n        nn.Linear(X7tr_s.shape[1], 64), nn.BatchNorm1d(64), nn.ReLU(), nn.Dropout(0.2),\n        nn.Linear(64, 32),              nn.BatchNorm1d(32), nn.ReLU(), nn.Dropout(0.2),\n        nn.Linear(32, 1)\n    )\n\nlrs, losses = lr_range_test(\n    fresh_clf, lr_loader, nn.BCEWithLogitsLoss(), num_iter=150\n)\n\nfig, ax = plt.subplots(figsize=(10, 4))\nax.plot(lrs, losses, '#2E75B6', linewidth=2)\nax.set_xscale('log')\nax.set_xlabel('Learning Rate (log scale)')\nax.set_ylabel('Smoothed Loss')\nax.set_title('LR Range Test\\nChoose LR just before the minimum -- typically 10x below the lowest point')\n\n# Mark suggested range\nmin_idx     = losses.index(min(losses))\nsuggest_lr  = lrs[max(0, min_idx - 10)]\nax.axvline(suggest_lr, color='red', linestyle='--', linewidth=1.5,\n           label=f'Suggested LR ≈ {suggest_lr:.1e}')\nax.legend()\nplt.tight_layout()\nplt.show()\n\nprint(f'Suggested learning rate from range test: {suggest_lr:.2e}')\nprint('Use this as max_lr in OneCycleLR or as the base lr in AdamW')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 7.8 -- Mixed Precision Training\n\nModern GPUs have dedicated hardware for 16-bit (float16) arithmetic that runs\n2-4x faster than 32-bit (float32). **Mixed precision training** uses float16\nfor the forward pass and gradient computation, but keeps a float32 master copy\nof weights for the update step (where numerical precision matters).\n\nPyTorch's `torch.cuda.amp` (Automatic Mixed Precision) handles this transparently:\n- `autocast()` context manager: automatically casts operations to float16\n- `GradScaler`: scales the loss upward before backprop to prevent float16 underflow,\n  then unscales before the optimiser step\n\n**Result:** ~2x faster training, ~50% less GPU memory. Free speedup with two lines of code.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.8.1 -- Mixed precision training with torch.cuda.amp\n\nimport time\n\n# Build a slightly larger model to make the timing difference visible\nclass LargerMLP(nn.Module):\n    def __init__(self, input_dim, hidden=256):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden),  nn.BatchNorm1d(hidden), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(hidden, hidden),     nn.BatchNorm1d(hidden), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(hidden, hidden//2),  nn.BatchNorm1d(hidden//2), nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(hidden//2, 1)\n        )\n    def forward(self, x): return self.net(x)\n\n\nN_BENCH_EPOCHS = 5\ninput_dim_7 = X7tr_s.shape[1]\n\ndef run_training(use_amp, n_epochs=N_BENCH_EPOCHS):\n    model   = LargerMLP(input_dim_7).to(DEVICE)\n    opt     = torch.optim.AdamW(model.parameters(), lr=1e-3)\n    crit    = nn.BCEWithLogitsLoss()\n    scaler  = torch.cuda.amp.GradScaler(enabled=use_amp)\n    loader  = DataLoader(lr_ds, batch_size=256, shuffle=True)\n\n    t0 = time.time()\n    for _ in range(n_epochs):\n        model.train()\n        for Xb, yb in loader:\n            Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)\n            opt.zero_grad()\n            with torch.cuda.amp.autocast(enabled=use_amp):\n                out  = model(Xb)\n                loss = crit(out.squeeze(), yb)\n            scaler.scale(loss).backward()\n            scaler.step(opt)\n            scaler.update()\n    elapsed = time.time() - t0\n    return elapsed\n\n\n# Warm up GPU\n_ = run_training(use_amp=False, n_epochs=1)\n\nt_fp32 = run_training(use_amp=False)\nt_amp  = run_training(use_amp=True)\n\namp_label = 'AMP (float16/32)' if DEVICE.type == 'cuda' else 'AMP (CPU -- no speedup on CPU)'\nprint(f'{N_BENCH_EPOCHS}-epoch training benchmark ({DEVICE}):')\nprint(f'  float32 only:   {t_fp32:.2f}s')\nprint(f'  {amp_label}: {t_amp:.2f}s')\nif DEVICE.type == 'cuda':\n    speedup = t_fp32 / t_amp\n    print(f'  Speedup:        {speedup:.2f}x')\n    print()\n    print('To add AMP to any training loop:')\n    print('  1. scaler = torch.cuda.amp.GradScaler()')\n    print('  2. Wrap forward pass: with torch.cuda.amp.autocast(): ...')\n    print('  3. Replace loss.backward() with scaler.scale(loss).backward()')\n    print('  4. Replace opt.step() with scaler.step(opt); scaler.update()')\nelse:\n    print('  Note: AMP speedup only applies on CUDA GPUs.')\n    print('  On CPU the results are identical -- enable T4 GPU to see the speedup.')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 7.9 -- `torch.compile`: PyTorch 2.0 Speedup\n\nPyTorch 2.0 introduced `torch.compile()` -- a one-line function that compiles\na model using TorchDynamo and TorchInductor, generating optimised kernel code\nfor your specific GPU.\n\nIt requires no changes to your model architecture or training loop.\nThe first forward pass is slow (compilation overhead), but all subsequent\npasses run significantly faster. On modern GPUs, expect 20-50% speedup\nfor typical MLP and CNN architectures.\n\n```python\n# Before -- standard model\nmodel = MyModel().to(DEVICE)\n\n# After -- compiled model (same API, faster execution)\nmodel = torch.compile(MyModel().to(DEVICE))\n```\n\n**When to use it:** large models, long training runs, production inference.\nThe compilation overhead (~30s on first run) is amortised over many iterations.\nNot worth it for tiny models or quick experiments.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 7.9.1 -- torch.compile benchmark\n\nimport torch\n\npytorch_version = tuple(int(x) for x in torch.__version__.split('.')[:2])\ncompile_available = pytorch_version >= (2, 0) and DEVICE.type == 'cuda'\n\nif not compile_available:\n    reason = 'CPU runtime' if DEVICE.type != 'cuda' else f'PyTorch {torch.__version__} < 2.0'\n    print(f'torch.compile benchmark skipped: {reason}')\n    print('torch.compile requires PyTorch >= 2.0 and a CUDA GPU.')\n    print('Enable T4 GPU in Colab to run this benchmark.')\nelse:\n    model_eager    = LargerMLP(input_dim_7).to(DEVICE)\n    model_compiled = torch.compile(LargerMLP(input_dim_7).to(DEVICE))\n\n    crit    = nn.BCEWithLogitsLoss()\n    loader  = DataLoader(lr_ds, batch_size=256, shuffle=True)\n\n    def bench(model, n_epochs=5):\n        opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n        scaler = torch.cuda.amp.GradScaler()\n        t0 = time.time()\n        for _ in range(n_epochs):\n            model.train()\n            for Xb, yb in loader:\n                Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)\n                opt.zero_grad()\n                with torch.cuda.amp.autocast():\n                    loss = crit(model(Xb).squeeze(), yb)\n                scaler.scale(loss).backward()\n                scaler.step(opt)\n                scaler.update()\n        return time.time() - t0\n\n    # Warm up both models (first pass triggers compilation)\n    print('Warming up (first pass compiles the model -- takes ~30s)...')\n    _ = bench(model_eager,    n_epochs=1)\n    _ = bench(model_compiled, n_epochs=1)\n\n    t_eager    = bench(model_eager,    n_epochs=5)\n    t_compiled = bench(model_compiled, n_epochs=5)\n\n    print(f'5-epoch benchmark on {DEVICE}:')\n    print(f'  Eager (standard):  {t_eager:.2f}s')\n    print(f'  torch.compile:     {t_compiled:.2f}s')\n    print(f'  Speedup:           {t_eager/t_compiled:.2f}x')\n    print()\n    print('Note: speedup is more pronounced for larger models and longer runs.')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Chapter 7 Summary\n\n### Key Takeaways\n\n- A **tensor** is a GPU-aware, autograd-enabled array. `requires_grad=True` enables gradient tracking.\n- **`model.train()` / `model.eval()`** toggle Dropout and BatchNorm behaviour.\n  Forgetting `.eval()` during validation is a common subtle bug.\n- The **training loop** has five steps every iteration: zero_grad, forward, loss, backward, step.\n- **`torch.no_grad()`** disables autograd during inference. Always use it for validation and test.\n- **BatchNorm** normalises activations within a batch -- accelerates training and\n  reduces sensitivity to learning rate.\n- **Dropout** randomly zeroes activations during training. Disabled automatically during eval.\n- **`BCEWithLogitsLoss`** is numerically more stable than `Sigmoid + BCELoss`.\n- **Save `state_dict`, not the model object.** `torch.save(model.state_dict(), path)` is portable.\n- The **LR Range Test** finds a good learning rate in one short run without expensive grid search.\n  Look for the learning rate just before the loss minimum -- one order of magnitude below it.\n- **Mixed precision (`torch.cuda.amp`)** gives 2-4x speedup and 50% memory reduction on GPU\n  with two extra lines of code: `GradScaler` and `autocast()`.\n- **`torch.compile`** (PyTorch 2.0+) compiles the model to optimised GPU kernels.\n  One line, no API changes, 20-50% additional speedup on large models.\n\n### Project Thread Status\n\n| Task | Architecture | Result |\n|------|-------------|--------|\n| Salary regression | MLP 128-64-32 + BatchNorm + Dropout | R^2 reported |\n| Python classification | MLP 64-32 + BatchNorm + Dropout | Accuracy reported |\n| Model persistence | torch.save / torch.load | Round-trip verified |\n| LR Range Test | Exponential LR sweep | Suggested LR identified |\n| Mixed precision | torch.cuda.amp | Speedup benchmarked |\n| torch.compile | PyTorch 2.0 compiler | Speedup benchmarked |\n\n---\n\n### What's Next: Chapter 8 -- NLP and Transformers\n\nChapter 8 applies neural networks to text: tokenisation, embeddings,\nsentiment analysis with a pre-trained transformer, fine-tuning a BERT-family\nmodel on SO 2025 developer comments, and RAG (Retrieval-Augmented Generation)\n-- the dominant production NLP pattern.\n\n---\n\n*End of Chapter 7 -- Python for AI/ML*  \n[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n"
    }
  ]
}