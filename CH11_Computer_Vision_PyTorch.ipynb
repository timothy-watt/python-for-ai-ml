{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Chapter 11 -- Computer Vision with PyTorch\n## *Python for AI/ML: A Complete Learning Journey*\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/CH11_Computer_Vision_PyTorch.ipynb)\n&nbsp;&nbsp;[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n\n---\n\n**Part:** 4 -- Production and Deployment  \n**Prerequisites:** Chapter 7 (Deep Learning with PyTorch)  \n**Estimated time:** 5-6 hours\n\n---\n\n> **Before running this notebook:** go to **Runtime → Change runtime type → T4 GPU**.\n> Transfer learning fine-tuning in Section 11.4 requires GPU.\n\n---\n\n### Learning Objectives\n\nBy the end of this chapter you will be able to:\n\n- Explain how convolutional layers detect spatial features in images\n- Build a CNN from scratch using `nn.Conv2d`, `nn.MaxPool2d`, and `nn.Linear`\n- Use `torchvision.transforms` to build an image augmentation pipeline\n- Load datasets with `torchvision.datasets` and `ImageFolder`\n- Apply transfer learning: freeze a pre-trained ResNet and replace its head\n- Fine-tune all layers of a pre-trained model with a lower learning rate\n- Visualise what a CNN has learned: feature maps and activation maximisation\n- Interpret predictions with Grad-CAM heatmaps\n\n---\n\n### Project Thread -- Chapter 11\n\nWe work with the **CIFAR-10** dataset (60,000 32x32 colour images, 10 classes)\nwhich is built into torchvision. We build three progressively more powerful models:\n\n1. **Custom CNN from scratch** -- understand every component\n2. **ResNet-18 with frozen backbone** -- transfer learning in minutes\n3. **ResNet-18 fine-tuned end-to-end** -- best accuracy\n\nCIFAR-10 is small enough to train quickly on a free Colab GPU\nwhile being complex enough to demonstrate why deep CNNs beat shallow ones.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Setup\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.optim.lr_scheduler import OneCycleLR\n\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torchvision.datasets import CIFAR10\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'PyTorch:  {torch.__version__}')\nprint(f'Device:   {DEVICE}')\nprint(f'Torchvision: {torchvision.__version__}')\n\nRANDOM_STATE = 42\ntorch.manual_seed(RANDOM_STATE)\nnp.random.seed(RANDOM_STATE)\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.dpi'] = 110\n\nCLASSES = ('plane','car','bird','cat','deer',\n           'dog','frog','horse','ship','truck')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 11.1 -- How Convolutional Neural Networks Work\n\nA fully-connected layer treats every pixel as an independent feature.\nFor a 32x32 colour image that is 32×32×3 = 3,072 inputs — and a 224x224\nimage is 150,528 inputs. This is computationally expensive and ignores\nthe spatial structure of images: nearby pixels are related, and the same\npattern (an edge, a curve) can appear anywhere in the image.\n\n**Convolutional layers** solve this with two ideas:\n\n**Local connectivity:** each neuron connects only to a small region\nof the input (the receptive field), not the whole image.\n\n**Weight sharing:** the same filter (kernel) is applied at every position.\nA filter that detects horizontal edges detects them everywhere in the image\nusing the same weights. This reduces parameters dramatically.\n\n**The building blocks:**\n- `nn.Conv2d(in_channels, out_channels, kernel_size)` -- learns filters\n- `nn.MaxPool2d(kernel_size)` -- downsamples by taking the max in each window\n- `nn.BatchNorm2d(channels)` -- normalises activations (same as Ch 7, but 2D)\n- `nn.ReLU()` -- non-linearity applied after each conv layer\n\nEarly layers learn low-level features (edges, colours).\nDeeper layers combine these into higher-level concepts (textures, shapes, objects).\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 11.1.1 -- Load CIFAR-10 with augmentation transforms\n\n# Training augmentation: random flips and crops make the model\n# robust to variations in position and orientation\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.ToTensor(),\n    # Normalise with CIFAR-10 channel means and stds\n    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n                         std= [0.2470, 0.2435, 0.2616]),\n])\n\n# Validation/test: only normalise, no augmentation\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n                         std= [0.2470, 0.2435, 0.2616]),\n])\n\n# Download CIFAR-10 (~170MB, cached after first run)\ntrain_dataset = CIFAR10(root='/tmp/cifar10', train=True,\n                        download=True, transform=train_transform)\ntest_dataset  = CIFAR10(root='/tmp/cifar10', train=False,\n                        download=True, transform=test_transform)\n\n# Split training into train + validation\nn_val   = 5000\nn_train = len(train_dataset) - n_val\ntrain_ds, val_ds = random_split(\n    train_dataset, [n_train, n_val],\n    generator=torch.Generator().manual_seed(RANDOM_STATE)\n)\n\ntrain_loader = DataLoader(train_ds,   batch_size=128, shuffle=True,  num_workers=2, pin_memory=True)\nval_loader   = DataLoader(val_ds,     batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\ntest_loader  = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n\nprint(f'Train: {len(train_ds):,}  Val: {len(val_ds):,}  Test: {len(test_dataset):,}')\nprint(f'Classes: {CLASSES}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 11.1.2 -- Visualise sample images\n\n# Get one batch and undo normalisation for display\nimages, labels = next(iter(DataLoader(test_dataset, batch_size=16, shuffle=True)))\n\nmean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3,1,1)\nstd  = torch.tensor([0.2470, 0.2435, 0.2616]).view(3,1,1)\nimages_display = (images * std + mean).clamp(0, 1)\n\nfig, axes = plt.subplots(2, 8, figsize=(16, 5))\nfor i, ax in enumerate(axes.flatten()):\n    img = images_display[i].permute(1, 2, 0).numpy()\n    ax.imshow(img)\n    ax.set_title(CLASSES[labels[i]], fontsize=8)\n    ax.axis('off')\nplt.suptitle('CIFAR-10 Sample Images (16 random test examples)',\n             fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 11.2 -- Building a CNN from Scratch\n\nBefore using pre-trained models, we build a CNN from scratch so every\ncomponent is transparent. This architecture follows the classic pattern:\nstacked conv blocks (conv → batchnorm → relu → pool) followed by\na classifier head (flatten → dense → output).\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 11.2.1 -- Define a custom CNN\n\nclass CifarCNN(nn.Module):\n    \"\"\"\n    Custom CNN for CIFAR-10 (32x32 colour images, 10 classes).\n\n    Architecture:\n        Conv Block 1: 3  -> 32  channels, 3x3 kernel\n        Conv Block 2: 32 -> 64  channels, 3x3 kernel\n        Conv Block 3: 64 -> 128 channels, 3x3 kernel\n        Classifier:   128*4*4 -> 256 -> 10\n    \"\"\"\n\n    def _conv_block(self, in_ch, out_ch):\n        return nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),       # halve spatial dimensions\n            nn.Dropout2d(0.1),\n        )\n\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.block1 = self._conv_block(3,   32)\n        self.block2 = self._conv_block(32,  64)\n        self.block3 = self._conv_block(64, 128)\n        # After 3 MaxPool2d(2): 32 -> 16 -> 8 -> 4\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(128 * 4 * 4, 256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(256, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        return self.classifier(x)\n\n\ncnn = CifarCNN(num_classes=10).to(DEVICE)\nn_params = sum(p.numel() for p in cnn.parameters() if p.requires_grad)\nprint(f'CifarCNN parameters: {n_params:,}')\n\n# Test forward pass\nx_test = torch.randn(4, 3, 32, 32).to(DEVICE)\nout    = cnn(x_test)\nprint(f'Input shape:  {x_test.shape}')\nprint(f'Output shape: {out.shape}  (4 samples, 10 class scores)')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 11.2.2 -- Training utilities (reuse Ch 7 pattern)\n\ndef train_epoch_clf(model, loader, criterion, optimizer, scheduler=None):\n    model.train()\n    total_loss, correct, total = 0.0, 0, 0\n    for images, labels in loader:\n        images, labels = images.to(DEVICE), labels.to(DEVICE)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss    = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()\n        total_loss += loss.item() * len(images)\n        correct    += (outputs.argmax(1) == labels).sum().item()\n        total      += len(images)\n    return total_loss / total, correct / total\n\n\ndef evaluate_clf(model, loader, criterion):\n    model.eval()\n    total_loss, correct, total = 0.0, 0, 0\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(DEVICE), labels.to(DEVICE)\n            outputs = model(images)\n            loss    = criterion(outputs, labels)\n            total_loss += loss.item() * len(images)\n            correct    += (outputs.argmax(1) == labels).sum().item()\n            total      += len(images)\n            all_preds.extend(outputs.argmax(1).cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    return total_loss / total, correct / total, all_preds, all_labels\n\n\nprint('Training utilities defined.')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 11.2.3 -- Train the custom CNN for 20 epochs\n\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\noptimizer = optim.AdamW(cnn.parameters(), lr=1e-3, weight_decay=1e-4)\nscheduler = OneCycleLR(\n    optimizer, max_lr=1e-2,\n    steps_per_epoch=len(train_loader), epochs=20\n)\n\nN_EPOCHS = 20\ncnn_history = {'train_loss':[], 'val_loss':[], 'train_acc':[], 'val_acc':[]}\nbest_val_acc = 0.0\nbest_cnn_weights = None\n\nprint(f'Training CifarCNN for {N_EPOCHS} epochs on {DEVICE}...')\nprint(f'{\"Epoch\":>6}  {\"Train Loss\":>11}  {\"Train Acc\":>10}  {\"Val Acc\":>9}')\nprint('-' * 42)\n\nfor epoch in range(1, N_EPOCHS + 1):\n    tr_loss, tr_acc = train_epoch_clf(cnn, train_loader, criterion, optimizer, scheduler)\n    val_loss, val_acc, _, _ = evaluate_clf(cnn, val_loader, criterion)\n    cnn_history['train_loss'].append(tr_loss)\n    cnn_history['val_loss'].append(val_loss)\n    cnn_history['train_acc'].append(tr_acc)\n    cnn_history['val_acc'].append(val_acc)\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        best_cnn_weights = {k: v.clone() for k, v in cnn.state_dict().items()}\n    if epoch % 5 == 0 or epoch == 1:\n        print(f'{epoch:>6}  {tr_loss:>11.4f}  {tr_acc:>10.4f}  {val_acc:>9.4f}')\n\ncnn.load_state_dict(best_cnn_weights)\n_, test_acc, _, _ = evaluate_clf(cnn, test_loader, criterion)\nprint(f'Best val acc: {best_val_acc:.4f}  |  Test acc: {test_acc:.4f}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 11.3 -- Visualising What the CNN Learned\n\nA common criticism of deep learning is that it is a black box.\nFor CNNs, this is less true than it seems -- we can directly inspect\nthe intermediate activations (feature maps) to see what each layer detects.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 11.3.1 -- Visualise feature maps from the first conv block\n\ncnn.eval()\n\n# Pick one test image\nsample_img, sample_label = test_dataset[42]\nsample_tensor = sample_img.unsqueeze(0).to(DEVICE)   # add batch dim\n\n# Register a forward hook to capture the output of block1\nfeature_maps = {}\n\ndef hook_fn(module, input, output):\n    feature_maps['block1'] = output.detach().cpu()\n\nhook = cnn.block1.register_forward_hook(hook_fn)\n\nwith torch.no_grad():\n    _ = cnn(sample_tensor)\n\nhook.remove()\n\nmaps = feature_maps['block1'][0]   # shape: (32, 16, 16) -- 32 filters\nprint(f'Feature map shape: {maps.shape}  (32 filters, 16x16 after MaxPool)')\n\n# Display original image and first 16 feature maps\nfig = plt.figure(figsize=(16, 5))\ngs  = gridspec.GridSpec(2, 9, figure=fig)\n\n# Original image\nmean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3,1,1)\nstd  = torch.tensor([0.2470, 0.2435, 0.2616]).view(3,1,1)\norig = (sample_img * std + mean).clamp(0,1).permute(1,2,0).numpy()\nax0  = fig.add_subplot(gs[:, 0])\nax0.imshow(orig)\nax0.set_title(f'Input:\\n{CLASSES[sample_label]}', fontsize=9)\nax0.axis('off')\n\nfor i in range(16):\n    row = i // 8\n    col = (i % 8) + 1\n    ax  = fig.add_subplot(gs[row, col])\n    ax.imshow(maps[i].numpy(), cmap='viridis')\n    ax.set_title(f'F{i}', fontsize=7)\n    ax.axis('off')\n\nplt.suptitle('CNN Feature Maps: First Conv Block (16 of 32 filters)',\n             fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 11.4 -- Transfer Learning with ResNet-18\n\nResNet-18 is a 18-layer residual network pre-trained on ImageNet --\n1.2 million images across 1,000 classes. Its weights encode rich visual\nknowledge: edges, textures, shapes, objects.\n\n**Transfer learning** re-uses this knowledge for a new task by:\n1. Loading the pre-trained weights\n2. Freezing all layers (they are not updated during training)\n3. Replacing the final classification head with a new one for our classes\n4. Training only the new head\n\nThis takes minutes instead of hours and often outperforms a custom CNN\ntrained from scratch, because the pre-trained features are so rich.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 11.4.1 -- ResNet-18 with frozen backbone (feature extraction)\n\n# Load pre-trained ResNet-18\nresnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n\n# Freeze all parameters -- they will not be updated\nfor param in resnet.parameters():\n    param.requires_grad = False\n\n# Replace the final fully-connected layer\n# ResNet-18's original fc: 512 -> 1000 (ImageNet classes)\n# Our new fc: 512 -> 10 (CIFAR-10 classes)\nn_features = resnet.fc.in_features\nresnet.fc  = nn.Linear(n_features, 10)\n# Only the new head has requires_grad=True\n\nresnet = resnet.to(DEVICE)\n\ntrainable = sum(p.numel() for p in resnet.parameters() if p.requires_grad)\ntotal     = sum(p.numel() for p in resnet.parameters())\nprint(f'ResNet-18 total parameters:     {total:,}')\nprint(f'Trainable (head only):          {trainable:,}  ({trainable/total*100:.1f}%)')\nprint(f'Frozen (backbone):              {total-trainable:,}  ({(total-trainable)/total*100:.1f}%)')\n\n# Larger transforms for ResNet (expects 224x224 but we adapt for CIFAR)\nresnet_transform = transforms.Compose([\n    transforms.Resize(64),           # upsample 32x32 to 64x64\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std= [0.229, 0.224, 0.225]),   # ImageNet stats\n])\nresnet_aug = transforms.Compose([\n    transforms.Resize(64),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(64, padding=8),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std= [0.229, 0.224, 0.225]),\n])\n\nrn_train_ds = CIFAR10('/tmp/cifar10', train=True,  download=False, transform=resnet_aug)\nrn_test_ds  = CIFAR10('/tmp/cifar10', train=False, download=False, transform=resnet_transform)\nrn_train_ds, rn_val_ds = random_split(\n    rn_train_ds, [45000, 5000],\n    generator=torch.Generator().manual_seed(RANDOM_STATE)\n)\nrn_train_loader = DataLoader(rn_train_ds, batch_size=128, shuffle=True,  num_workers=2)\nrn_val_loader   = DataLoader(rn_val_ds,   batch_size=256, shuffle=False, num_workers=2)\nrn_test_loader  = DataLoader(rn_test_ds,  batch_size=256, shuffle=False, num_workers=2)\nprint('ResNet data loaders ready.')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 11.4.2 -- Train the ResNet head for 10 epochs\n\nrn_criterion = nn.CrossEntropyLoss()\nrn_optimizer = optim.AdamW(resnet.fc.parameters(), lr=1e-3, weight_decay=1e-4)\n\nN_RN_EPOCHS = 10\nrn_history  = {'train_acc': [], 'val_acc': []}\nbest_rn_acc = 0.0\nbest_rn_weights = None\n\nprint(f'Training ResNet-18 head for {N_RN_EPOCHS} epochs on {DEVICE}...')\nprint(f'{\"Epoch\":>6}  {\"Train Acc\":>10}  {\"Val Acc\":>9}')\nprint('-' * 30)\n\nfor epoch in range(1, N_RN_EPOCHS + 1):\n    tr_loss, tr_acc = train_epoch_clf(resnet, rn_train_loader, rn_criterion, rn_optimizer)\n    val_loss, val_acc, _, _ = evaluate_clf(resnet, rn_val_loader, rn_criterion)\n    rn_history['train_acc'].append(tr_acc)\n    rn_history['val_acc'].append(val_acc)\n    if val_acc > best_rn_acc:\n        best_rn_acc = val_acc\n        best_rn_weights = {k: v.clone() for k, v in resnet.state_dict().items()}\n    if epoch % 2 == 0 or epoch == 1:\n        print(f'{epoch:>6}  {tr_acc:>10.4f}  {val_acc:>9.4f}')\n\nresnet.load_state_dict(best_rn_weights)\n_, rn_test_acc, _, _ = evaluate_clf(resnet, rn_test_loader, rn_criterion)\nprint(f'ResNet head-only  test accuracy: {rn_test_acc:.4f}')\nprint(f'Custom CNN        test accuracy: {test_acc:.4f}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 11.4.3 -- Compare models and plot training curves\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Training curves -- custom CNN\nepochs_cnn = range(1, len(cnn_history['val_acc']) + 1)\naxes[0].plot(epochs_cnn, cnn_history['train_acc'], '#E8722A', linewidth=2, label='Train')\naxes[0].plot(epochs_cnn, cnn_history['val_acc'],   '#2E75B6', linewidth=2, label='Val')\naxes[0].axhline(test_acc, color='green', linestyle='--', linewidth=1.5,\n                label=f'Test acc={test_acc:.3f}')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Accuracy')\naxes[0].set_title('Custom CNN from Scratch')\naxes[0].legend()\naxes[0].set_ylim(0, 1)\n\n# Training curves -- ResNet\nepochs_rn = range(1, len(rn_history['val_acc']) + 1)\naxes[1].plot(epochs_rn, rn_history['train_acc'], '#E8722A', linewidth=2, label='Train')\naxes[1].plot(epochs_rn, rn_history['val_acc'],   '#2E75B6', linewidth=2, label='Val')\naxes[1].axhline(rn_test_acc, color='green', linestyle='--', linewidth=1.5,\n                label=f'Test acc={rn_test_acc:.3f}')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Accuracy')\naxes[1].set_title('ResNet-18 Transfer Learning (head only, 10 epochs)')\naxes[1].legend()\naxes[1].set_ylim(0, 1)\n\nplt.suptitle('CIFAR-10: Custom CNN vs Transfer Learning',\n             fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(f'Custom CNN (20 epochs):           {test_acc:.4f}')\nprint(f'ResNet-18 head-only (10 epochs):  {rn_test_acc:.4f}')\nprint(f'Improvement from transfer learning: {(rn_test_acc - test_acc)*100:+.1f} pp')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Chapter 11 Summary\n\n### Key Takeaways\n\n- **Convolutional layers** apply learned filters across the entire image using\n  weight sharing. Early layers detect edges; later layers detect shapes and objects.\n- **`padding=1` with a 3x3 kernel** preserves spatial dimensions.\n  `MaxPool2d(2)` halves them. After three pool layers: 32 → 16 → 8 → 4.\n- **Data augmentation** (random flips, crops, colour jitter) is the single\n  most effective regularisation technique for image models. Always use it.\n- **`OneCycleLR`** is the recommended scheduler for CNNs: it warms up,\n  peaks, then anneals the learning rate in one cycle per training run.\n- **Transfer learning beats training from scratch** on small datasets.\n  Freeze the backbone, train only the head first; then optionally unfreeze\n  all layers at a 10x lower learning rate for further gains.\n- **Feature map visualisation** with forward hooks is the primary tool\n  for understanding what a CNN has learned.\n- **ImageNet normalisation** (mean=[0.485, 0.456, 0.406]) must be used\n  with all torchvision pre-trained models -- using wrong stats degrades accuracy.\n\n### Model Comparison\n\n| Model | Epochs | Parameters | Test Accuracy |\n|-------|--------|------------|---------------|\n| Custom CNN (from scratch) | 20 | ~300k | reported above |\n| ResNet-18 (head only) | 10 | 11M (512 trainable) | reported above |\n\n---\n\n### What's Next\n\nChapters 10 and 11 complete Part 4. The appendices cover:\nreinforcement learning (App D), SQL for data scientists (App E),\nand Git/GitHub for ML projects (App F).\n\n---\n\n*End of Chapter 11 -- Python for AI/ML*  \n[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n"
    }
  ]
}