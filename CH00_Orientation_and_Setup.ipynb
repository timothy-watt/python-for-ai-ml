{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Chapter 0 â€” Orientation & Setup\n## *Python for AI/ML: A Complete Learning Journey*\n\n[![Back to TOC](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)](Python_for_AIML_TOC.ipynb)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/CH00_Orientation_and_Setup.ipynb)\n\n---\n\n**Part:** Pre-requisite (before Part 1)  \n**Prerequisites:** None â€” this is where everyone starts  \n**Estimated time:** 45â€“60 minutes  \n\n---\n\n### ðŸŽ¯ Learning Objectives\n\nBy the end of this chapter you will be able to:\n\n- Navigate the Google Colab interface confidently\n- Connect Google Drive to a Colab session to save your work\n- Enable GPU/TPU runtimes for deep learning chapters\n- Install and import Python packages in Colab using `!pip install`\n- Load and preview the Stack Overflow 2025 dataset that runs through the entire book\n- Understand the structure and key columns of the project dataset\n- (Optional) Set up a local Python environment with Anaconda and VS Code\n\n---\n\n### ðŸ§µ Project Thread â€” Chapter 0\n\nThis chapter introduces the **Stack Overflow 2025 Developer Survey** â€” the dataset  \nthat powers every hands-on example across all 9 chapters of this book.  \nBy the end of this chapter, you will have it loaded, previewed, and understood.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 0.1 â€” What This Book Is and How to Use It\n\n### The Three-Part Journey\n\nThis book is structured as a deliberate, cumulative learning arc across three parts:\n\n- **Part 1 (Chapters 1â€“2): Core Python Fundamentals** â€” Variables, control flow, data structures, functions, OOP, file I/O, and error handling. Pure Python, no external libraries.\n- **Part 2 (Chapters 3â€“4): Data Science Foundations** â€” NumPy, Pandas, Matplotlib, and Seaborn. The tools every data scientist uses daily.\n- **Part 3 (Chapters 5â€“9): Machine Learning & AI** â€” SciPy, scikit-learn, PyTorch, Keras 3, NLP, Hugging Face Transformers, and professional deployment practices.\n\n### The Project Thread\n\nA single dataset â€” the **Stack Overflow 2025 Developer Survey** â€” runs through every chapter.  \nYou will load it in Chapter 0, clean it in Chapter 3, visualize it in Chapter 4, build ML models on it in Chapters 6 and 7, run NLP on its text fields in Chapter 8, and deploy a model trained on it in Chapter 9.\n\nThis continuity is intentional: real data science work involves a single dataset across many weeks and many techniques. This book mirrors that reality.\n\n### How to Use Each Notebook\n\nEvery chapter notebook follows the same structure:\n1. **Section header** â€” context and learning objectives\n2. **Pre-code explanation** â€” plain-English description of what the code does and why\n3. **Code cell** â€” Python code with inline comments explaining *why* each line works\n4. **Post-code interpretation** â€” what the output means, common errors, key takeaways\n5. **Chapter summary** â€” skills gained and preview of what's next\n\n> ðŸ’¡ **The Golden Rule:** Comments in every code cell explain **WHY** a line works, not just what it does.  \n> Bad: `# multiply by 2`  \n> Good: `# divide by maximum value to compress all salaries into the 0â€“1 range that gradient descent needs`\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 0.2 â€” Google Colab Interface Orientation\n\nGoogle Colab is a free, cloud-based Jupyter Notebook environment that runs entirely in your browser.  \nIt requires no local installation, gives you access to free GPUs, and saves notebooks directly to Google Drive.\n\n### Key Interface Elements\n\n| Element | Location | What It Does |\n|---|---|---|\n| **Runtime menu** | Top menu bar | Controls the Python kernel â€” restart, reconnect, change hardware |\n| **+ Code / + Text** | Top left under menu | Adds a new code or markdown cell below the current one |\n| **â–¶ Run button** | Left of each cell | Executes that cell; keyboard shortcut: `Shift + Enter` |\n| **RAM / Disk indicator** | Top right | Shows current memory usage â€” click it for full resource details |\n| **Table of Contents** | Left sidebar (â‰¡) | Navigates by markdown headers â€” use this to jump between sections |\n| **Files** | Left sidebar (ðŸ“) | Browse the Colab filesystem and upload local files |\n\n### Essential Keyboard Shortcuts\n\n| Shortcut | Action |\n|---|---|\n| `Shift + Enter` | Run current cell, move to next |\n| `Ctrl + Enter` | Run current cell, stay |\n| `Ctrl + M + B` | Insert cell below |\n| `Ctrl + M + D` | Delete current cell |\n| `Ctrl + M + M` | Convert cell to Markdown |\n| `Ctrl + M + Y` | Convert cell to Code |\n| `Ctrl + /` | Comment / uncomment selected lines |\n\nLet's run our very first cell. This is the universal first program in every language â€” it confirms that the environment is working and introduces the `print()` function.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Our very first Python statement.\n#\n# print() is a built-in Python function that displays output to the\n# screen. We pass it a \"string\" (text wrapped in quotes) as input,\n# and it prints that text followed by a newline character.\n#\n# This is the traditional first program in any language â€” simple,\n# instant feedback, and proof that everything is working.\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nprint(\"Hello from Google Colab â€” Python for AI/ML is ready!\")\n\n# We can print multiple things on separate lines by calling print() again.\n# Each call to print() starts on a new line automatically.\nprint(\"Python version check coming next...\")\n\n# We can also print the result of an expression directly.\n# Python evaluates the expression first, then passes the result to print().\nprint(\"2025 minus 1991 =\", 2025 - 1991)   # just a fun calculation\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Checking Your Python Version\n\nBefore we go further, let's confirm which version of Python is running.  \nThis book requires Python 3.10 or higher. Google Colab always provides a recent version, so this is mainly for awareness.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# The 'sys' module is part of Python's standard library â€” it ships with Python\n# and never needs to be installed. It gives us access to system-level information\n# like the Python version, the operating system, and the file system path.\nimport sys  # 'import' makes a module available in the current session\n\n# sys.version is a string containing the full Python version details.\n# f-strings (formatted strings) let us embed Python expressions directly\n# inside a string using curly braces {}.\nprint(f\"Python version: {sys.version}\")\n\n# sys.version_info is a named tuple â€” we can access its fields by name.\n# We check that the major version is 3 and minor is at least 10.\nmajor = sys.version_info.major   # e.g., 3\nminor = sys.version_info.minor   # e.g., 11\n\nif major == 3 and minor >= 10:\n    # This branch runs when the condition is True\n    print(f\"âœ… Python {major}.{minor} â€” all good! This book requires Python 3.10+\")\nelse:\n    # This branch runs when the condition is False\n    print(f\"âš ï¸  Python {major}.{minor} detected. Please use Python 3.10 or higher.\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 0.3 â€” Mounting Google Drive\n\nBy default, any files you create in a Colab session are **temporary** â€” they disappear when the session ends.  \nMounting your Google Drive gives Colab persistent access to your Drive storage, so you can:\n\n- Save notebooks and outputs permanently\n- Load your own data files\n- Keep checkpoints from long training runs in Chapters 7 and 8\n\n> âš ï¸ **Run this cell individually** â€” click the **â–¶** button on the left of the cell, do not use Run All for this step.  \n> A Google authentication popup will appear â€” click **Connect to Google Drive** and sign in.  \n> **If you skip Drive mounting entirely, that is fine** â€” all notebooks load the dataset directly from GitHub.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# GOOGLE DRIVE MOUNT\n#\n# This cell connects your Google Drive to Colab so your work persists\n# between sessions. When you run it, a permissions popup will appear â€”\n# click 'Connect to Google Drive' and sign in with your Google account.\n#\n# IMPORTANT: If you are running this as part of 'Run All', the popup\n# cannot appear automatically. Run THIS CELL INDIVIDUALLY first by\n# clicking the â–¶ button on the left, complete the sign-in, then\n# use Runtime â†’ Run after to continue with the remaining cells.\n#\n# If you skip Drive mounting entirely, that is fine â€” all notebooks\n# load the dataset directly from GitHub, so Drive is never required.\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nimport os\n\ntry:\n    from google.colab import drive\n\n    # drive.mount() triggers the Google authentication popup.\n    # force_remount=False means: if Drive is already mounted, skip re-mounting.\n    drive.mount('/content/drive', force_remount=False)\n\n    drive_root = '/content/drive/MyDrive'\n\n    if os.path.exists(drive_root):\n        print(f\"âœ… Google Drive mounted successfully at {drive_root}\")\n        # Show first 5 items as a sanity check â€” these are YOUR Drive folders\n        items = os.listdir(drive_root)[:5]\n        print(f\"   Top-level items: {items}\")\n    else:\n        print(\"âš ï¸  Mount appeared to succeed but Drive path not found.\")\n        print(\"   Try: Runtime â†’ Disconnect and delete runtime, then re-run this cell.\")\n\nexcept Exception as e:\n    # This branch runs if:\n    #   - You ran this as part of Run All (popup could not appear)\n    #   - You declined the Drive permissions request\n    #   - You are running outside of Google Colab\n    print(\"â„¹ï¸  Google Drive not mounted â€” this is fine!\")\n    print(\"   All notebooks load the SO 2025 dataset directly from GitHub.\")\n    print(\"   Drive mounting is only needed if you want to SAVE outputs permanently.\")\n    print()\n    print(\"   To mount Drive manually: run THIS CELL INDIVIDUALLY (click the â–¶ button)\")\n    print(\"   and follow the Google authentication popup.\")\n    print()\n    print(f\"   (Technical detail: {type(e).__name__}: {e})\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 0.3b â€” GPU and TPU Runtimes\n\nGoogle Colab offers three compute options:\n\n| Runtime | Best For | How to Enable |\n|---|---|---|\n| **CPU** (default) | Chapters 0â€“6 â€” no GPU needed | Default, nothing to change |\n| **T4 GPU** (free) | Chapters 7 & 8 â€” deep learning & transformers | Runtime â†’ Change runtime type â†’ T4 GPU |\n| **A100 GPU** (Colab Pro) | Large models, faster training | Requires Colab Pro subscription |\n| **TPU** (free) | Specialized tensor workloads | Runtime â†’ Change runtime type â†’ TPU v2 |\n\n> ðŸ“Œ **For now (Chapter 0), CPU is perfectly fine.** We'll remind you to switch to GPU at the start of Chapters 7 and 8.\n\nThe cell below checks what hardware is currently available â€” run it now to see your current runtime.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# torch is PyTorch's main library. We import it here just to check GPU availability.\n# We'll learn PyTorch in depth in Chapter 7 â€” for now we're just using it as a detector.\n#\n# NOTE: If PyTorch isn't installed in your environment, this cell will print a message\n# and skip the GPU check gracefully. The 'try/except' pattern handles errors cleanly â€”\n# we cover this fully in Chapter 2.\ntry:\n    import torch  # PyTorch deep learning framework\n\n    # torch.cuda.is_available() returns True if a CUDA-capable GPU is detected.\n    # CUDA is NVIDIA's parallel computing platform â€” PyTorch uses it to run on GPUs.\n    gpu_available = torch.cuda.is_available()\n\n    if gpu_available:\n        # torch.cuda.get_device_name(0) returns the name of the first (index 0) GPU.\n        gpu_name = torch.cuda.get_device_name(0)\n        print(f\"âœ… GPU detected: {gpu_name}\")\n        print(f\"   GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n    else:\n        print(\"â„¹ï¸  No GPU detected â€” running on CPU.\")\n        print(\"   This is fine for Chapters 0â€“6.\")\n        print(\"   For Chapters 7â€“8: Runtime â†’ Change runtime type â†’ T4 GPU\")\n\nexcept ImportError:\n    # ImportError is raised when Python can't find the requested module.\n    # PyTorch may not be pre-installed in all environments â€” that's okay here.\n    print(\"â„¹ï¸  PyTorch not yet installed â€” GPU check skipped.\")\n    print(\"   We'll install and use PyTorch fully in Chapter 7.\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 0.4 â€” Installing Packages in Google Colab\n\nPython's power comes from its enormous ecosystem of third-party libraries.  \nIn Colab, you install libraries using `!pip install` â€” the `!` prefix tells Colab to run the command in the **terminal shell** rather than as Python code.\n\n### How `!pip install` Works\n\n```\n!pip install library_name==version_number\n```\n\n- `pip` is Python's package manager â€” it downloads libraries from PyPI (the Python Package Index)\n- The `==version_number` part is optional but **recommended** in a book â€” it pins the exact version so your code keeps working as libraries evolve\n- Most libraries you'll need in this book are **pre-installed in Colab** â€” you only need to install the ones that aren't\n\n### Libraries Pre-installed in Colab (no install needed)\nNumPy Â· Pandas Â· Matplotlib Â· Seaborn Â· scikit-learn Â· SciPy Â· TensorFlow Â· PyTorch\n\n### Libraries We'll Install When Needed\nPlotly Â· spaCy Â· Hugging Face Transformers Â· XGBoost Â· LightGBM Â· FastAPI Â· Optuna\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# The ! prefix runs this as a shell command, not Python code.\n# pip install downloads the specified package from PyPI and installs it\n# into the current Python environment.\n#\n# We install plotly here as a demonstration â€” it's not pre-installed in Colab\n# and we'll use it in Chapter 4 for interactive visualizations.\n#\n# The -q flag means \"quiet\" â€” it suppresses most of the installation output\n# so the cell doesn't flood with download progress text.\n!pip install plotly -q\n\n# After installation, we verify it worked by importing the library\n# and printing its version number.\nimport plotly  # if this line runs without error, installation succeeded\n\nprint(f\"âœ… Plotly {plotly.__version__} installed and ready\")\n\n# â”€â”€ Good practice: always print library versions at the top of notebooks â”€â”€\n# This makes it easy to reproduce results if someone runs the notebook later\n# on a different version. We'll do this systematically in each chapter.\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport seaborn as sns\n\nprint(f\"   NumPy:      {np.__version__}\")\nprint(f\"   Pandas:     {pd.__version__}\")\nprint(f\"   Matplotlib: {matplotlib.__version__}\")\nprint(f\"   Seaborn:    {sns.__version__}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 0.5 â€” Introducing the Stack Overflow 2025 Developer Survey\n\n### What Is This Dataset?\n\nThe **Stack Overflow Annual Developer Survey** is the world's largest survey of software developers.  \nConducted every year since 2011, the 2025 edition captured responses from **~49,000 developers**  \nacross **180+ countries**, covering career paths, salaries, tools, technologies, and attitudes toward AI.\n\nStack Overflow releases the full dataset publicly under the **Open Database License (ODbL)** â€”  \nit is freely usable for any purpose, including education and research.\n\n### Why This Dataset for This Book?\n\n| Reason | Detail |\n|---|---|\n| **Directly relevant** | You are a developer learning Python. This dataset is *about* developers. Every insight feels personal. |\n| **Task-rich** | Supports regression, classification, clustering, AND NLP from a single source |\n| **Realistic mess** | Real missing values, multi-value columns, salary outliers â€” authentic cleaning practice |\n| **Ethics built in** | Gender pay gap, geographic disparities, AI adoption gaps â€” real bias to audit in Chapter 9 |\n| **Current** | 2025 data reflects the industry as it exists right now, including AI tool adoption |\n\n### The Curated Subset We Use\n\nThe full survey has ~49,000 rows and 80+ columns. For this book we use a **curated subset**:\n- **15,000 rows** â€” English-language respondents with non-null salary data\n- **~18 columns** â€” the most relevant for our ML tasks\n- Hosted at: `https://raw.githubusercontent.com/timothy-watt/python-for-ai-ml/main/data/so_survey_2025_curated.csv`\n\nThe cell below loads it directly â€” no download, no Drive mounting required.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Loading the Dataset\n\nThe cell below loads the SO 2025 curated subset directly from GitHub using Pandas.  \nWe'll see exactly what's in it, understand its shape, and look at the first few rows.\n\n> ðŸ” **This exact loading pattern repeats at the start of every chapter (Ch 3 onward).**  \n> Memorize the `pd.read_csv(URL)` pattern â€” it's the most common first line in real data science work.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# pandas is the primary data manipulation library in Python's data science stack.\n# We import it with the alias 'pd' â€” this is a universal convention. Every data\n# scientist in the world writes 'pd.something', not 'pandas.something'.\nimport pandas as pd\n\n# The URL where our curated SO 2025 dataset lives.\n# This is a raw GitHub URL â€” it serves the file directly as plain text (CSV format).\n# pd.read_csv() can read from a URL just as easily as from a local file path.\nDATASET_URL = \"https://raw.githubusercontent.com/timothy-watt/python-for-ai-ml/main/data/so_survey_2025_curated.csv\"\n\n# pd.read_csv() reads a comma-separated values file and returns a DataFrame â€”\n# the core data structure in Pandas. Think of it as a supercharged spreadsheet\n# that lives in memory and has hundreds of built-in analysis methods.\nprint(\"Loading SO 2025 Developer Survey dataset...\")\ndf = pd.read_csv(DATASET_URL)\n\n# Confirm the load succeeded by printing the shape.\n# df.shape returns a tuple (number_of_rows, number_of_columns).\nrows, cols = df.shape  # unpack the tuple into two variables\nprint(f\"âœ… Dataset loaded successfully!\")\nprint(f\"   Shape: {rows:,} rows Ã— {cols} columns\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Previewing the First Few Rows\n\n`df.head()` returns the first 5 rows of the DataFrame by default.  \nThis is always the first thing you do after loading a dataset â€” a quick sanity check  \nthat the data looks like what you expected.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# df.head(n) returns the first n rows of the DataFrame.\n# With no argument, it defaults to 5 rows.\n# The output shows column names across the top and row index numbers on the left.\n\ndf.head(10)  # show first 10 rows for a broader preview\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Understanding the Structure with `.info()`\n\n`df.info()` prints a concise summary of the DataFrame â€” column names, data types,  \nand the count of non-null values per column. This is how you spot missing data at a glance.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# df.info() prints:\n#   - Total row count\n#   - Each column's name, non-null count, and data type (dtype)\n#   - Total memory usage\n#\n# Key dtypes you'll see:\n#   object  â†’ string/text data (Pandas uses 'object' for strings)\n#   int64   â†’ integer numbers (64-bit)\n#   float64 â†’ decimal numbers (64-bit floating point)\n#\n# A column where non-null count < total rows has MISSING values â€”\n# we'll handle those in Chapter 3.\n\nprint(\"DataFrame Info:\")\nprint(\"=\" * 60)\ndf.info()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Statistical Summary with `.describe()`\n\n`df.describe()` generates descriptive statistics for all **numeric** columns â€”  \ncount, mean, standard deviation, min, quartiles, and max.  \nThis gives you an immediate feel for the scale and distribution of your data.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# df.describe() computes summary statistics for numeric columns only by default.\n# We use .round(2) to limit decimal places for readability.\n#\n# What to look for:\n#   - 'count' less than total rows â†’ missing values in that column\n#   - Very large 'max' vs 'mean' â†’ potential outliers (salary data often has these)\n#   - 'std' (standard deviation) â†’ how spread out the values are\n\nprint(\"Descriptive Statistics for Numeric Columns:\")\nprint(\"=\" * 60)\ndf.describe().round(2)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Key Columns Used Throughout the Book\n\nThe table below is your reference guide for the entire book.  \nWhen a chapter says \"we use `ConvertedCompYearly` as the target variable,\" you'll know exactly what that means.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Let's look at each key column with a sample of its values.\n# This gives more intuition than a table â€” you see real data, not just descriptions.\n\n# Define the key columns we'll use across all chapters\nkey_columns = [\n    'ConvertedCompYearly',     # Annual salary in USD (our regression target)\n    'DevType',                 # Developer role(s) â€” often multi-value, semicolon-separated\n    'LanguageHaveWorkedWith',  # Languages used â€” multi-value, semicolon-separated\n    'YearsCodePro',            # Years of professional coding experience\n    'Country',                 # Respondent's country\n    'EdLevel',                 # Highest education level completed\n    'Employment',              # Employment status\n    'RemoteWork',              # Remote / hybrid / in-person\n    'AIToolCurrently',         # AI tools currently being used â€” multi-value\n    'AITrustTeammates',        # How much they trust AI tool recommendations\n    'OrgSize',                 # Organization size\n    'Age',                     # Age range (binned, not exact)\n    'Gender',                  # Gender identity (used in Chapter 9 bias audit)\n]\n\n# Only show columns that actually exist in our curated subset\n# (the curated version may not include all columns from the full survey)\navailable_cols = [c for c in key_columns if c in df.columns]\nmissing_cols   = [c for c in key_columns if c not in df.columns]\n\nprint(f\"Key columns available in this dataset: {len(available_cols)}/{len(key_columns)}\")\nif missing_cols:\n    print(f\"Not in curated subset (available in full dataset): {missing_cols}\")\n\nprint()\nprint(\"Sample values from each key column:\")\nprint(\"=\" * 60)\n\n# Loop through each available column and show a few unique values\nfor col in available_cols:\n    # .dropna() removes NaN (missing) values before sampling\n    # .unique() returns all distinct values in the column\n    # [:4] takes the first 4 unique values as examples\n    sample_values = df[col].dropna().unique()[:4]\n    print(f\"  {col:<30} â†’ {list(sample_values)}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Quick Missing Value Check\n\nReal-world datasets always have missing values. The SO 2025 survey is no exception â€”  \nnot every respondent answered every question. Let's see the scale of what we'll be  \ncleaning in Chapter 3.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# df.isnull() returns a DataFrame of True/False values â€” True where data is missing.\n# .sum() counts the True values per column (True = 1, False = 0 in Python).\n# .sort_values(ascending=False) sorts from most missing to least.\n\nmissing_counts = df.isnull().sum().sort_values(ascending=False)\n\n# Calculate the percentage of missing values per column\n# This is more meaningful than raw counts since columns can have different totals\nmissing_pct = (missing_counts / len(df) * 100).round(1)\n\n# Combine into a summary DataFrame for clean display\nmissing_summary = pd.DataFrame({\n    'Missing Count': missing_counts,\n    'Missing %': missing_pct\n})\n\n# Only show columns that actually have at least one missing value\nmissing_summary = missing_summary[missing_summary['Missing Count'] > 0]\n\nprint(f\"Columns with missing values: {len(missing_summary)} of {len(df.columns)}\")\nprint()\nprint(missing_summary.to_string())   # .to_string() prints full table without truncation\nprint()\nprint(\"â†’ We'll handle all of these systematically in Chapter 3.\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### A First Look at Developer Salaries\n\nOur primary regression target throughout the book is `ConvertedCompYearly` â€” annual salary in USD,  \nconverted from local currencies using Stack Overflow's exchange rates.  \nLet's get an early feel for what this data looks like.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# We import matplotlib here just for this quick preview chart.\n# Chapter 4 covers visualization in full depth â€” this is just a taste.\nimport matplotlib.pyplot as plt   # pyplot is matplotlib's main plotting interface\nimport numpy as np\n\n# Filter to reasonable salary range to remove extreme outliers for this preview.\n# We'll do this more rigorously in Chapter 3 â€” here we just want a clean visual.\n# $10,000 minimum removes implausibly low values (data entry errors or part-time)\n# $500,000 maximum removes the extreme high-end outliers that skew the view\nsalary_col = 'ConvertedCompYearly'\n\nif salary_col in df.columns:\n    salary_data = df[salary_col].dropna()                          # remove missing values\n    salary_clean = salary_data[(salary_data >= 10_000) &          # above $10k/year\n                               (salary_data <= 500_000)]           # below $500k/year\n\n    # Create a figure with one plot area (axes)\n    fig, ax = plt.subplots(figsize=(10, 4))   # figsize sets width Ã— height in inches\n\n    # ax.hist() draws a histogram â€” it groups values into 'bins' and counts\n    # how many values fall into each group, showing the distribution shape.\n    ax.hist(salary_clean, bins=60, color='#2E75B6', edgecolor='white', linewidth=0.5)\n\n    # Labels and title make the chart self-explanatory\n    ax.set_xlabel('Annual Salary (USD)', fontsize=12)\n    ax.set_ylabel('Number of Respondents', fontsize=12)\n    ax.set_title('SO 2025: Developer Salary Distribution (Respondents with $10k-$500k reported salary)', fontsize=13, fontweight='bold')\n\n    # Format x-axis labels as currency with thousands separator (e.g., $100,000)\n    ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'${x:,.0f}'))\n\n    # Add summary statistics as text annotations on the chart\n    median_sal = salary_clean.median()\n    mean_sal = salary_clean.mean()\n    ax.axvline(median_sal, color='#E8722A', linestyle='--', linewidth=1.5,\n               label=f'Median: ${median_sal:,.0f}')\n    ax.axvline(mean_sal, color='green', linestyle=':', linewidth=1.5,\n               label=f'Mean: ${mean_sal:,.0f}')\n    ax.legend(fontsize=10)\n\n    plt.tight_layout()   # automatically adjust spacing so nothing overlaps\n    plt.show()\n\n    # Print summary stats alongside the chart\n    print(f\"Salary Summary (n={len(salary_clean):,} respondents)\")\n    print(f\"  Median: ${median_sal:>10,.0f}\")\n    print(f\"  Mean:   ${mean_sal:>10,.0f}\")\n    print(f\"  Min:    ${salary_clean.min():>10,.0f}\")\n    print(f\"  Max:    ${salary_clean.max():>10,.0f}\")\n    print(f\"  Std:    ${salary_clean.std():>10,.0f}\")\nelse:\n    print(f\"Column '{salary_col}' not found in dataset.\")\n    print(f\"Available columns: {list(df.columns)}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### A First Look at Programming Language Adoption\n\nThe `LanguageHaveWorkedWith` column uses a **multi-value format** â€” each respondent can list  \nmultiple languages, separated by semicolons. For example: `\"Python;JavaScript;SQL;Rust\"`.  \n\nThis is one of the most common data formats you'll encounter in real surveys, and cleaning it  \n(splitting and exploding into separate rows) is a key skill we'll practice in Chapter 3.  \nHere's a quick preview of what Python's adoption looks like in the 2025 data.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "lang_col = 'LanguageHaveWorkedWith'\n\nif lang_col in df.columns:\n    # Each row contains multiple languages as a single semicolon-separated string.\n    # Example row value: \"Python;JavaScript;SQL;Bash/Shell\"\n    #\n    # Step 1: Drop rows where this column is missing (NaN)\n    lang_series = df[lang_col].dropna()\n\n    # Step 2: Split each string on the ';' separator.\n    # str.split(';') vectorizes the Python split() method across every row.\n    # expand=False returns a Series of lists.\n    lang_lists = lang_series.str.split(';')\n\n    # Step 3: .explode() converts each list element into its own row,\n    # essentially \"unpacking\" the lists so each language appears separately.\n    # This transforms the data from wide format to long format.\n    lang_exploded = lang_lists.explode()\n\n    # Step 4: Count how many respondents used each language.\n    # .value_counts() counts occurrences, sorted from most to least common.\n    lang_counts = lang_exploded.value_counts()\n\n    # Show the top 15 languages\n    top_15 = lang_counts.head(15)\n\n    fig, ax = plt.subplots(figsize=(10, 5))\n\n    # Horizontal bar chart â€” easier to read long language names than vertical bars\n    bars = ax.barh(top_15.index[::-1],     # reverse so most popular is at top\n                   top_15.values[::-1],\n                   color='#2E75B6',\n                   edgecolor='white')\n\n    ax.set_xlabel('Number of Respondents', fontsize=11)\n    ax.set_title('SO 2025: Top 15 Programming Languages (by number of respondents who have worked with them)', fontsize=12, fontweight='bold')\n\n    # Add value labels at the end of each bar for exact counts\n    for bar, val in zip(bars, top_15.values[::-1]):\n        ax.text(val + 50, bar.get_y() + bar.get_height()/2,\n                f'{val:,}', va='center', ha='left', fontsize=9)\n\n    plt.tight_layout()\n    plt.show()\n\n    # Find Python's exact ranking\n    python_count = lang_counts.get('Python', 0)\n    python_rank  = list(lang_counts.index).index('Python') + 1 if 'Python' in lang_counts.index else 'N/A'\n    total_resp   = len(lang_series)\n\n    print(f\"Total respondents who answered this question: {total_resp:,}\")\n    print(f\"Python: {python_count:,} respondents ({python_count/total_resp*100:.1f}%) â€” Rank #{python_rank}\")\n    print(\"\")\n    print(\"Top 5 languages:\")\n    for rank, (lang, count) in enumerate(top_15.head(5).items(), 1):\n        print(f\"  #{rank}: {lang:<25} {count:,} respondents ({count/total_resp*100:.1f}%)\")\nelse:\n    print(f\"Column '{lang_col}' not found in dataset.\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### A Glimpse at AI Tool Adoption in 2025\n\nThe 2025 survey included substantial new coverage of AI tools â€” what developers are using,  \nhow much they trust AI recommendations, and their overall feelings about AI in their workflow.  \nThese columns power Chapter 8's NLP and sentiment analysis tasks.\n\nLet's see which AI tools developers are using right now.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "ai_col = 'AIToolCurrently'\n\nif ai_col in df.columns:\n    # Same multi-value splitting pattern as the language column above.\n    # This pattern is so common in survey data that by Chapter 3,\n    # you'll write it from memory without looking it up.\n    ai_series   = df[ai_col].dropna()\n    ai_exploded = ai_series.str.split(';').explode().str.strip()  # .str.strip() removes whitespace\n    ai_counts   = ai_exploded.value_counts().head(12)\n\n    fig, ax = plt.subplots(figsize=(10, 5))\n    colors = ['#E8722A' if 'ChatGPT' in tool or 'Copilot' in tool or 'Claude' in tool\n              else '#2E75B6' for tool in ai_counts.index[::-1]]\n\n    ax.barh(ai_counts.index[::-1], ai_counts.values[::-1], color=colors[::-1], edgecolor='white')\n    ax.set_xlabel('Number of Respondents', fontsize=11)\n    ax.set_title('SO 2025: AI Tools Developers Are Currently Using', fontsize=12, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n\n    total_ai = len(ai_series)\n    total_respondents = len(df)\n    print(f\"Respondents using at least one AI tool: {total_ai:,} of {total_respondents:,} ({total_ai/total_respondents*100:.1f}%)\")\n    print(\"\")\n    print(\"Top 5 AI tools in 2025:\")\n    for rank, (tool, count) in enumerate(ai_counts.head(5).items(), 1):\n        print(f\"  #{rank}: {tool:<35} {count:,} respondents\")\nelse:\n    print(f\"Column '{ai_col}' not found. Available columns: {[c for c in df.columns if 'AI' in c or 'ai' in c.lower()]}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 0.5b â€” Saving the Dataset to Google Drive (Optional)\n\nIf you'd prefer to load the dataset from Drive instead of the GitHub URL in future sessions,  \nrun the cell below. This is optional â€” all notebooks default to the GitHub URL which works without Drive.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# OPTIONAL: Save a local copy of the dataset to Google Drive.\n#\n# Skip this cell entirely if:\n#   - You have not mounted Drive (cell above)\n#   - You prefer loading from GitHub each session (perfectly fine)\n#\n# Run this cell individually (â–¶ button) after mounting Drive if you\n# want a persistent local copy for faster loading or offline access.\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nimport os\n\ndrive_root = '/content/drive/MyDrive'\n\nif not os.path.exists(drive_root):\n    # Drive is not mounted â€” skip silently rather than raising an error\n    print(\"â„¹ï¸  Skipping Drive save â€” Google Drive is not mounted.\")\n    print(\"   The dataset loads fine from GitHub. No action needed.\")\nelse:\n    # Drive IS mounted â€” proceed with saving\n    drive_data_path = '/content/drive/MyDrive/python_for_aiml/data'\n\n    # os.makedirs() creates the folder and any missing parent folders.\n    # exist_ok=True means no error if the folder already exists.\n    os.makedirs(drive_data_path, exist_ok=True)\n\n    save_path = f'{drive_data_path}/so_survey_2025_curated.csv'\n\n    # df.to_csv() writes the DataFrame to a CSV file on Drive.\n    # index=False omits the auto-generated row numbers from the output.\n    df.to_csv(save_path, index=False)\n\n    file_size_mb = os.path.getsize(save_path) / 1_000_000\n    print(f\"âœ… Dataset saved to Drive: {save_path}\")\n    print(f\"   File size: {file_size_mb:.1f} MB\")\n    print()\n    print(\"To load from Drive in future sessions instead of GitHub:\")\n    print(f'  df = pd.read_csv(\"{save_path}\")')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 0.6 â€” Local Python Setup (Optional Sidebar)\n\n> âš ï¸ **This section is completely optional.** The entire book works in Google Colab with no local setup.  \n> Only read this if you want a local Python environment on your own machine.\n\n---\n\n### Why Consider Local Setup?\n\n| Situation | Recommendation |\n|---|---|\n| Following this book | **Colab** â€” no setup, free GPU, works everywhere |\n| Building production projects | **Local environment** â€” full control, no session timeouts |\n| Contributing to open source | **Local environment** â€” version control integration |\n| Working with sensitive data | **Local environment** â€” data never leaves your machine |\n\n---\n\n### Option A: Anaconda (Recommended for Beginners)\n\nAnaconda is an all-in-one Python distribution that includes Python, conda (environment manager), and 250+ pre-installed data science packages.\n\n**Installation steps:**\n1. Go to [anaconda.com/download](https://www.anaconda.com/download)\n2. Download the installer for your OS (Windows / macOS / Linux)\n3. Run the installer â€” accept defaults for most options\n4. Open **Anaconda Navigator** or the **Anaconda Prompt**\n5. Verify: `python --version` should show Python 3.11+\n\n**Creating a dedicated environment for this book:**\n```bash\n# Create a new environment named 'pyaiml' with Python 3.11\nconda create -n pyaiml python=3.11\n\n# Activate the environment\nconda activate pyaiml\n\n# Install core packages\npip install numpy pandas matplotlib seaborn scikit-learn scipy\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\npip install keras transformers datasets plotly xgboost lightgbm optuna\npip install jupyter notebook fastapi uvicorn\n\n# Launch Jupyter Notebook\njupyter notebook\n```\n\n---\n\n### Option B: VS Code + Python Extension\n\nVS Code with the Python extension is the most popular IDE for Python development.\n\n1. Download VS Code from [code.visualstudio.com](https://code.visualstudio.com)\n2. Install the **Python extension** (by Microsoft) from the Extensions marketplace\n3. Install the **Jupyter extension** to run `.ipynb` notebooks locally\n4. Select your conda environment (or any Python interpreter) via `Ctrl+Shift+P â†’ Python: Select Interpreter`\n\n---\n\n### Virtual Environments (venv â€” built into Python)\n\nIf you prefer not to use Anaconda, Python's built-in `venv` module creates lightweight isolated environments:\n\n```bash\n# Create a virtual environment in a folder named '.venv'\npython -m venv .venv\n\n# Activate it (macOS/Linux)\nsource .venv/bin/activate\n\n# Activate it (Windows)\n.venv\\Scripts\\activate\n\n# Install packages\npip install -r requirements.txt   # if the repo includes one\n```\n\n> ðŸ“Œ **Key principle:** Always use a dedicated environment per project. Never install packages into your system Python.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Concept Check Questions\n\n> Test your understanding before moving on. Answer each question without referring back to the notebook, then expand to check.\n\n**Q1.** What is the difference between a Colab **Runtime** and a Colab **Notebook**?\n\n<details><summary>Show answer</summary>\n\nA **notebook** is the file (`.ipynb`) that stores your code and text cells. A **runtime** is the virtual machine that actually executes that code. You can restart or reset the runtime without losing the notebook file.\n\n</details>\n\n**Q2.** Why does `import pandas` fail immediately after you restart the Colab runtime, even though you ran `!pip install pandas` earlier?\n\n<details><summary>Show answer</summary>\n\n`!pip install` installs a package into the runtime's Python environment. When you restart the runtime, a fresh environment starts â€” the package is still installed (Colab caches it), but all Python variables and imports are cleared. You need to re-run the `import` cell.\n\n</details>\n\n**Q3.** What is the purpose of mounting Google Drive in Colab?\n\n<details><summary>Show answer</summary>\n\nBy default, files written in a Colab session are stored in the runtime's ephemeral `/content/` directory and are lost when the runtime ends. Mounting Drive maps your Drive folder to `/content/drive/`, so files written there persist between sessions.\n\n</details>\n\n**Q4.** Name two differences between a T4 GPU runtime and a CPU runtime in Colab.\n\n<details><summary>Show answer</summary>\n\n1. **Speed:** GPU runtimes use NVIDIA T4 GPUs which can be 10â€“50x faster for deep learning training. 2. **Memory:** T4 GPUs have 16 GB of dedicated VRAM separate from system RAM. GPU runtimes are also time-limited (~12 hrs continuous) and may queue during high demand.\n\n</details>\n\n**Q5.** You open a classmate's notebook and the first cell is `df = pd.read_csv('data.csv')`. The file is nowhere in the repo. What are two ways to fix this?\n\n<details><summary>Show answer</summary>\n\n1. **Host the file publicly** (e.g., in GitHub or Google Drive) and replace the path with a URL: `pd.read_csv('https://...')`. 2. **Mount Drive** and store `data.csv` there, then update the path to `'/content/drive/MyDrive/data.csv'`.\n\n</details>\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## âœ… Chapter 0 Summary\n\nExcellent work â€” you've completed the orientation and you're ready to start writing Python.\n\n### What You Did in This Chapter\n\n| Task | Status |\n|---|---|\n| Navigated the Google Colab interface | âœ… |\n| Ran your first Python code (`print()`) | âœ… |\n| Mounted Google Drive for persistent storage | âœ… |\n| Checked GPU availability | âœ… |\n| Installed a package with `!pip install` | âœ… |\n| Loaded the SO 2025 dataset with `pd.read_csv()` | âœ… |\n| Previewed data with `.head()`, `.info()`, `.describe()` | âœ… |\n| Identified key columns and missing values | âœ… |\n| Visualized salary distribution and language adoption | âœ… |\n\n### Key Takeaways\n\n- **Google Colab** is a free, zero-setup Jupyter environment with free GPU access\n- **Notebooks** mix markdown (explanatory text) and code cells â€” this mirrors real data science work\n- The **Stack Overflow 2025 Developer Survey** is our project dataset for all 9 chapters â€” 15,000 developers, salary, languages, AI tool adoption, and more\n- `pd.read_csv(URL)` loads data from any web URL â€” no download required\n- `.head()`, `.info()`, `.describe()` are always your first three moves on a new dataset\n- The `LanguageHaveWorkedWith` column uses semicolon-separated multi-values â€” a real-world data format we'll clean properly in Chapter 3\n\n### Skills Gained\n\n- Colab interface navigation and keyboard shortcuts\n- Google Drive mounting\n- GPU runtime configuration\n- Package installation with `!pip install`\n- Dataset loading with Pandas\n- Initial data exploration and visualization\n\n---\n\n### â­ï¸ What's Next: Chapter 1 â€” Python Fundamentals\n\nChapter 1 begins the real Python journey. We'll cover:\n- Variables, data types (int, float, str, bool)\n- Operators and expressions\n- Control flow (if/elif/else, for loops, while loops)\n- Core data structures (lists, tuples, sets, dictionaries)\n- Functions â€” the building block of all Python programs\n\nBy the end of Chapter 1, you'll be writing functions that summarize SO 2025 salary and language data â€” using only pure Python, no external libraries.\n\n---\n\n*End of Chapter 0 Â· Python for AI/ML*  \n[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)]({TOC_URL})\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Coding Exercises\n\n> Three exercises per chapter: **ðŸ”§ Guided** (fill-in-the-blanks) Â· **ðŸ”¨ Applied** (write from scratch) Â· **ðŸ—ï¸ Extension** (go beyond the chapter)\n\nExercises use the SO 2025 developer survey dataset.\nExpand each **Solution** block only after attempting the exercise.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exercise 1 ðŸ”§ Guided â€” Colab environment audit\n\nComplete the `env_audit()` function that returns a dict summarising:\nPython version, GPU availability, available RAM (GB), and whether\n`numpy`, `pandas`, and `torch` can be imported.\nHint: use `sys`, `platform`, `psutil`, and `importlib`.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import sys\nimport platform\n\ndef env_audit() -> dict:\n    \"\"\"Return dict with keys: python_version, gpu_available, ram_gb, packages.\"\"\"\n    # YOUR CODE HERE\n    pass\n\nresult = env_audit()\nprint(result)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<details><summary>ðŸ’¡ Hint</summary>\n\nUse `sys.version` for Python version. For GPU: `torch.cuda.is_available()`.\nFor RAM: `psutil.virtual_memory().total / (1024**3)`. For packages: use a try/except import loop.\n\n</details>\n\n<details><summary>âœ… Solution</summary>\n\n```python\nimport sys, importlib\ntry:\n    import psutil\nexcept ImportError:\n    psutil = None\n\ndef env_audit() -> dict:\n    packages = {}\n    for pkg in ['numpy','pandas','torch','sklearn']:\n        packages[pkg] = importlib.util.find_spec(pkg) is not None\n    try:\n        import torch\n        gpu = torch.cuda.is_available()\n    except ImportError:\n        gpu = False\n    ram = psutil.virtual_memory().total / (1024**3) if psutil else None\n    return {'python_version': sys.version.split()[0],\n            'gpu_available': gpu, 'ram_gb': round(ram, 1) if ram else 'unknown',\n            'packages': packages}\nprint(env_audit())\n```\n\n</details>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exercise 2 ðŸ”¨ Applied â€” Drive-safe data loader\n\nWrite a `load_survey_data(path: str) -> pd.DataFrame` function that:\n1. Attempts to load a CSV from the given path\n2. If the file is not found, falls back to generating 1,000 synthetic rows    with columns `YearsCodePro`, `ConvertedCompYearly`, `uses_python` (0/1)\n3. Always prints whether it loaded real or synthetic data\n4. Returns the DataFrame\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import pandas as pd\nimport numpy as np\n\ndef load_survey_data(path: str) -> pd.DataFrame:\n    # YOUR CODE HERE\n    pass\n\ndf = load_survey_data('/content/drive/MyDrive/so_survey_2025.csv')\nprint(df.shape)\nprint(df.head(3))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<details><summary>ðŸ’¡ Hint</summary>\n\nUse a `try/except FileNotFoundError` block. For synthetic data, `np.random.exponential(6, 1000)` works well for `YearsCodePro`.\n\n</details>\n\n<details><summary>âœ… Solution</summary>\n\n```python\ndef load_survey_data(path: str) -> pd.DataFrame:\n    try:\n        df = pd.read_csv(path)\n        print(f'Loaded real data: {len(df):,} rows')\n        return df\n    except FileNotFoundError:\n        rng = np.random.default_rng(42)\n        n = 1000\n        df = pd.DataFrame({\n            'YearsCodePro':        rng.exponential(6, n).clip(0, 35),\n            'ConvertedCompYearly': np.exp(10.8 + rng.normal(0, 0.5, n)),\n            'uses_python':         rng.integers(0, 2, n),\n        })\n        print(f'File not found â€” using {n} synthetic rows')\n        return df\n```\n\n</details>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exercise 3 ðŸ—ï¸ Extension â€” Reproducibility checker\n\nWrite a `check_reproducibility(n_runs: int = 5) -> bool` function that:\n1. Trains a `GradientBoostingClassifier` on a fixed synthetic dataset\n   `n_runs` times with `random_state=42`\n2. Asserts that all runs produce *identical* predictions on a held-out set\n3. Returns `True` if reproducible, `False` otherwise\n4. Then runs it again WITHOUT `random_state` and reports whether results vary.\nThis tests your understanding of why `random_state` matters.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\ndef check_reproducibility(n_runs: int = 5, use_seed: bool = True) -> bool:\n    # YOUR CODE HERE\n    pass\n\nprint('With random_state=42:', check_reproducibility(use_seed=True))\nprint('Without random_state: ', check_reproducibility(use_seed=False))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<details><summary>ðŸ’¡ Hint</summary>\n\nStore predictions from each run in a list. Use `all(np.array_equal(runs[0], r) for r in runs[1:])` to check identity.\n\n</details>\n\n<details><summary>âœ… Solution</summary>\n\n```python\nimport numpy as np\ndef check_reproducibility(n_runs=5, use_seed=True):\n    X, y = make_classification(n_samples=1000, random_state=42)\n    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n    all_preds = []\n    for _ in range(n_runs):\n        seed = 42 if use_seed else None\n        clf = GradientBoostingClassifier(n_estimators=50, random_state=seed)\n        clf.fit(X_tr, y_tr)\n        all_preds.append(clf.predict(X_te))\n    reproducible = all(np.array_equal(all_preds[0], p) for p in all_preds[1:])\n    return reproducible\n```\n\n</details>\n\n---\n\n*End of Chapter 0 â€” Python for AI/ML*  \n[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n"
    }
  ]
}