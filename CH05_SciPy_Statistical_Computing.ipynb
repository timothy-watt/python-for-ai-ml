{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Chapter 5 -- SciPy and Statistical Computing\n## *Python for AI/ML: A Complete Learning Journey*\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/CH05_SciPy_Statistical_Computing.ipynb)\n&nbsp;&nbsp;[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n\n---\n\n**Part:** 3 -- Machine Learning and AI  \n**Prerequisites:** Chapter 4 (Data Visualisation)  \n**Estimated time:** 3-4 hours\n\n---\n\n### Learning Objectives\n\nBy the end of this chapter you will be able to:\n\n- Understand why statistical grounding matters before building ML models\n- Use `scipy.stats` to describe distributions and test hypotheses\n- Run t-tests, Mann-Whitney U tests, and chi-squared tests on real data\n- Interpret p-values correctly -- and understand their limitations\n- Use `scipy.optimize` for curve fitting and root finding\n- Apply `scipy.interpolate` for filling gaps in time-series data\n- Use NumPy structured arrays for mixed-type tabular data\n\n---\n\n### Why Statistics Before Machine Learning?\n\nMany practitioners skip from data cleaning straight to model training.\nThis leads to models that are technically running but statistically meaningless --\ntrained on features that have no real relationship with the target,\nor evaluated on metrics that mask fundamental data problems.\n\nThis chapter gives you the tools to ask: *Is this relationship real, or just noise?*\nThat question should precede every modelling decision.\n\n---\n\n### Project Thread -- Chapter 5\n\nWe run formal hypothesis tests on the SO 2025 dataset:\n- Is the salary difference between Python and non-Python developers statistically significant?\n- Does education level have a measurable effect on salary?\n- Is there a significant salary gap between remote and on-site workers?\n\nThe answers inform which features are worth including in the Chapter 6 ML pipeline.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Setup -- Imports and Data\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy import optimize\nfrom scipy import interpolate\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport scipy\nprint(f'SciPy:      {scipy.__version__}')\nprint(f'NumPy:      {np.__version__}')\nprint(f'Pandas:     {pd.__version__}')\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.dpi']     = 110\nplt.rcParams['axes.titlesize'] = 13\nplt.rcParams['axes.titleweight'] = 'bold'\n\nDATASET_URL = 'https://raw.githubusercontent.com/timothy-watt/python-for-ai-ml/main/data/so_survey_2025_curated.csv'\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Load and clean SO 2025 -- same pipeline as Chapters 3 and 4\ndf_raw = pd.read_csv(DATASET_URL)\ndf = df_raw.copy()\ndf = df.dropna(subset=['ConvertedCompYearly'])\ndf['ConvertedCompYearly'] = pd.to_numeric(df['ConvertedCompYearly'], errors='coerce')\nQ1, Q3 = df['ConvertedCompYearly'].quantile([0.25, 0.75])\nIQR = Q3 - Q1\ndf = df[\n    (df['ConvertedCompYearly'] >= max(Q1 - 3*IQR, 5_000)) &\n    (df['ConvertedCompYearly'] <= min(Q3 + 3*IQR, 600_000))\n].copy()\nif 'YearsCodePro' in df.columns:\n    df['YearsCodePro'] = pd.to_numeric(df['YearsCodePro'], errors='coerce')\n    df['YearsCodePro'] = df['YearsCodePro'].fillna(df['YearsCodePro'].median())\nfor col in ['Country', 'DevType', 'EdLevel', 'Employment', 'RemoteWork']:\n    if col in df.columns:\n        df[col] = df[col].fillna('Unknown')\ndf['uses_python'] = df.get('LanguageHaveWorkedWith', pd.Series(dtype=str)).str.contains('Python', na=False)\ndf = df.reset_index(drop=True)\nprint(f'Dataset ready: {len(df):,} rows')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 5.1 -- Describing Distributions with scipy.stats\n\nBefore testing hypotheses, you need to understand the shape of your data.\n`scipy.stats` provides distribution fitting, descriptive statistics beyond\nwhat Pandas offers, and the statistical tests that follow.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 5.1.1 -- Descriptive statistics and distribution shape\n\nsal = df['ConvertedCompYearly'].dropna()\n\n# scipy.stats.describe gives a comprehensive summary in one call\ndesc = stats.describe(sal)\nprint('scipy.stats.describe() output:')\nprint(f'  n:          {desc.nobs:,}')\nprint(f'  min/max:    ${desc.minmax[0]:,.0f} / ${desc.minmax[1]:,.0f}')\nprint(f'  mean:       ${desc.mean:,.0f}')\nprint(f'  variance:   {desc.variance:,.0f}')\nprint(f'  skewness:   {desc.skewness:.4f}')\nprint(f'  kurtosis:   {desc.kurtosis:.4f}')\n\n# Interpreting skewness and kurtosis\nprint()\nprint('Interpretation:')\nif desc.skewness > 0.5:\n    print(f'  Skewness {desc.skewness:.2f} > 0.5 -> right-skewed (long tail of high earners)')\n    print('  -> Consider log-transforming salary before regression modelling')\nif desc.kurtosis > 1:\n    print(f'  Kurtosis {desc.kurtosis:.2f} > 1 -> heavier tails than normal distribution')\n    print('  -> More extreme outliers than a normal distribution would predict')\n\n# Log-transform and compare skewness\nlog_sal = np.log(sal)\nlog_skew = stats.skew(log_sal)\nprint(f'  Original skewness:      {stats.skew(sal):.4f}')\nprint(f'  Log-transform skewness: {log_skew:.4f}  (closer to 0 = more symmetric)')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 5.1.2 -- Visualising the distribution and testing for normality\n\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\nsal = df['ConvertedCompYearly'].dropna()\nlog_sal = np.log(sal)\n\n# Left: raw salary histogram with normal fit overlay\naxes[0].hist(sal, bins=60, density=True, color='#2E75B6', alpha=0.7, edgecolor='white')\nx = np.linspace(sal.min(), sal.max(), 300)\nmu, sigma = stats.norm.fit(sal)\naxes[0].plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2, label='Normal fit')\naxes[0].xaxis.set_major_formatter(mticker.FuncFormatter(lambda v, _: f'${v/1000:.0f}k'))\naxes[0].set_title('Raw Salary vs Normal Fit')\naxes[0].set_xlabel('Salary')\naxes[0].set_ylabel('Density')\naxes[0].legend()\n\n# Middle: log-salary histogram with normal fit overlay\naxes[1].hist(log_sal, bins=60, density=True, color='#2E75B6', alpha=0.7, edgecolor='white')\nlmu, lsigma = stats.norm.fit(log_sal)\nlx = np.linspace(log_sal.min(), log_sal.max(), 300)\naxes[1].plot(lx, stats.norm.pdf(lx, lmu, lsigma), 'r-', linewidth=2, label='Normal fit')\naxes[1].set_title('Log(Salary) vs Normal Fit')\naxes[1].set_xlabel('log(Salary)')\naxes[1].set_ylabel('Density')\naxes[1].legend()\n\n# Right: Q-Q plot -- compares quantiles of data vs theoretical normal\n# Points on the diagonal line = data follows a normal distribution\n# Deviations at the tails = heavier or lighter tails than normal\nsample_log = log_sal.sample(n=min(2000, len(log_sal)), random_state=42)\n(osm, osr), (slope, intercept, r) = stats.probplot(sample_log, dist='norm')\naxes[2].scatter(osm, osr, alpha=0.3, s=8, color='#2E75B6')\naxes[2].plot([osm[0], osm[-1]],\n             [slope*osm[0]+intercept, slope*osm[-1]+intercept],\n             'r-', linewidth=2)\naxes[2].set_title('Q-Q Plot: log(Salary) vs Normal')\naxes[2].set_xlabel('Theoretical Quantiles')\naxes[2].set_ylabel('Sample Quantiles')\n\nplt.suptitle('SO 2025: Salary Distribution Analysis', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Shapiro-Wilk normality test on a sample (test requires n < 5000)\nsample = sal.sample(n=500, random_state=42)\nstat, p = stats.shapiro(sample)\nprint(f'Shapiro-Wilk test on raw salary (n=500):  W={stat:.4f}, p={p:.2e}')\nstat_log, p_log = stats.shapiro(np.log(sample))\nprint(f'Shapiro-Wilk test on log salary  (n=500): W={stat_log:.4f}, p={p_log:.2e}')\nprint()\nprint('Interpretation:')\nprint('  p < 0.05 -> reject the null hypothesis that the data is normally distributed')\nprint('  Even log-salary is not perfectly normal at this sample size -- this is common.')\nprint('  For large n, use non-parametric tests (Mann-Whitney U) as a safer alternative.')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 5.2 -- Hypothesis Testing\n\nA hypothesis test asks: *Could this observed difference have occurred by chance?*\n\nThe framework:\n1. **H0 (null hypothesis):** No real difference -- any observed gap is due to random sampling\n2. **H1 (alternative hypothesis):** A real difference exists\n3. **p-value:** Probability of observing data this extreme *if H0 were true*\n4. **Significance level (alpha):** Threshold below which we reject H0. Convention: 0.05\n\n**Critical p-value warning:** p < 0.05 does NOT mean the effect is large or practically\nimportant. With 15,000 respondents, tiny meaningless differences can be statistically\nsignificant. Always report **effect size** alongside the p-value.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 5.2.1 -- Two-sample t-test: Python vs non-Python salary\n#\n# Question: Is the mean salary of Python developers different from non-Python developers?\n# H0: mu_python == mu_non_python  (no difference)\n# H1: mu_python != mu_non_python  (two-tailed: we do not assume direction)\n\nif 'uses_python' in df.columns:\n    python_sal  = df[df['uses_python']]['ConvertedCompYearly'].dropna()\n    nopython_sal = df[~df['uses_python']]['ConvertedCompYearly'].dropna()\n\n    print(f'Python developers:     n={len(python_sal):,}, '\n          f'mean=${python_sal.mean():,.0f}, median=${python_sal.median():,.0f}')\n    print(f'Non-Python developers: n={len(nopython_sal):,}, '\n          f'mean=${nopython_sal.mean():,.0f}, median=${nopython_sal.median():,.0f}')\n    print(f'Observed difference:   ${python_sal.mean() - nopython_sal.mean():+,.0f} (mean)')\n\n    # Independent samples t-test\n    # equal_var=False -> Welch's t-test, does not assume equal variances\n    # This is the safer default for real-world data\n    t_stat, p_val = stats.ttest_ind(python_sal, nopython_sal, equal_var=False)\n\n    print()\n    print('Welch t-test result:')\n    print(f'  t-statistic: {t_stat:.4f}')\n    print(f'  p-value:     {p_val:.4e}')\n    print(f'  Significant at alpha=0.05: {p_val < 0.05}')\n\n    # Cohen's d -- effect size (how large is the difference relative to spread?)\n    # < 0.2: negligible, 0.2-0.5: small, 0.5-0.8: medium, > 0.8: large\n    pooled_std = np.sqrt(\n        (python_sal.std()**2 + nopython_sal.std()**2) / 2\n    )\n    cohens_d = (python_sal.mean() - nopython_sal.mean()) / pooled_std\n    print(f\"  Cohen's d:   {cohens_d:.4f}  \", end='')\n    if abs(cohens_d) < 0.2:   print('(negligible effect size)')\n    elif abs(cohens_d) < 0.5: print('(small effect size)')\n    elif abs(cohens_d) < 0.8: print('(medium effect size)')\n    else:                      print('(large effect size)')\n\n    print()\n    print('Conclusion:')\n    if p_val < 0.05:\n        print(f'  We reject H0. The salary difference is statistically significant.')\n        print(f'  However, Cohen\\'s d={cohens_d:.2f} -- interpret practical significance carefully.')\n    else:\n        print('  We fail to reject H0. No statistically significant salary difference.')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 5.2.2 -- Mann-Whitney U test: a non-parametric alternative\n#\n# The t-test assumes approximate normality. Since salary is skewed,\n# the Mann-Whitney U test is a safer choice -- it makes no distribution assumptions.\n# It tests whether one group tends to have higher values than the other.\n\nif 'uses_python' in df.columns:\n    u_stat, p_mw = stats.mannwhitneyu(\n        python_sal, nopython_sal,\n        alternative='two-sided'   # two-sided: we test for any difference\n    )\n\n    # Effect size for Mann-Whitney: rank-biserial correlation\n    # Ranges from -1 to +1; 0 = no difference\n    n1, n2 = len(python_sal), len(nopython_sal)\n    r_rb = 1 - (2 * u_stat) / (n1 * n2)\n\n    print('Mann-Whitney U test (non-parametric):')\n    print(f'  U-statistic:          {u_stat:,.0f}')\n    print(f'  p-value:              {p_mw:.4e}')\n    print(f'  Significant (a=0.05): {p_mw < 0.05}')\n    print(f'  Rank-biserial r:      {r_rb:.4f}')\n\n    print()\n    print('Both t-test and Mann-Whitney U tell the same story -- good.')\n    print('When they disagree, prefer Mann-Whitney U for skewed data.')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 5.2.3 -- One-way ANOVA: salary differences across education levels\n#\n# ANOVA (Analysis of Variance) tests whether means differ across 3+ groups.\n# H0: all group means are equal\n# H1: at least one group mean is different\n\nif 'EdLevel' in df.columns:\n    ed_map = {\n        \"Bachelor's degree (B.A., B.S., B.Eng., etc.)\": \"Bachelor's\",\n        \"Master's degree (M.A., M.S., M.Eng., MBA, etc.)\": \"Master's\",\n        'Some college/university study without earning a degree': 'Some college',\n        'Other doctoral degree (Ph.D., Ed.D., etc.)': 'PhD',\n        'Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)': 'Secondary',\n    }\n    df['ed_short'] = df['EdLevel'].replace(ed_map)\n\n    # Keep the 5 most common education levels\n    top_ed = df['ed_short'].value_counts().head(5).index.tolist()\n    groups = [\n        df[df['ed_short'] == ed]['ConvertedCompYearly'].dropna().values\n        for ed in top_ed\n    ]\n\n    f_stat, p_anova = stats.f_oneway(*groups)\n\n    print('One-way ANOVA: salary by education level')\n    print(f'  Groups tested: {top_ed}')\n    print(f'  F-statistic:   {f_stat:.4f}')\n    print(f'  p-value:       {p_anova:.4e}')\n    print(f'  Significant:   {p_anova < 0.05}')\n\n    print()\n    print('Group medians:')\n    for ed, grp in zip(top_ed, groups):\n        print(f'  {ed:<20} n={len(grp):>5,}  median=${np.median(grp):>10,.0f}')\n\n    # Eta-squared: effect size for ANOVA\n    # Proportion of total variance explained by group membership\n    all_vals = np.concatenate(groups)\n    grand_mean = all_vals.mean()\n    ss_between = sum(len(g) * (g.mean() - grand_mean)**2 for g in groups)\n    ss_total   = sum((v - grand_mean)**2 for v in all_vals)\n    eta_sq = ss_between / ss_total\n    print(f'  Eta-squared: {eta_sq:.4f}  ({eta_sq*100:.1f}% of salary variance explained by education)')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 5.2.4 -- Chi-squared test: independence of categorical variables\n#\n# Chi-squared tests whether two categorical variables are INDEPENDENT.\n# H0: the variables are independent (no association)\n# H1: the variables are associated\n#\n# Here: is Python adoption independent of remote work status?\n\nif 'RemoteWork' in df.columns and 'uses_python' in df.columns:\n    top_remote = df['RemoteWork'].value_counts().head(3).index.tolist()\n    df_sub = df[df['RemoteWork'].isin(top_remote)].copy()\n\n    # Build a contingency table: rows=remote status, cols=uses Python\n    contingency = pd.crosstab(df_sub['RemoteWork'], df_sub['uses_python'])\n    contingency.columns = ['Non-Python', 'Python']\n    print('Contingency table (RemoteWork x Python usage):')\n    print(contingency)\n\n    chi2, p_chi, dof, expected = stats.chi2_contingency(contingency)\n\n    print()\n    print('Chi-squared test:')\n    print(f'  chi2 statistic: {chi2:.4f}')\n    print(f'  p-value:        {p_chi:.4e}')\n    print(f'  Degrees of freedom: {dof}')\n    print(f'  Significant:    {p_chi < 0.05}')\n\n    # Cramer's V: effect size for chi-squared\n    n = contingency.values.sum()\n    cramers_v = np.sqrt(chi2 / (n * (min(contingency.shape) - 1)))\n    print(f\"  Cramer's V:     {cramers_v:.4f}  \", end='')\n    if cramers_v < 0.1:   print('(negligible association)')\n    elif cramers_v < 0.3: print('(weak association)')\n    elif cramers_v < 0.5: print('(moderate association)')\n    else:                  print('(strong association)')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 5.2.5 -- Visualising the hypothesis test results\n\nif 'uses_python' in df.columns and 'RemoteWork' in df.columns:\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n    # Left: salary distributions for Python vs non-Python\n    for uses_py, label, colour in [\n        (True,  f'Python (n={len(python_sal):,})',     '#E8722A'),\n        (False, f'Non-Python (n={len(nopython_sal):,})', '#2E75B6')\n    ]:\n        data = df[df['uses_python'] == uses_py]['ConvertedCompYearly'].dropna()\n        axes[0].hist(data, bins=50, density=True, alpha=0.5, color=colour, label=label)\n        axes[0].axvline(data.median(), color=colour, linestyle='--', linewidth=2)\n    axes[0].xaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f'${x/1000:.0f}k'))\n    axes[0].set_title(f'Salary: Python vs Non-Python\\n(p={p_mw:.2e}, Mann-Whitney U)')\n    axes[0].set_xlabel('Annual Salary (USD)')\n    axes[0].legend(fontsize=9)\n\n    # Right: median salary by education level\n    if 'ed_short' in df.columns:\n        ed_medians = (\n            df[df['ed_short'].isin(top_ed)]\n            .groupby('ed_short')['ConvertedCompYearly']\n            .median().sort_values(ascending=True)\n        )\n        axes[1].barh(ed_medians.index, ed_medians.values, color='#2E75B6', edgecolor='white')\n        axes[1].xaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f'${x/1000:.0f}k'))\n        for i, v in enumerate(ed_medians.values):\n            axes[1].text(v + 500, i, f'${v/1000:.0f}k', va='center', fontsize=9)\n        axes[1].set_title(f'Median Salary by Education\\n(ANOVA p={p_anova:.2e}, eta^2={eta_sq:.3f})')\n        axes[1].set_xlabel('Median Annual Salary (USD)')\n\n    plt.suptitle('SO 2025: Hypothesis Test Results', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 5.3 -- scipy.optimize: Curve Fitting and Root Finding\n\n`scipy.optimize` solves mathematical optimisation problems. Two uses matter most\nfor data science:\n\n- **Curve fitting:** find the parameters of a function that best fit your data\n- **Root finding:** find where a function equals zero (useful for threshold analysis)\n\nUnderstanding curve fitting conceptually prepares you for neural network training\nin Chapter 7 -- gradient descent is just a more sophisticated optimiser doing\nthe same thing: minimising a loss function by adjusting parameters.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 5.3.1 -- Curve fitting: model salary growth with experience\n#\n# We fit a logarithmic curve to the experience-salary relationship.\n# Logarithmic growth makes intuitive sense: early career years have\n# a large salary impact; later years have diminishing returns.\n\nif 'YearsCodePro' in df.columns:\n    # Bin experience into groups and compute median salary per group\n    exp_bins = list(range(0, 26, 2))   # 0,2,4,...,24\n    df['exp_bin'] = pd.cut(df['YearsCodePro'], bins=exp_bins, labels=exp_bins[:-1])\n    exp_salary = (\n        df.groupby('exp_bin', observed=True)['ConvertedCompYearly']\n        .agg(['median', 'count'])\n        .reset_index()\n    )\n    exp_salary.columns = ['years', 'median_salary', 'count']\n    exp_salary = exp_salary[exp_salary['count'] >= 20].copy()\n    exp_salary['years'] = exp_salary['years'].astype(float) + 1   # avoid log(0)\n\n    x_data = exp_salary['years'].values\n    y_data = exp_salary['median_salary'].values\n\n    # Define the model function to fit\n    # a * log(x) + b: logarithmic growth with two free parameters\n    def log_model(x, a, b):\n        return a * np.log(x) + b\n\n    # curve_fit finds a and b that minimise sum of squared residuals\n    popt, pcov = optimize.curve_fit(log_model, x_data, y_data)\n    a_fit, b_fit = popt\n\n    # R-squared: goodness of fit\n    y_pred   = log_model(x_data, *popt)\n    ss_res   = np.sum((y_data - y_pred) ** 2)\n    ss_tot   = np.sum((y_data - y_data.mean()) ** 2)\n    r_squared = 1 - ss_res / ss_tot\n\n    fig, ax = plt.subplots(figsize=(10, 5))\n    ax.scatter(x_data, y_data, s=exp_salary['count']/5,\n               color='#2E75B6', alpha=0.8, label='Observed median salary')\n    x_smooth = np.linspace(x_data.min(), x_data.max(), 200)\n    ax.plot(x_smooth, log_model(x_smooth, *popt), 'r-', linewidth=2.5,\n            label=f'Fit: {a_fit:,.0f} * log(x) + {b_fit:,.0f}  (R^2={r_squared:.3f})')\n    ax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda v, _: f'${v/1000:.0f}k'))\n    ax.set_xlabel('Years of Professional Experience')\n    ax.set_ylabel('Median Annual Salary (USD)')\n    ax.set_title('SO 2025: Salary vs Experience -- Logarithmic Curve Fit')\n    ax.legend(fontsize=10)\n    plt.tight_layout()\n    plt.show()\n\n    print(f'Fitted model: salary = {a_fit:,.0f} * log(years) + {b_fit:,.0f}')\n    print(f'R-squared: {r_squared:.4f}  ({r_squared*100:.1f}% of variance explained)')\n    for yrs in [1, 3, 5, 10, 15, 20]:\n        pred = log_model(yrs, *popt)\n        print(f'  Predicted median at {yrs:>2} years: ${pred:>10,.0f}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 5.3.2 -- Root finding: find experience level at a salary threshold\n#\n# Root finding answers: at what experience level does the model predict\n# a salary of exactly $X? We solve log_model(x) - target = 0.\n\nif 'YearsCodePro' in df.columns:\n    targets = [80_000, 100_000, 120_000, 150_000]\n\n    print('Experience years needed to reach salary targets (model prediction):')\n    for target in targets:\n        # Define f(x) = model(x) - target; find x where f(x) = 0\n        def equation(x):\n            return log_model(x, a_fit, b_fit) - target\n\n        try:\n            # brentq: reliable bracketed root-finding method\n            # We search between 0.5 and 40 years\n            root = optimize.brentq(equation, 0.5, 40)\n            print(f'  ${target:>10,}  ->  {root:.1f} years experience')\n        except ValueError:\n            print(f'  ${target:>10,}  ->  outside model range')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 5.4 -- scipy.interpolate: Filling Gaps\n\n`scipy.interpolate` estimates values between known data points.\nThe most common use in data science is filling gaps in time series data\nor smoothing noisy measurements. We demonstrate with a synthetic salary\ntrend over time -- the same technique applies to sensor data, financial\ntime series, or any sequential dataset with missing observations.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 5.4.1 -- Spline interpolation on salary trend data\n\n# Simulate annual median salary data with some missing years\nyears_known  = np.array([2017, 2018, 2020, 2021, 2023, 2024, 2025])\nsalary_known = np.array([85_000, 90_000, 98_000, 105_000, 118_000, 125_000, 132_000])\n\n# Cubic spline: smooth curve that passes exactly through all known points\nspline = interpolate.CubicSpline(years_known, salary_known)\n\n# Linear interpolation: straight lines between known points\nlinear = interpolate.interp1d(years_known, salary_known, kind='linear')\n\nyears_fine = np.linspace(2017, 2025, 200)   # 200 evenly spaced points\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax.scatter(years_known, salary_known, s=80, color='#1B3A5C', zorder=5,\n           label='Known data points')\nax.plot(years_fine, spline(years_fine),  color='#E8722A', linewidth=2.5,\n        label='Cubic spline interpolation')\nax.plot(years_fine, linear(years_fine),  color='#2E75B6', linewidth=1.5,\n        linestyle='--', label='Linear interpolation')\n\n# Mark the gap years that were filled\ngap_years = np.array([2019, 2022])\nfor yr in gap_years:\n    spline_val = float(spline(yr))\n    ax.scatter([yr], [spline_val], s=100, color='#E8722A', marker='D', zorder=6)\n    ax.annotate(f'Filled:\\n${spline_val/1000:.0f}k',\n                xy=(yr, spline_val), xytext=(yr+0.15, spline_val-4000),\n                fontsize=8.5, color='#E8722A')\n\nax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda v, _: f'${v/1000:.0f}k'))\nax.set_xlabel('Year')\nax.set_ylabel('Median Annual Salary (USD)')\nax.set_title('Salary Trend Interpolation: Cubic Spline vs Linear')\nax.legend(fontsize=10)\nplt.tight_layout()\nplt.show()\n\nprint('Filled missing years with cubic spline:')\nfor yr in gap_years:\n    print(f'  {yr}: ${float(spline(yr)):,.0f}')\nprint()\nprint('Cubic spline is smoother (captures curvature); linear is simpler and never overshoots.')\nprint('Use linear for data with sharp changes; cubic for smooth continuous trends.')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 5.5 -- Project: Statistical Feature Selection Summary\n\nWe now have statistical evidence for which features are related to salary.\nThis directly informs the feature engineering decisions in Chapter 6.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "print('=' * 65)\nprint('  SO 2025: Statistical Analysis Summary')\nprint('  Feature Relevance for Salary Prediction')\nprint('=' * 65)\n\nresults = []\n\nif 'uses_python' in df.columns:\n    results.append({\n        'Feature':    'Python usage',\n        'Test':       'Mann-Whitney U',\n        'p-value':    f'{p_mw:.2e}',\n        'Effect':     f'r={r_rb:.3f} (rank-biserial)',\n        'Include':    'YES' if p_mw < 0.05 else 'NO'\n    })\n\nif 'EdLevel' in df.columns:\n    results.append({\n        'Feature':    'Education level',\n        'Test':       'One-way ANOVA',\n        'p-value':    f'{p_anova:.2e}',\n        'Effect':     f'eta^2={eta_sq:.3f} ({eta_sq*100:.1f}% variance)',\n        'Include':    'YES' if p_anova < 0.05 else 'NO'\n    })\n\nif 'RemoteWork' in df.columns:\n    results.append({\n        'Feature':    'Remote work',\n        'Test':       'Chi-squared',\n        'p-value':    f'{p_chi:.2e}',\n        'Effect':     f\"Cramer's V={cramers_v:.3f}\",\n        'Include':    'YES' if p_chi < 0.05 else 'NO'\n    })\n\nif 'YearsCodePro' in df.columns:\n    results.append({\n        'Feature':    'Years experience',\n        'Test':       'Curve fit (R^2)',\n        'p-value':    'N/A',\n        'Effect':     f'R^2={r_squared:.3f} ({r_squared*100:.1f}% variance)',\n        'Include':    'YES' if r_squared > 0.5 else 'MARGINAL'\n    })\n\nprint(f'{\"Feature\":<20} {\"Test\":<20} {\"p-value\":<12} {\"Effect size\":<30} {\"Include\"}')\nprint('-' * 90)\nfor r in results:\n    print(f'{r[\"Feature\"]:<20} {r[\"Test\"]:<20} {r[\"p-value\"]:<12} {r[\"Effect\"]:<30} {r[\"Include\"]}')\n\nprint()\nprint('All tested features show statistically significant relationships with salary.')\nprint('All will be included in the Chapter 6 scikit-learn pipeline.')\nprint('Remember: statistical significance does not guarantee predictive power --')\nprint('that is what cross-validated model evaluation in Chapter 6 measures.')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Concept Check Questions\n\n> Test your understanding before moving on. Answer each question without referring back to the notebook, then expand to check.\n\n**Q1.** What is a **p-value**, and what does p = 0.03 tell you â€” and not tell you?\n\n<details><summary>Show answer</summary>\n\nA p-value is the probability of seeing results at least this extreme *if the null hypothesis were true*. p = 0.03 gives grounds to reject the null. It does **not** tell you the probability the null is true, the effect size, or whether the result is practically meaningful.\n\n</details>\n\n**Q2.** What is Cohen's d, and why should you always report it alongside a p-value?\n\n<details><summary>Show answer</summary>\n\nCohen's d measures effect size â€” the mean difference divided by the pooled standard deviation. With large samples, trivially small differences become statistically significant. Cohen's d (small â‰ˆ 0.2, medium â‰ˆ 0.5, large â‰ˆ 0.8) tells you whether the finding is practically meaningful.\n\n</details>\n\n**Q3.** When should you use a Mann-Whitney U test instead of a t-test?\n\n<details><summary>Show answer</summary>\n\nWhen the data is non-normal (heavy tails, outliers) or ordinal. The Mann-Whitney U test is non-parametric â€” it compares rank orderings, not means, making it robust when normality cannot be assumed.\n\n</details>\n\n**Q4.** What does `scipy.optimize.curve_fit` return?\n\n<details><summary>Show answer</summary>\n\n`(popt, pcov)`. `popt` is the array of optimal parameter values. `pcov` is the covariance matrix â€” its diagonal's square roots give the standard errors, quantifying uncertainty in each parameter estimate.\n\n</details>\n\n**Q5.** Python developers earn $12k more (p < 0.001, Cohen's d = 0.18). How would you explain this to a business stakeholder?\n\n<details><summary>Show answer</summary>\n\nThe difference is statistically real but the effect size is small (d = 0.18 is below the conventional 'small' threshold of 0.2). The $12k gap exists, but experience, country, and role likely matter far more. Don't over-interpret it as a strong causal signal.\n\n</details>\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Coding Exercises\n\n> Three exercises per chapter: **ðŸ”§ Guided** (fill-in-the-blanks) Â· **ðŸ”¨ Applied** (write from scratch) Â· **ðŸ—ï¸ Extension** (go beyond the chapter)\n\nExercises use the SO 2025 developer survey dataset.\nExpand each **Solution** block only after attempting the exercise.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exercise 1 ðŸ”§ Guided â€” Salary distribution hypothesis test\n\nComplete the `compare_salary_groups()` function that:\n1. Tests whether `uses_python=1` earns significantly more than `uses_python=0`\n2. Checks normality of each group with Shapiro-Wilk\n3. Chooses Mann-Whitney U (non-normal) or independent t-test (normal)\n4. Returns a dict with `statistic`, `p_value`, `test_used`, `significant` (Î±=0.05)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from scipy import stats\nimport numpy as np\n\nrng = np.random.default_rng(42)\ngroup0 = np.exp(10.6 + rng.normal(0,0.5,500))\ngroup1 = np.exp(10.9 + rng.normal(0,0.5,500))\n\ndef compare_salary_groups(g0: np.ndarray, g1: np.ndarray, alpha: float = 0.05) -> dict:\n    # 1. Shapiro-Wilk normality test (use sample of 500)\n    _, p_norm0 = stats.shapiro(g0[:500])\n    _, p_norm1 = None, None  # YOUR CODE\n    both_normal = None  # YOUR CODE\n    # 2. Choose and run appropriate test\n    if both_normal:\n        stat, p = None, None  # YOUR CODE (t-test)\n        test_name = 'ttest'\n    else:\n        stat, p = None, None  # YOUR CODE (Mann-Whitney U)\n        test_name = 'mannwhitneyu'\n    return {'statistic': round(stat,4), 'p_value': round(p,6),\n            'test_used': test_name, 'significant': p < alpha}\n\nprint(compare_salary_groups(group0, group1))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<details><summary>ðŸ’¡ Hint</summary>\n\n`stats.ttest_ind(g0, g1, alternative='less')` for one-tailed.\n`stats.mannwhitneyu(g0, g1, alternative='less')` for non-parametric.\n`both_normal = p_norm0 > 0.05 and p_norm1 > 0.05`\n\n</details>\n\n<details><summary>âœ… Solution</summary>\n\n```python\ndef compare_salary_groups(g0, g1, alpha=0.05):\n    _, p0 = stats.shapiro(g0[:500]); _, p1 = stats.shapiro(g1[:500])\n    both_normal = p0 > 0.05 and p1 > 0.05\n    if both_normal:\n        stat, p = stats.ttest_ind(g0, g1); test_name='ttest'\n    else:\n        stat, p = stats.mannwhitneyu(g0, g1, alternative='two-sided'); test_name='mannwhitneyu'\n    return {'statistic': round(stat,4), 'p_value': round(p,6), 'test_used': test_name, 'significant': p<alpha}\n```\n\n</details>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exercise 2 ðŸ”¨ Applied â€” Effect size across all binary features\n\nFor each binary feature in the SO 2025 dataset (`uses_python`, `uses_sql`,\n`uses_js`, `uses_ai`), compute:\n- Mann-Whitney U p-value (salary: feature=1 vs feature=0)\n- Cohen's d effect size\n- Rank the features by effect size and plot as a horizontal bar chart\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from scipy import stats\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng(42)\nn = 5000\ndf = pd.DataFrame({\n    'salary':     np.exp(10.8 + rng.normal(0,0.5,n)),\n    'uses_python':rng.integers(0,2,n),\n    'uses_sql':   rng.integers(0,2,n),\n    'uses_js':    rng.integers(0,2,n),\n    'uses_ai':    rng.integers(0,2,n)})\n\ndef cohens_d(g1: np.ndarray, g2: np.ndarray) -> float:\n    # YOUR CODE\n    pass\n\n# YOUR CODE: compute results for all 4 features and plot"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<details><summary>ðŸ’¡ Hint</summary>\n\nCohen's d = (mean1 - mean2) / pooled_std\npooled_std = sqrt(((n1-1)*std1Â² + (n2-1)*std2Â²) / (n1+n2-2))\nFilter df by feature value to get each group's salary distribution.\n\n</details>\n\n<details><summary>âœ… Solution</summary>\n\n```python\ndef cohens_d(g1, g2):\n    n1,n2 = len(g1),len(g2)\n    pooled = np.sqrt(((n1-1)*g1.std()**2+(n2-1)*g2.std()**2)/(n1+n2-2))\n    return (g1.mean()-g2.mean())/pooled\nresults=[]\nfor feat in ['uses_python','uses_sql','uses_js','uses_ai']:\n    g1=df[df[feat]==1]['salary'].values; g0=df[df[feat]==0]['salary'].values\n    _,p=stats.mannwhitneyu(g1,g0,alternative='two-sided')\n    d=cohens_d(g1,g0)\n    results.append({'feature':feat,'p_value':p,'cohens_d':d})\nres=pd.DataFrame(results).sort_values('cohens_d')\nres.plot.barh(x='feature',y='cohens_d'); plt.title('Cohen\\'s d Effect Size on Salary'); plt.show()\n```\n\n</details>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exercise 3 ðŸ—ï¸ Extension â€” Bootstrap confidence intervals\n\nImplement `bootstrap_ci(data, statistic_fn, n_bootstrap=10000, ci=0.95)`\nthat returns a (lower, upper) confidence interval for any statistic.\n\nUse it to compute 95% CIs for the median salary of each education level.\nPlot the results as a forest plot (point estimate + error bars).\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\ndef bootstrap_ci(\n    data: np.ndarray,\n    statistic_fn,\n    n_bootstrap: int = 10000,\n    ci: float = 0.95,\n    seed: int = 42,\n) -> tuple[float, float]:\n    # YOUR CODE\n    pass\n\n# Test\nsalaries = np.exp(10.8 + np.random.normal(0, 0.5, 500))\nlow, high = bootstrap_ci(salaries, np.median)\nprint(f'Median: {np.median(salaries):,.0f}  95% CI: [{low:,.0f}, {high:,.0f}]')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<details><summary>ðŸ’¡ Hint</summary>\n\nDraw `n_bootstrap` samples with replacement using `rng.choice(data, size=len(data), replace=True)`.\nApply `statistic_fn` to each bootstrap sample. The CI is the `(1-ci)/2` and `(1+ci)/2` percentiles of the bootstrap distribution.\n\n</details>\n\n<details><summary>âœ… Solution</summary>\n\n```python\ndef bootstrap_ci(data, statistic_fn, n_bootstrap=10000, ci=0.95, seed=42):\n    rng = np.random.default_rng(seed)\n    bootstraps = []\n    for _ in range(n_bootstrap):\n        sample = rng.choice(data,size=len(data),replace=True)\n        bootstraps.append(statistic_fn(sample))\n    bootstraps = np.array(bootstraps)\n    alpha = (1-ci)/2\n    return (np.percentile(bootstraps, 100*alpha), np.percentile(bootstraps, 100*(1-alpha)))\n```\n\n</details>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Chapter 5 Summary\n\n### Key Takeaways\n\n- **Describe before modelling:** skewness, kurtosis, and Q-Q plots reveal whether\n  your data meets modelling assumptions. Salary is right-skewed -- log-transform it.\n- **t-test** is the standard two-group mean comparison. Use **Welch's version**\n  (`equal_var=False`) by default -- it does not assume equal variances.\n- **Mann-Whitney U** is the non-parametric alternative. Safer for skewed data.\n  Use it when you cannot justify normality assumptions.\n- **ANOVA** extends the t-test to 3+ groups. Follow with a post-hoc test\n  (Tukey HSD) to find which specific pairs differ.\n- **Chi-squared** tests independence between two categorical variables.\n- **p-value alone is not enough.** Always report an effect size:\n  Cohen's d (t-test), rank-biserial r (Mann-Whitney), eta-squared (ANOVA),\n  Cramer's V (chi-squared).\n- **curve_fit** finds optimal parameters for any user-defined function.\n  The same idea -- minimising residuals -- underlies all of ML.\n- **Root finding** with `brentq` inverts a fitted model to answer threshold questions.\n- **Spline interpolation** fills gaps smoothly; linear interpolation is safer\n  when you cannot assume smooth curvature.\n\n### Project Thread Status\n\n| Test | Feature | Result |\n|------|---------|--------|\n| Mann-Whitney U | Python usage vs salary | Statistically significant |\n| One-way ANOVA | Education level vs salary | Statistically significant |\n| Chi-squared | Remote work vs Python usage | Tested |\n| Curve fit | Experience vs salary | Logarithmic model fitted |\n| Feature selection | All four features | Included in Chapter 6 pipeline |\n\n---\n\n### What's Next: Chapter 6 -- Machine Learning with scikit-learn\n\nChapter 6 is the largest chapter in the book. We build a complete scikit-learn\npipeline: preprocessing, feature engineering, salary regression, developer role\nclassification, and developer clustering. The statistical groundwork from this\nchapter informs every feature decision we make there.\n\n---\n\n*End of Chapter 5 -- Python for AI/ML*  \n[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n"
    }
  ]
}