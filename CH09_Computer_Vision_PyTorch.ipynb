{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Chapter 9 -- Computer Vision with PyTorch\n## *Python for AI/ML: A Complete Learning Journey*\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/CH09_Computer_Vision_PyTorch.ipynb)\n&nbsp;&nbsp;[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n\n---\n\n**Part:** 4 -- Production and Deployment  \n**Prerequisites:** Chapter 7 (Deep Learning with PyTorch)  \n**Estimated time:** 5-6 hours\n\n---\n\n> **Before running this notebook:** go to **Runtime â†’ Change runtime type â†’ T4 GPU**.\n> Transfer learning fine-tuning in Section 11.4 requires GPU.\n\n---\n\n### Learning Objectives\n\nBy the end of this chapter you will be able to:\n\n- Explain how convolutional layers detect spatial features in images\n- Build a CNN from scratch using `nn.Conv2d`, `nn.MaxPool2d`, and `nn.Linear`\n- Use `torchvision.transforms` to build an image augmentation pipeline\n- Load datasets with `torchvision.datasets` and `ImageFolder`\n- Apply transfer learning: freeze a pre-trained ResNet and replace its head\n- Fine-tune all layers of a pre-trained model with a lower learning rate\n- Visualise what a CNN has learned: feature maps and activation maximisation\n- Interpret predictions with Grad-CAM heatmaps\n\n---\n\n### Project Thread -- Chapter 11\n\nWe work with the **CIFAR-10** dataset (60,000 32x32 colour images, 10 classes)\nwhich is built into torchvision. We build three progressively more powerful models:\n\n1. **Custom CNN from scratch** -- understand every component\n2. **ResNet-18 with frozen backbone** -- transfer learning in minutes\n3. **ResNet-18 fine-tuned end-to-end** -- best accuracy\n\nCIFAR-10 is small enough to train quickly on a free Colab GPU\nwhile being complex enough to demonstrate why deep CNNs beat shallow ones.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Setup\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.optim.lr_scheduler import OneCycleLR\n\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torchvision.datasets import CIFAR10\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'PyTorch:  {torch.__version__}')\nprint(f'Device:   {DEVICE}')\nprint(f'Torchvision: {torchvision.__version__}')\n\nRANDOM_STATE = 42\ntorch.manual_seed(RANDOM_STATE)\nnp.random.seed(RANDOM_STATE)\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.dpi'] = 110\n\nCLASSES = ('plane','car','bird','cat','deer',\n           'dog','frog','horse','ship','truck')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 11.1 -- How Convolutional Neural Networks Work\n\nA fully-connected layer treats every pixel as an independent feature.\nFor a 32x32 colour image that is 32Ã—32Ã—3 = 3,072 inputs â€” and a 224x224\nimage is 150,528 inputs. This is computationally expensive and ignores\nthe spatial structure of images: nearby pixels are related, and the same\npattern (an edge, a curve) can appear anywhere in the image.\n\n**Convolutional layers** solve this with two ideas:\n\n**Local connectivity:** each neuron connects only to a small region\nof the input (the receptive field), not the whole image.\n\n**Weight sharing:** the same filter (kernel) is applied at every position.\nA filter that detects horizontal edges detects them everywhere in the image\nusing the same weights. This reduces parameters dramatically.\n\n**The building blocks:**\n- `nn.Conv2d(in_channels, out_channels, kernel_size)` -- learns filters\n- `nn.MaxPool2d(kernel_size)` -- downsamples by taking the max in each window\n- `nn.BatchNorm2d(channels)` -- normalises activations (same as Ch 7, but 2D)\n- `nn.ReLU()` -- non-linearity applied after each conv layer\n\nEarly layers learn low-level features (edges, colours).\nDeeper layers combine these into higher-level concepts (textures, shapes, objects).\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 11.1.1 -- Load CIFAR-10 with augmentation transforms\n\n# Training augmentation: random flips and crops make the model\n# robust to variations in position and orientation\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomCrop(32, padding=4),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.ToTensor(),\n    # Normalise with CIFAR-10 channel means and stds\n    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n                         std= [0.2470, 0.2435, 0.2616]),\n])\n\n# Validation/test: only normalise, no augmentation\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n                         std= [0.2470, 0.2435, 0.2616]),\n])\n\n# Download CIFAR-10 (~170MB, cached after first run)\ntrain_dataset = CIFAR10(root='/tmp/cifar10', train=True,\n                        download=True, transform=train_transform)\ntest_dataset  = CIFAR10(root='/tmp/cifar10', train=False,\n                        download=True, transform=test_transform)\n\n# Split training into train + validation\nn_val   = 5000\nn_train = len(train_dataset) - n_val\ntrain_ds, val_ds = random_split(\n    train_dataset, [n_train, n_val],\n    generator=torch.Generator().manual_seed(RANDOM_STATE)\n)\n\ntrain_loader = DataLoader(train_ds,   batch_size=128, shuffle=True,  num_workers=2, pin_memory=True)\nval_loader   = DataLoader(val_ds,     batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\ntest_loader  = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n\nprint(f'Train: {len(train_ds):,}  Val: {len(val_ds):,}  Test: {len(test_dataset):,}')\nprint(f'Classes: {CLASSES}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 11.1.2 -- Visualise sample images\n\n# Get one batch and undo normalisation for display\nimages, labels = next(iter(DataLoader(test_dataset, batch_size=16, shuffle=True)))\n\nmean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3,1,1)\nstd  = torch.tensor([0.2470, 0.2435, 0.2616]).view(3,1,1)\nimages_display = (images * std + mean).clamp(0, 1)\n\nfig, axes = plt.subplots(2, 8, figsize=(16, 5))\nfor i, ax in enumerate(axes.flatten()):\n    img = images_display[i].permute(1, 2, 0).numpy()\n    ax.imshow(img)\n    ax.set_title(CLASSES[labels[i]], fontsize=8)\n    ax.axis('off')\nplt.suptitle('CIFAR-10 Sample Images (16 random test examples)',\n             fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 11.2 -- Building a CNN from Scratch\n\nBefore using pre-trained models, we build a CNN from scratch so every\ncomponent is transparent. This architecture follows the classic pattern:\nstacked conv blocks (conv â†’ batchnorm â†’ relu â†’ pool) followed by\na classifier head (flatten â†’ dense â†’ output).\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 11.2.1 -- Define a custom CNN\n\nclass CifarCNN(nn.Module):\n    \"\"\"\n    Custom CNN for CIFAR-10 (32x32 colour images, 10 classes).\n\n    Architecture:\n        Conv Block 1: 3  -> 32  channels, 3x3 kernel\n        Conv Block 2: 32 -> 64  channels, 3x3 kernel\n        Conv Block 3: 64 -> 128 channels, 3x3 kernel\n        Classifier:   128*4*4 -> 256 -> 10\n    \"\"\"\n\n    def _conv_block(self, in_ch, out_ch):\n        return nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),       # halve spatial dimensions\n            nn.Dropout2d(0.1),\n        )\n\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.block1 = self._conv_block(3,   32)\n        self.block2 = self._conv_block(32,  64)\n        self.block3 = self._conv_block(64, 128)\n        # After 3 MaxPool2d(2): 32 -> 16 -> 8 -> 4\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(128 * 4 * 4, 256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(256, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        return self.classifier(x)\n\n\ncnn = CifarCNN(num_classes=10).to(DEVICE)\nn_params = sum(p.numel() for p in cnn.parameters() if p.requires_grad)\nprint(f'CifarCNN parameters: {n_params:,}')\n\n# Test forward pass\nx_test = torch.randn(4, 3, 32, 32).to(DEVICE)\nout    = cnn(x_test)\nprint(f'Input shape:  {x_test.shape}')\nprint(f'Output shape: {out.shape}  (4 samples, 10 class scores)')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 11.2.2 -- Training utilities (reuse Ch 7 pattern)\n\ndef train_epoch_clf(model, loader, criterion, optimizer, scheduler=None):\n    model.train()\n    total_loss, correct, total = 0.0, 0, 0\n    for images, labels in loader:\n        images, labels = images.to(DEVICE), labels.to(DEVICE)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss    = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()\n        total_loss += loss.item() * len(images)\n        correct    += (outputs.argmax(1) == labels).sum().item()\n        total      += len(images)\n    return total_loss / total, correct / total\n\n\ndef evaluate_clf(model, loader, criterion):\n    model.eval()\n    total_loss, correct, total = 0.0, 0, 0\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(DEVICE), labels.to(DEVICE)\n            outputs = model(images)\n            loss    = criterion(outputs, labels)\n            total_loss += loss.item() * len(images)\n            correct    += (outputs.argmax(1) == labels).sum().item()\n            total      += len(images)\n            all_preds.extend(outputs.argmax(1).cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    return total_loss / total, correct / total, all_preds, all_labels\n\n\nprint('Training utilities defined.')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 11.2.3 -- Train the custom CNN for 20 epochs\n\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\noptimizer = optim.AdamW(cnn.parameters(), lr=1e-3, weight_decay=1e-4)\nscheduler = OneCycleLR(\n    optimizer, max_lr=1e-2,\n    steps_per_epoch=len(train_loader), epochs=20\n)\n\nN_EPOCHS = 20\ncnn_history = {'train_loss':[], 'val_loss':[], 'train_acc':[], 'val_acc':[]}\nbest_val_acc = 0.0\nbest_cnn_weights = None\n\nprint(f'Training CifarCNN for {N_EPOCHS} epochs on {DEVICE}...')\nprint(f'{\"Epoch\":>6}  {\"Train Loss\":>11}  {\"Train Acc\":>10}  {\"Val Acc\":>9}')\nprint('-' * 42)\n\nfor epoch in range(1, N_EPOCHS + 1):\n    tr_loss, tr_acc = train_epoch_clf(cnn, train_loader, criterion, optimizer, scheduler)\n    val_loss, val_acc, _, _ = evaluate_clf(cnn, val_loader, criterion)\n    cnn_history['train_loss'].append(tr_loss)\n    cnn_history['val_loss'].append(val_loss)\n    cnn_history['train_acc'].append(tr_acc)\n    cnn_history['val_acc'].append(val_acc)\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        best_cnn_weights = {k: v.clone() for k, v in cnn.state_dict().items()}\n    if epoch % 5 == 0 or epoch == 1:\n        print(f'{epoch:>6}  {tr_loss:>11.4f}  {tr_acc:>10.4f}  {val_acc:>9.4f}')\n\ncnn.load_state_dict(best_cnn_weights)\n_, test_acc, _, _ = evaluate_clf(cnn, test_loader, criterion)\nprint(f'Best val acc: {best_val_acc:.4f}  |  Test acc: {test_acc:.4f}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 11.3 -- Visualising What the CNN Learned\n\nA common criticism of deep learning is that it is a black box.\nFor CNNs, this is less true than it seems -- we can directly inspect\nthe intermediate activations (feature maps) to see what each layer detects.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 11.3.1 -- Visualise feature maps from the first conv block\n\ncnn.eval()\n\n# Pick one test image\nsample_img, sample_label = test_dataset[42]\nsample_tensor = sample_img.unsqueeze(0).to(DEVICE)   # add batch dim\n\n# Register a forward hook to capture the output of block1\nfeature_maps = {}\n\ndef hook_fn(module, input, output):\n    feature_maps['block1'] = output.detach().cpu()\n\nhook = cnn.block1.register_forward_hook(hook_fn)\n\nwith torch.no_grad():\n    _ = cnn(sample_tensor)\n\nhook.remove()\n\nmaps = feature_maps['block1'][0]   # shape: (32, 16, 16) -- 32 filters\nprint(f'Feature map shape: {maps.shape}  (32 filters, 16x16 after MaxPool)')\n\n# Display original image and first 16 feature maps\nfig = plt.figure(figsize=(16, 5))\ngs  = gridspec.GridSpec(2, 9, figure=fig)\n\n# Original image\nmean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3,1,1)\nstd  = torch.tensor([0.2470, 0.2435, 0.2616]).view(3,1,1)\norig = (sample_img * std + mean).clamp(0,1).permute(1,2,0).numpy()\nax0  = fig.add_subplot(gs[:, 0])\nax0.imshow(orig)\nax0.set_title(f'Input:\\n{CLASSES[sample_label]}', fontsize=9)\nax0.axis('off')\n\nfor i in range(16):\n    row = i // 8\n    col = (i % 8) + 1\n    ax  = fig.add_subplot(gs[row, col])\n    ax.imshow(maps[i].numpy(), cmap='viridis')\n    ax.set_title(f'F{i}', fontsize=7)\n    ax.axis('off')\n\nplt.suptitle('CNN Feature Maps: First Conv Block (16 of 32 filters)',\n             fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 11.4 -- Transfer Learning with ResNet-18\n\nResNet-18 is a 18-layer residual network pre-trained on ImageNet --\n1.2 million images across 1,000 classes. Its weights encode rich visual\nknowledge: edges, textures, shapes, objects.\n\n**Transfer learning** re-uses this knowledge for a new task by:\n1. Loading the pre-trained weights\n2. Freezing all layers (they are not updated during training)\n3. Replacing the final classification head with a new one for our classes\n4. Training only the new head\n\nThis takes minutes instead of hours and often outperforms a custom CNN\ntrained from scratch, because the pre-trained features are so rich.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 11.4.1 -- ResNet-18 with frozen backbone (feature extraction)\n\n# Load pre-trained ResNet-18\nresnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n\n# Freeze all parameters -- they will not be updated\nfor param in resnet.parameters():\n    param.requires_grad = False\n\n# Replace the final fully-connected layer\n# ResNet-18's original fc: 512 -> 1000 (ImageNet classes)\n# Our new fc: 512 -> 10 (CIFAR-10 classes)\nn_features = resnet.fc.in_features\nresnet.fc  = nn.Linear(n_features, 10)\n# Only the new head has requires_grad=True\n\nresnet = resnet.to(DEVICE)\n\ntrainable = sum(p.numel() for p in resnet.parameters() if p.requires_grad)\ntotal     = sum(p.numel() for p in resnet.parameters())\nprint(f'ResNet-18 total parameters:     {total:,}')\nprint(f'Trainable (head only):          {trainable:,}  ({trainable/total*100:.1f}%)')\nprint(f'Frozen (backbone):              {total-trainable:,}  ({(total-trainable)/total*100:.1f}%)')\n\n# Larger transforms for ResNet (expects 224x224 but we adapt for CIFAR)\nresnet_transform = transforms.Compose([\n    transforms.Resize(64),           # upsample 32x32 to 64x64\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std= [0.229, 0.224, 0.225]),   # ImageNet stats\n])\nresnet_aug = transforms.Compose([\n    transforms.Resize(64),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(64, padding=8),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std= [0.229, 0.224, 0.225]),\n])\n\nrn_train_ds = CIFAR10('/tmp/cifar10', train=True,  download=False, transform=resnet_aug)\nrn_test_ds  = CIFAR10('/tmp/cifar10', train=False, download=False, transform=resnet_transform)\nrn_train_ds, rn_val_ds = random_split(\n    rn_train_ds, [45000, 5000],\n    generator=torch.Generator().manual_seed(RANDOM_STATE)\n)\nrn_train_loader = DataLoader(rn_train_ds, batch_size=128, shuffle=True,  num_workers=2)\nrn_val_loader   = DataLoader(rn_val_ds,   batch_size=256, shuffle=False, num_workers=2)\nrn_test_loader  = DataLoader(rn_test_ds,  batch_size=256, shuffle=False, num_workers=2)\nprint('ResNet data loaders ready.')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 11.4.2 -- Train the ResNet head for 10 epochs\n\nrn_criterion = nn.CrossEntropyLoss()\nrn_optimizer = optim.AdamW(resnet.fc.parameters(), lr=1e-3, weight_decay=1e-4)\n\nN_RN_EPOCHS = 10\nrn_history  = {'train_acc': [], 'val_acc': []}\nbest_rn_acc = 0.0\nbest_rn_weights = None\n\nprint(f'Training ResNet-18 head for {N_RN_EPOCHS} epochs on {DEVICE}...')\nprint(f'{\"Epoch\":>6}  {\"Train Acc\":>10}  {\"Val Acc\":>9}')\nprint('-' * 30)\n\nfor epoch in range(1, N_RN_EPOCHS + 1):\n    tr_loss, tr_acc = train_epoch_clf(resnet, rn_train_loader, rn_criterion, rn_optimizer)\n    val_loss, val_acc, _, _ = evaluate_clf(resnet, rn_val_loader, rn_criterion)\n    rn_history['train_acc'].append(tr_acc)\n    rn_history['val_acc'].append(val_acc)\n    if val_acc > best_rn_acc:\n        best_rn_acc = val_acc\n        best_rn_weights = {k: v.clone() for k, v in resnet.state_dict().items()}\n    if epoch % 2 == 0 or epoch == 1:\n        print(f'{epoch:>6}  {tr_acc:>10.4f}  {val_acc:>9.4f}')\n\nresnet.load_state_dict(best_rn_weights)\n_, rn_test_acc, _, _ = evaluate_clf(resnet, rn_test_loader, rn_criterion)\nprint(f'ResNet head-only  test accuracy: {rn_test_acc:.4f}')\nprint(f'Custom CNN        test accuracy: {test_acc:.4f}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 11.4.3 -- Compare models and plot training curves\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Training curves -- custom CNN\nepochs_cnn = range(1, len(cnn_history['val_acc']) + 1)\naxes[0].plot(epochs_cnn, cnn_history['train_acc'], '#E8722A', linewidth=2, label='Train')\naxes[0].plot(epochs_cnn, cnn_history['val_acc'],   '#2E75B6', linewidth=2, label='Val')\naxes[0].axhline(test_acc, color='green', linestyle='--', linewidth=1.5,\n                label=f'Test acc={test_acc:.3f}')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Accuracy')\naxes[0].set_title('Custom CNN from Scratch')\naxes[0].legend()\naxes[0].set_ylim(0, 1)\n\n# Training curves -- ResNet\nepochs_rn = range(1, len(rn_history['val_acc']) + 1)\naxes[1].plot(epochs_rn, rn_history['train_acc'], '#E8722A', linewidth=2, label='Train')\naxes[1].plot(epochs_rn, rn_history['val_acc'],   '#2E75B6', linewidth=2, label='Val')\naxes[1].axhline(rn_test_acc, color='green', linestyle='--', linewidth=1.5,\n                label=f'Test acc={rn_test_acc:.3f}')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Accuracy')\naxes[1].set_title('ResNet-18 Transfer Learning (head only, 10 epochs)')\naxes[1].legend()\naxes[1].set_ylim(0, 1)\n\nplt.suptitle('CIFAR-10: Custom CNN vs Transfer Learning',\n             fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(f'Custom CNN (20 epochs):           {test_acc:.4f}')\nprint(f'ResNet-18 head-only (10 epochs):  {rn_test_acc:.4f}')\nprint(f'Improvement from transfer learning: {(rn_test_acc - test_acc)*100:+.1f} pp')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 11.5 -- Object Detection with a Pre-trained Model\n\n**Classification** asks: *what is in this image?*  \n**Object detection** asks: *what is in this image, and where is each instance?*\n\nDetection models output bounding boxes (x, y, width, height) and class labels\nfor every object found. This is fundamental to:\nautonomous vehicles, security cameras, medical imaging, and robotics.\n\n**Two dominant architectures:**\n\n**Faster R-CNN** (two-stage): proposes regions of interest, then classifies them.\nSlower but very accurate. Available in `torchvision.models.detection`.\n\n**YOLO** (one-stage): divides the image into a grid and predicts boxes and classes\nin a single forward pass. Much faster, slightly less accurate on small objects.\n\nWe use a pre-trained Faster R-CNN from torchvision -- no training required,\nsame transfer learning principle as Chapter 11's ResNet classifier.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 11.5.1 -- Load pre-trained Faster R-CNN and run inference\n\nimport torchvision.models.detection as detection_models\nfrom torchvision.transforms.functional import to_tensor\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport numpy as np\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\n\n# Load Faster R-CNN pre-trained on COCO (80 object categories)\nprint('Loading Faster R-CNN (ResNet-50 FPN backbone, pre-trained on COCO)...')\nfrcnn = detection_models.fasterrcnn_resnet50_fpn(\n    weights=detection_models.FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n)\nfrcnn = frcnn.to(DEVICE)\nfrcnn.eval()\n\n# COCO category names (80 classes)\nCOCO_NAMES = [\n    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n    'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator',\n    'N/A', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n    'toothbrush'\n]\n\nprint(f'Model loaded: Faster R-CNN with {sum(p.numel() for p in frcnn.parameters()):,} parameters')\nprint(f'Classes: {len([n for n in COCO_NAMES if n != \"N/A\" and n != \"__background__\"])} COCO categories')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 11.5.2 -- Run detection on a sample image\n\n# Download a public domain street scene image\nIMG_URL = 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/PNG_transparency_demonstration_1.png/280px-PNG_transparency_demonstration_1.png'\n\n# Use a simpler approach: generate a synthetic test image with shapes\n# This avoids network dependency and copyright issues\nfrom torchvision.datasets import CIFAR10\n\n# Use the CIFAR-10 test images we already downloaded\n# Get a batch of test images\ntest_imgs, test_lbls = next(iter(\n    torch.utils.data.DataLoader(\n        CIFAR10('/tmp/cifar10', train=False, download=False,\n                transform=torchvision.transforms.ToTensor()),\n        batch_size=4, shuffle=False\n    )\n))\n\n# Run Faster R-CNN on CIFAR images\n# Note: Faster R-CNN expects images in [0,1] range, any size\n# CIFAR-10 at 32x32 is too small for meaningful detection,\n# but this demonstrates the API correctly\nfrcnn.eval()\nwith torch.no_grad():\n    # Upsample to 128x128 for better detection\n    imgs_up = torch.nn.functional.interpolate(test_imgs, size=(128,128))\n    imgs_list = [img.to(DEVICE) for img in imgs_up]\n    predictions = frcnn(imgs_list)\n\nprint(f'Processed {len(predictions)} images')\nprint(f'Keys in each prediction: {list(predictions[0].keys())}')\nprint(f'  boxes:  bounding box coordinates [x1, y1, x2, y2]')\nprint(f'  labels: COCO class index for each detected object')\nprint(f'  scores: confidence score for each detection')\nprint()\n\nSCORE_THRESHOLD = 0.5\nfor i, pred in enumerate(predictions):\n    keep  = pred['scores'] > SCORE_THRESHOLD\n    boxes  = pred['boxes'][keep].cpu()\n    labels = pred['labels'][keep].cpu()\n    scores = pred['scores'][keep].cpu()\n    print(f'Image {i+1} (true label: {CLASSES[test_lbls[i]]}): '\n          f'{keep.sum().item()} detections above {SCORE_THRESHOLD}')\n    for box, lbl, scr in zip(boxes[:3], labels[:3], scores[:3]):\n        print(f'  {COCO_NAMES[lbl.item()]:<15} score={scr:.3f}  '\n              f'box=[{box[0]:.0f},{box[1]:.0f},{box[2]:.0f},{box[3]:.0f}]')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 11.5.3 -- Visualise detection results with bounding boxes\n\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\ncolours = plt.cm.tab10.colors\n\nfor ax_idx, (ax, img_t, pred, true_lbl) in enumerate(\n    zip(axes, test_imgs, predictions, test_lbls)\n):\n    img_np = img_t.permute(1,2,0).numpy()\n    # Upsample for display\n    from PIL import Image as PILImage\n    img_pil = PILImage.fromarray((img_np * 255).astype(np.uint8)).resize((128,128))\n    ax.imshow(img_pil)\n\n    keep   = pred['scores'] > SCORE_THRESHOLD\n    boxes  = pred['boxes'][keep].cpu()\n    labels = pred['labels'][keep].cpu()\n    scores = pred['scores'][keep].cpu()\n\n    for j, (box, lbl, scr) in enumerate(zip(boxes, labels, scores)):\n        x1, y1, x2, y2 = box.tolist()\n        w, h = x2 - x1, y2 - y1\n        colour = colours[j % len(colours)]\n        rect = patches.Rectangle((x1, y1), w, h,\n                                   linewidth=2, edgecolor=colour, facecolor='none')\n        ax.add_patch(rect)\n        name = COCO_NAMES[lbl.item()]\n        ax.text(x1, y1 - 2, f'{name} {scr:.2f}',\n                color='white', fontsize=7,\n                bbox=dict(facecolor=colour, alpha=0.7, pad=1))\n\n    n_det = keep.sum().item()\n    ax.set_title(f'True: {CLASSES[true_lbl]}\\n{n_det} detection(s)', fontsize=9)\n    ax.axis('off')\n\nplt.suptitle('Faster R-CNN Object Detection (pre-trained on COCO)',\n             fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint('Note: CIFAR-10 images are 32x32 (upsampled to 128x128 here).')\nprint('Faster R-CNN is designed for full-resolution photos.')\nprint('On real photos it detects people, cars, animals, etc. with high accuracy.')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 11.6 -- Semantic Segmentation\n\n**Semantic segmentation** assigns a class label to *every pixel* in an image.\nRather than drawing a bounding box around a car, segmentation colours every\npixel that belongs to a car with the same label.\n\nApplications: medical imaging (tumour boundaries), autonomous driving\n(road vs pedestrian vs kerb), satellite imagery (land use classification).\n\nThe standard architecture is the **encoder-decoder**:\n- **Encoder:** a CNN backbone (like ResNet) that progressively downsamples the image\n  to a compact feature representation\n- **Decoder:** upsamples back to the original resolution, producing a\n  per-pixel class prediction\n\n**DeepLabV3** with a ResNet-50 backbone is available pre-trained in torchvision\non the Pascal VOC and COCO datasets (21 and 91 classes respectively).\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 11.6.1 -- Semantic segmentation with pre-trained DeepLabV3\n\nimport torchvision.models.segmentation as seg_models\n\nprint('Loading DeepLabV3+ (ResNet-50 backbone, pre-trained on COCO)...')\ndeeplab = seg_models.deeplabv3_resnet50(\n    weights=seg_models.DeepLabV3_ResNet50_Weights.DEFAULT\n)\ndeeplab = deeplab.to(DEVICE)\ndeeplab.eval()\n\nn_params = sum(p.numel() for p in deeplab.parameters())\nprint(f'DeepLabV3 parameters: {n_params:,}')\n\n# VOC class names and colours for visualisation\nVOC_CLASSES = [\n    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n    'bus', 'car', 'cat', 'chair', 'cow', 'dining table', 'dog',\n    'horse', 'motorbike', 'person', 'potted plant', 'sheep', 'sofa',\n    'train', 'tv/monitor'\n]\n\n# Run segmentation on upsampled CIFAR images\nseg_transform = torchvision.transforms.Normalize(\n    mean=[0.485, 0.456, 0.406],\n    std= [0.229, 0.224, 0.225]\n)\n\n# Take 2 test images, upsample to 224x224\nimgs_seg = torch.nn.functional.interpolate(test_imgs[:2], size=(224, 224))\nimgs_norm = torch.stack([seg_transform(img) for img in imgs_seg]).to(DEVICE)\n\nwith torch.no_grad():\n    seg_output = deeplab(imgs_norm)\n\nseg_masks = seg_output['out'].argmax(dim=1).cpu()   # shape: (2, 224, 224)\n\nprint(f'Segmentation output shape: {seg_output[\"out\"].shape}')\nprint(f'  (batch, num_classes, H, W) -> argmax -> (batch, H, W) class per pixel')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 11.6.2 -- Visualise segmentation masks\n\ncmap = plt.cm.get_cmap('tab20', len(VOC_CLASSES))\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 9))\n\nfor i in range(2):\n    # Original image\n    img_np = imgs_seg[i].cpu().permute(1,2,0).numpy()\n    axes[i][0].imshow(img_np)\n    axes[i][0].set_title(f'Input: {CLASSES[test_lbls[i]]}', fontsize=10)\n    axes[i][0].axis('off')\n\n    # Segmentation mask\n    mask = seg_masks[i].numpy()\n    unique_classes = np.unique(mask)\n    im = axes[i][1].imshow(mask, cmap=cmap, vmin=0, vmax=len(VOC_CLASSES)-1)\n    detected = [VOC_CLASSES[c] for c in unique_classes if c < len(VOC_CLASSES)]\n    axes[i][1].set_title(f'Segmentation\\nDetected: {\", \".join(detected[:4])}', fontsize=9)\n    axes[i][1].axis('off')\n\nplt.suptitle('DeepLabV3 Semantic Segmentation (pre-trained on COCO/VOC)',\n             fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint('Each colour = one semantic class (background, person, car, etc.)')\nprint('Every pixel is classified -- no bounding boxes, just per-pixel labels.')\nprint()\nprint('Three CV tasks and when to use them:')\nprint('  Classification:  What is in the image? (one label per image)')\nprint('  Detection:       Where are the objects? (boxes + labels)')\nprint('  Segmentation:    What is every pixel? (mask + labels)')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Concept Check Questions\n\n> Test your understanding before moving on. Answer each question without referring back to the notebook, then expand to check.\n\n**Q1.** Explain **weight sharing** in a convolutional layer and why it makes CNNs efficient.\n\n<details><summary>Show answer</summary>\n\nA convolutional layer uses the same filter (e.g., 3Ã—3, 9 weights) at every position. A fully connected layer on a 224Ã—224 image would need millions of weights for the first layer alone. Weight sharing also encodes translation equivariance: a feature detected top-left is detected identically bottom-right.\n\n</details>\n\n**Q2.** What is **transfer learning** and why does it work for image tasks?\n\n<details><summary>Show answer</summary>\n\nTransfer learning reuses a model pretrained on a large dataset (e.g., ImageNet) as a starting point. Early layers learn generic features (edges, textures) useful across almost all visual tasks. Freezing these layers and only training a new classification head gives strong performance with far less data and compute.\n\n</details>\n\n**Q3.** What is the difference between object detection and image classification? What extra output does detection produce?\n\n<details><summary>Show answer</summary>\n\nClassification answers 'what is in this image?' with one label per image. Detection answers 'where are the objects?' â€” it outputs **bounding boxes** (x1, y1, x2, y2), a **class label**, and a **confidence score** per object. A single image can contain multiple detected objects of different classes.\n\n</details>\n\n**Q4.** ResNet-18 achieves 94% accuracy in 5 epochs; your custom CNN achieves 74% in 20 epochs. What explains the gap and how would you close it?\n\n<details><summary>Show answer</summary>\n\nThe gap comes from pretrained ImageNet weights encoding rich visual features. To close it: (1) unfreeze more backbone layers and fine-tune with a small LR; (2) increase data augmentation; (3) try a larger backbone (ResNet-50 or EfficientNet).\n\n</details>\n\n**Q5.** What does a forward hook do in PyTorch, and how is it used to visualise feature maps?\n\n<details><summary>Show answer</summary>\n\nA forward hook registers a callback on a module with `register_forward_hook(fn)`. PyTorch calls `fn(module, input, output)` after each forward pass. Register it on a conv layer to capture the output tensor, then plot each channel as a grayscale image to see which spatial regions activated that filter.\n\n</details>\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Coding Exercises\n\n> Three exercises per chapter: **ðŸ”§ Guided** (fill-in-the-blanks) Â· **ðŸ”¨ Applied** (write from scratch) Â· **ðŸ—ï¸ Extension** (go beyond the chapter)\n\nExercises use the SO 2025 developer survey dataset.\nExpand each **Solution** block only after attempting the exercise.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exercise 1 ðŸ”§ Guided â€” Complete a custom CNN block\n\nFill in `ConvBlock.forward()` to implement:\nConv2d â†’ BatchNorm2d â†’ ReLU â†’ MaxPool2d (if `pool=True`)\nThen verify the output shape is correct for CIFAR-10 input.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import torch\nimport torch.nn as nn\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_ch: int, out_ch: int, pool: bool = False) -> None:\n        super().__init__()\n        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1)\n        self.bn   = nn.BatchNorm2d(out_ch)\n        self.relu = nn.ReLU(inplace=True)\n        self.pool = nn.MaxPool2d(2) if pool else nn.Identity()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # YOUR CODE: apply layers in order\n        pass\n\n# Test: CIFAR-10 batch\nx = torch.randn(4, 3, 32, 32)\nb1 = ConvBlock(3, 32, pool=False)\nb2 = ConvBlock(32, 64, pool=True)\nprint(b1(x).shape)      # expect [4, 32, 32, 32]\nprint(b2(b1(x)).shape)  # expect [4, 64, 16, 16]"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<details><summary>ðŸ’¡ Hint</summary>\n\nApply in order: `self.conv(x)` â†’ `self.bn(...)` â†’ `self.relu(...)` â†’ `self.pool(...)`\nEach output feeds into the next operation.\n\n</details>\n\n<details><summary>âœ… Solution</summary>\n\n```python\ndef forward(self, x):\n    return self.pool(self.relu(self.bn(self.conv(x))))\n```\n\n</details>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exercise 2 ðŸ”¨ Applied â€” Grad-CAM visualisation\n\nImplement a simplified Grad-CAM that shows *which spatial regions*\na ResNet-18 uses to make its classification decision.\n\nFor a batch of CIFAR-10 test images:\n1. Register a forward hook on the last conv layer to capture activations\n2. Compute the gradient of the class score w.r.t. those activations\n3. Weight activations by global-average-pooled gradients\n4. Visualise the heatmap overlaid on the original image\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torchvision\nimport torchvision.transforms as T\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load a pretrained or trained ResNet-18\nmodel = models.resnet18(weights=None)\nmodel.fc = nn.Linear(512, 10)\nmodel.eval()\n\nclass GradCAM:\n    def __init__(self, model: nn.Module, target_layer: nn.Module) -> None:\n        self.activations = None\n        self.gradients    = None\n        target_layer.register_forward_hook(self._save_activation)\n        target_layer.register_full_backward_hook(self._save_gradient)\n\n    def _save_activation(self, module, input, output) -> None:\n        self.activations = output.detach()\n\n    def _save_gradient(self, module, grad_input, grad_output) -> None:\n        self.gradients = grad_output[0].detach()\n\n    def generate(self, x: torch.Tensor, class_idx: int) -> np.ndarray:\n        # YOUR CODE: forward, backward, weight activations by avg-pooled grads\n        pass\n\n# cam = GradCAM(model, model.layer4[-1].conv2)\n# heatmap = cam.generate(img_tensor, class_idx)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<details><summary>ðŸ’¡ Hint</summary>\n\nIn `generate`:\n1. `output = model(x)` â€” forward pass\n2. `output[0, class_idx].backward()` â€” backward for chosen class\n3. `weights = self.gradients.mean(dim=[2,3], keepdim=True)` â€” global avg pool over spatial dims\n4. `cam = (weights * self.activations).sum(dim=1, keepdim=True)` â€” weighted sum of channels\n5. `F.relu(cam)` and upsample to input size\n\n</details>\n\n<details><summary>âœ… Solution</summary>\n\n```python\ndef generate(self, x, class_idx):\n    x.requires_grad_(True)\n    output = model(x)\n    model.zero_grad()\n    output[0, class_idx].backward()\n    weights = self.gradients.mean(dim=[2,3], keepdim=True)\n    cam = torch.nn.functional.relu((weights*self.activations).sum(dim=1,keepdim=True))\n    cam = torch.nn.functional.interpolate(cam, x.shape[2:], mode='bilinear', align_corners=False)\n    cam = cam.squeeze().numpy()\n    return (cam - cam.min())/(cam.max()-cam.min()+1e-8)\n```\n\n</details>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exercise 3 ðŸ—ï¸ Extension â€” Custom data augmentation pipeline\n\nImplement a `SurveyImageAugmentor` class that:\n1. Applies random horizontal flip, colour jitter, and random crop\n2. Supports `strength` parameter (0.0 = no aug, 1.0 = heavy aug)\n3. Tracks and reports how many transformations were applied per batch\n4. Compare CIFAR-10 val accuracy after 5 epochs: no augmentation vs your augmentor\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import torch\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as F\nfrom typing import Callable\n\nclass SurveyImageAugmentor:\n    def __init__(self, strength: float = 0.5) -> None:\n        self.strength  = strength\n        self._applied  = 0\n        self._total    = 0\n\n    def __call__(self, img: torch.Tensor) -> torch.Tensor:\n        # YOUR CODE: apply transforms probabilistically based on self.strength\n        pass\n\n    @property\n    def augmentation_rate(self) -> float:\n        return self._applied / self._total if self._total else 0.0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<details><summary>ðŸ’¡ Hint</summary>\n\nFor each transform, apply it with probability `self.strength`:\n`if torch.rand(1) < self.strength: img = F.hflip(img)`\nIncrement `self._applied` each time a transform is applied, `self._total` for each image.\n\n</details>\n\n<details><summary>âœ… Solution</summary>\n\n```python\nclass SurveyImageAugmentor:\n    def __init__(self, strength=0.5):\n        self.strength=strength; self._applied=self._total=0\n        self.jitter=T.ColorJitter(0.4*strength,0.4*strength,0.4*strength)\n    def __call__(self, img):\n        self._total+=1\n        if torch.rand(1)<self.strength: img=F.hflip(img); self._applied+=1\n        if torch.rand(1)<self.strength*0.8: img=self.jitter(img); self._applied+=1\n        return img\n    @property\n    def augmentation_rate(self): return self._applied/max(1,self._total)\n```\n\n</details>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Chapter 9 Summary\n\n### Key Takeaways\n\n- **Convolutional layers** apply learned filters across the entire image using\n  weight sharing. Early layers detect edges; later layers detect shapes and objects.\n- **`padding=1` with a 3x3 kernel** preserves spatial dimensions.\n  `MaxPool2d(2)` halves them. After three pool layers: 32 â†’ 16 â†’ 8 â†’ 4.\n- **Data augmentation** (random flips, crops, colour jitter) is the single\n  most effective regularisation technique for image models. Always use it.\n- **`OneCycleLR`** is the recommended scheduler for CNNs: it warms up,\n  peaks, then anneals the learning rate in one cycle per training run.\n- **Transfer learning beats training from scratch** on small datasets.\n  Freeze the backbone, train only the head first; then optionally unfreeze\n  all layers at a 10x lower learning rate for further gains.\n- **Feature map visualisation** with forward hooks is the primary tool\n  for understanding what a CNN has learned.\n- **Object detection** models output bounding boxes and class labels.\n  `torchvision.models.detection` provides Faster R-CNN and SSD pre-trained on COCO.\n  Load with `weights=...Weights.DEFAULT` and call `model(image_list)` -- output\n  is a list of dicts with `boxes`, `labels`, and `scores` keys.\n- **Semantic segmentation** assigns a class to every pixel.\n  DeepLabV3 encoder-decoder architecture outputs a `(batch, classes, H, W)` tensor;\n  argmax over the class dimension gives the per-pixel label map.\n- **ImageNet normalisation** (mean=[0.485, 0.456, 0.406]) must be used\n  with all torchvision pre-trained models -- using wrong stats degrades accuracy.\n\n### Model Comparison\n\n| Model | Epochs | Parameters | Test Accuracy |\n|-------|--------|------------|---------------|\n| Custom CNN (from scratch) | 20 | ~300k | reported above |\n| ResNet-18 (head only) | 10 | 11M (512 trainable) | reported above |\n\n---\n\n### What's Next\n\nChapters 10 and 11 complete Part 4. The appendices cover:\nreinforcement learning (App D), SQL for data scientists (App E),\nand Git/GitHub for ML projects (App F).\n\n---\n\n*End of Chapter 9 -- Python for AI/ML*  \n[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n"
    }
  ]
}