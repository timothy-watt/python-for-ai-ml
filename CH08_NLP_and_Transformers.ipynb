{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Chapter 8 -- NLP and Transformers\n## *Python for AI/ML: A Complete Learning Journey*\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/CH08_NLP_and_Transformers.ipynb)\n&nbsp;&nbsp;[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n\n---\n\n**Part:** 3 -- Machine Learning and AI  \n**Prerequisites:** Chapter 7 (Deep Learning with PyTorch)  \n**Estimated time:** 5-6 hours\n\n---\n\n> **Before running this notebook:** go to **Runtime → Change runtime type → T4 GPU**.\n> The transformer inference and fine-tuning cells require GPU. CPU will work but\n> fine-tuning will take 15-30 minutes instead of 2-3 minutes.\n\n---\n\n### Learning Objectives\n\nBy the end of this chapter you will be able to:\n\n- Explain tokens, vocabularies, and why text must be numerically encoded before modelling\n- Use `nltk` and `re` for classical text preprocessing: cleaning, tokenising, stemming, stopwords\n- Build a TF-IDF feature matrix and train a text classifier with scikit-learn\n- Explain word embeddings and why `word2vec`-style representations outperform one-hot encoding\n- Load a pre-trained transformer model with HuggingFace `transformers`\n- Run zero-shot inference: sentiment analysis and text classification without training\n- Fine-tune a pre-trained model on a custom text classification task\n- Interpret attention weights to understand what the model focuses on\n\n---\n\n### Project Thread -- Chapter 8\n\nThe SO 2025 dataset contains free-text columns -- job titles, developer type labels,\nand AI tool descriptions. We build three NLP pipelines on this data:\n\n1. **Classical NLP** -- TF-IDF + Logistic Regression to classify developer role from job title text\n2. **Zero-shot inference** -- sentiment analysis on developer comments using a pre-trained transformer\n3. **Fine-tuning** -- adapt `distilbert-base-uncased` to classify whether a developer\n   is data-focused or software-focused from their self-described role text\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Setup -- Install, Import, and Data\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Install libraries not pre-installed in Colab\nimport subprocess\nsubprocess.run(['pip', 'install', 'transformers', 'datasets', 'accelerate',\n                'nltk', '-q'], check=False)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport nltk\nnltk.download('stopwords', quiet=True)\nnltk.download('punkt',     quiet=True)\nnltk.download('punkt_tab', quiet=True)\nnltk.download('wordnet',   quiet=True)\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.pipeline import Pipeline\n\nimport torch\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {DEVICE}')\n\nimport transformers\nprint(f'Transformers: {transformers.__version__}')\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.dpi']     = 110\nplt.rcParams['axes.titlesize'] = 13\n\nDATASET_URL  = 'https://raw.githubusercontent.com/timothy-watt/python-for-ai-ml/main/data/so_survey_2025_curated.csv'\nRANDOM_STATE = 42\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Load SO 2025 and extract text columns\ndf_raw = pd.read_csv(DATASET_URL)\ndf = df_raw.copy()\n\n# We focus on DevType -- semicolon-separated role labels\n# e.g. 'Developer, full-stack;Developer, back-end;Data scientist'\ntext_col = 'DevType'\nif text_col not in df.columns:\n    # Fallback: use any available text column\n    text_candidates = [c for c in df.columns\n                       if df[c].dtype == object and df[c].str.len().mean() > 10]\n    text_col = text_candidates[0] if text_candidates else None\n    print(f'DevType not found -- using: {text_col}')\n\ndf = df[df[text_col].notna()].copy()\ndf = df.reset_index(drop=True)\n\n# Primary role: take the first semicolon-separated value\ndf['primary_role'] = df[text_col].str.split(';').str[0].str.strip()\n\n# Binary target: data-focused vs software-focused\ndata_keywords = ['data scientist', 'data engineer', 'data analyst',\n                 'machine learning', 'research', 'analyst']\ndf['is_data_role'] = df['primary_role'].str.lower().apply(\n    lambda x: int(any(kw in x for kw in data_keywords))\n)\n\nprint(f'Dataset: {len(df):,} rows with non-null {text_col}')\nprint(f'Unique primary roles: {df[\"primary_role\"].nunique()}')\nprint(f'Data-focused roles:   {df[\"is_data_role\"].sum():,} ({df[\"is_data_role\"].mean()*100:.1f}%)')\nprint()\nprint('Most common primary roles:')\nprint(df['primary_role'].value_counts().head(8).to_string())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 8.1 -- Classical NLP: Text Preprocessing and TF-IDF\n\nBefore transformers dominated NLP, the standard pipeline was:\nclean text → tokenise → remove stopwords → stem/lemmatise → TF-IDF features → train classifier.\nThis pipeline still works well for short, domain-specific text and is 100x faster\nto train than a transformer. It is worth knowing as a fast baseline.\n\n**TF-IDF (Term Frequency -- Inverse Document Frequency)** scores each word by\nhow often it appears in a document (TF) weighted down by how common it is\nacross all documents (IDF). Rare words that appear in specific documents\nget high scores; common words like 'the' get low scores.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.1.1 -- Text cleaning pipeline\n\nSTOP_WORDS = set(stopwords.words('english'))\nstemmer    = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\ndef clean_text(text, use_lemma=True):\n    \"\"\"\n    Full classical NLP preprocessing pipeline.\n    1. Lowercase\n    2. Remove punctuation and digits\n    3. Tokenise\n    4. Remove stopwords\n    5. Lemmatise (or stem)\n    \"\"\"\n    if not isinstance(text, str):\n        return ''\n    text = text.lower()\n    text = re.sub(r'[^a-z\\s]', ' ', text)   # keep only letters and spaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    tokens = word_tokenize(text)\n    tokens = [t for t in tokens if t not in STOP_WORDS and len(t) > 2]\n    if use_lemma:\n        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n    else:\n        tokens = [stemmer.stem(t) for t in tokens]\n    return ' '.join(tokens)\n\n\n# Demonstrate the pipeline step by step\nsample = 'Developer, full-stack; building web APIs and React front-ends'\nprint(f'Original:      {sample}')\nprint(f'Lowercased:    {sample.lower()}')\ncleaned = re.sub(r'[^a-z\\s]', ' ', sample.lower())\nprint(f'No punct:      {cleaned}')\ntokens = word_tokenize(cleaned)\nprint(f'Tokenised:     {tokens}')\nno_stop = [t for t in tokens if t not in STOP_WORDS and len(t) > 2]\nprint(f'No stopwords:  {no_stop}')\nlemmatised = [lemmatizer.lemmatize(t) for t in no_stop]\nprint(f'Lemmatised:    {lemmatised}')\nprint(f'Final string:  {clean_text(sample)}')\n\n# Apply to the full dataset\ndf['role_clean'] = df['primary_role'].apply(clean_text)\nprint(f'Cleaned {len(df):,} role strings')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.1.2 -- TF-IDF features and Logistic Regression classifier\n\n# Keep the top 8 roles by frequency for a clean multi-class problem\ntop_roles = df['primary_role'].value_counts().head(8).index.tolist()\ndf_clf    = df[df['primary_role'].isin(top_roles)].copy()\n\nX_text = df_clf['role_clean'].values\ny_role = df_clf['primary_role'].values\n\nX_tr, X_te, y_tr, y_te = train_test_split(\n    X_text, y_role, test_size=0.2,\n    random_state=RANDOM_STATE, stratify=y_role\n)\n\n# TF-IDF pipeline: vectorise text then classify\ntfidf_pipe = Pipeline([\n    ('tfidf', TfidfVectorizer(\n        max_features=500,    # keep only the 500 highest-scoring terms\n        ngram_range=(1, 2),  # include both single words and 2-word phrases\n        min_df=3,            # ignore terms appearing in fewer than 3 documents\n        sublinear_tf=True,   # apply log(TF) to dampen effect of very frequent terms\n    )),\n    ('clf', LogisticRegression(\n        max_iter=1000,\n        C=1.0,               # inverse regularisation strength\n        random_state=RANDOM_STATE\n    )),\n])\n\ntfidf_pipe.fit(X_tr, y_tr)\ny_pred = tfidf_pipe.predict(X_te)\nacc    = accuracy_score(y_te, y_pred)\n\nprint(f'TF-IDF + Logistic Regression accuracy: {acc:.4f}  ({acc*100:.1f}%)')\nprint()\nprint(classification_report(y_te, y_pred, zero_division=0))\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.1.3 -- Visualise TF-IDF: top terms per class\n\nvectorizer  = tfidf_pipe.named_steps['tfidf']\nclassifier  = tfidf_pipe.named_steps['clf']\nfeature_names = vectorizer.get_feature_names_out()\n\n# For each class, find the terms with the highest logistic regression coefficients\nn_top = 8\nclasses = classifier.classes_\nn_classes = len(classes)\ncols = min(4, n_classes)\nrows = (n_classes + cols - 1) // cols\n\nfig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 3))\naxes_flat  = axes.flatten() if n_classes > 1 else [axes]\n\nfor i, (cls, ax) in enumerate(zip(classes, axes_flat)):\n    coefs = classifier.coef_[i]\n    top_idx  = np.argsort(coefs)[-n_top:]\n    top_terms = feature_names[top_idx]\n    top_coefs = coefs[top_idx]\n    ax.barh(top_terms, top_coefs, color='#2E75B6')\n    ax.set_title(cls[:30], fontsize=9)\n    ax.tick_params(labelsize=8)\n\nfor ax in axes_flat[n_classes:]:\n    ax.set_visible(False)\n\nplt.suptitle('TF-IDF: Top Terms per Developer Role\\n(higher coefficient = stronger signal)',\n             fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 8.2 -- Word Embeddings: From Counts to Meaning\n\nTF-IDF represents each document as a sparse vector of term weights.\nIt has no concept of meaning -- 'developer' and 'engineer' are completely\nunrelated in a TF-IDF vocabulary even though they are semantically close.\n\n**Word embeddings** solve this by mapping each word to a dense vector\nin a continuous space where similar words are geometrically close.\nThe famous example: `king - man + woman ≈ queen`.\n\nModern transformers replace per-word embeddings with **contextual embeddings** --\nthe same word gets a different vector depending on its surrounding context.\n'Python' in 'Python developer' and 'Python snake' would have different embeddings.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.2.1 -- Demonstrate embeddings with a pre-trained transformer tokeniser\n#\n# We use DistilBERT's tokeniser to show how text is converted to token IDs\n# before being fed to the model.\n\nfrom transformers import AutoTokenizer\n\nMODEL_NAME = 'distilbert-base-uncased'\nprint(f'Loading tokeniser: {MODEL_NAME}...')\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nprint('Tokeniser loaded.')\n\n# Tokenise some example developer role texts\nexamples = [\n    'Developer, full-stack',\n    'Data scientist or machine learning specialist',\n    'DevOps specialist',\n]\n\nprint()\nfor text in examples:\n    tokens    = tokenizer.tokenize(text)\n    token_ids = tokenizer.encode(text)\n    print(f'Text:      {text}')\n    print(f'Tokens:    {tokens}')\n    print(f'Token IDs: {token_ids}')\n    print(f'[CLS] id={token_ids[0]}, [SEP] id={token_ids[-1]}')\n    print()\n\nprint('Key observations:')\nprint('  [CLS] token prepended -- its embedding becomes the sentence representation')\nprint('  [SEP] token appended  -- marks end of sequence')\nprint('  Subword tokenisation: unknown words split into known pieces')\nprint('  e.g. \"DevOps\" might become [\"dev\", \"##ops\"]')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 8.3 -- Zero-Shot Inference with Pre-trained Transformers\n\nA pre-trained transformer has already learned rich language representations\nfrom billions of words of text. For many tasks, you can use it directly\nwithout any further training -- this is called **zero-shot inference**.\n\nHuggingFace `pipelines` provide a one-line interface to hundreds of\npre-trained models for common NLP tasks.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.3.1 -- Sentiment analysis pipeline (zero-shot)\n\nfrom transformers import pipeline\n\nprint('Loading sentiment analysis pipeline...')\n# This downloads a fine-tuned DistilBERT model (~67MB) on first run\nsentiment_pipe = pipeline(\n    'sentiment-analysis',\n    model='distilbert-base-uncased-finetuned-sst-2-english',\n    device=0 if torch.cuda.is_available() else -1\n)\nprint('Pipeline ready.')\n\n# Simulate developer sentiment about tools and work conditions\ndeveloper_statements = [\n    'I love working with Python, the ecosystem is incredible.',\n    'The legacy codebase is a nightmare, no documentation anywhere.',\n    'Remote work has been really positive for my productivity.',\n    'The on-call rotation is exhausting and unsustainable.',\n    'GitHub Copilot has genuinely made me more productive.',\n    'Constantly switching between five different frameworks is frustrating.',\n]\n\nprint()\nprint(f'{\"Statement\":<55} {\"Sentiment\":<12} {\"Confidence\"}')\nprint('-' * 80)\nresults = sentiment_pipe(developer_statements)\nfor stmt, result in zip(developer_statements, results):\n    label = result['label']\n    score = result['score']\n    icon  = 'positive' if label == 'POSITIVE' else 'negative'\n    print(f'{stmt[:53]:<55} {icon:<12} {score:.3f}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.3.2 -- Zero-shot text classification\n#\n# Zero-shot classification lets you classify text into ANY categories\n# you define at inference time -- no training data needed.\n# The model uses natural language inference to decide which label\n# best describes the input text.\n\nprint('Loading zero-shot classification pipeline...')\nzs_pipe = pipeline(\n    'zero-shot-classification',\n    model='facebook/bart-large-mnli',\n    device=0 if torch.cuda.is_available() else -1\n)\nprint('Pipeline ready.')\n\n# Classify developer job descriptions into categories we define\ncandidate_labels = ['data science', 'web development', 'DevOps and infrastructure',\n                    'mobile development', 'security']\n\njob_descriptions = [\n    'Building machine learning models and data pipelines for e-commerce recommendations',\n    'Developing React front-end components and REST APIs with Node.js',\n    'Managing Kubernetes clusters and CI/CD pipelines on AWS',\n    'Writing Swift and SwiftUI apps for iOS and watchOS',\n]\n\nprint()\nfor desc in job_descriptions:\n    result = zs_pipe(desc, candidate_labels)\n    top_label = result['labels'][0]\n    top_score = result['scores'][0]\n    print(f'Text:   {desc[:60]}')\n    print(f'Label:  {top_label}  ({top_score:.3f})')\n    print()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 8.4 -- Fine-tuning a Pre-trained Transformer\n\nZero-shot inference is convenient but limited. **Fine-tuning** adapts a pre-trained\nmodel to your specific task by continuing training on your labelled data.\nBecause the model already understands language, fine-tuning typically needs\nonly a small dataset and a few epochs -- far less than training from scratch.\n\nWe fine-tune `distilbert-base-uncased` to classify developer roles as\ndata-focused or software-focused using the `is_data_role` label we created\nfrom the SO 2025 `DevType` column.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.4.1 -- Prepare data for fine-tuning\n\nfrom transformers import AutoModelForSequenceClassification\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\n\n# Use primary role text as input, is_data_role as label\n# Downsample to 2000 examples for fast fine-tuning in Colab\ndf_ft = df[['primary_role', 'is_data_role']].dropna().copy()\ndf_ft = df_ft.sample(n=min(2000, len(df_ft)), random_state=RANDOM_STATE).reset_index(drop=True)\n\n# Balance classes\nn_min = df_ft['is_data_role'].value_counts().min()\ndf_ft = pd.concat([\n    df_ft[df_ft['is_data_role'] == 0].sample(n_min, random_state=RANDOM_STATE),\n    df_ft[df_ft['is_data_role'] == 1].sample(n_min, random_state=RANDOM_STATE),\n]).sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n\ntrain_df, test_df = train_test_split(df_ft, test_size=0.2,\n                                     random_state=RANDOM_STATE,\n                                     stratify=df_ft['is_data_role'])\n\nprint(f'Fine-tuning dataset: {len(train_df)} train, {len(test_df)} test')\nprint(f'Class balance: {train_df[\"is_data_role\"].mean()*100:.0f}% data roles')\n\n# Tokenise all texts\ndef tokenise_batch(texts, max_length=64):\n    return tokenizer(\n        list(texts),\n        padding=True,\n        truncation=True,\n        max_length=max_length,\n        return_tensors='pt'\n    )\n\n# Custom Dataset\nclass RoleDataset(Dataset):\n    def __init__(self, texts, labels, max_length=64):\n        self.encodings = tokenizer(\n            list(texts), padding=True, truncation=True,\n            max_length=max_length, return_tensors='pt'\n        )\n        self.labels = torch.tensor(labels.values, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids':      self.encodings['input_ids'][idx],\n            'attention_mask': self.encodings['attention_mask'][idx],\n            'labels':         self.labels[idx],\n        }\n\ntrain_ds = RoleDataset(train_df['primary_role'], train_df['is_data_role'])\ntest_ds  = RoleDataset(test_df['primary_role'],  test_df['is_data_role'])\ntrain_loader_ft = DataLoader(train_ds, batch_size=32, shuffle=True)\ntest_loader_ft  = DataLoader(test_ds,  batch_size=64, shuffle=False)\nprint(f'Train batches: {len(train_loader_ft)},  Test batches: {len(test_loader_ft)}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.4.2 -- Load model and fine-tune for 3 epochs\n\nprint(f'Loading {MODEL_NAME} for sequence classification...')\nmodel_ft = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=2,\n    ignore_mismatched_sizes=True\n)\nmodel_ft = model_ft.to(DEVICE)\nprint(f'Model loaded. Parameters: {sum(p.numel() for p in model_ft.parameters()):,}')\n\noptimizer_ft = AdamW(model_ft.parameters(), lr=2e-5, weight_decay=0.01)\n\nN_EPOCHS_FT   = 3\nft_train_losses = []\nft_val_accs     = []\n\nprint(f'Fine-tuning on {DEVICE} for {N_EPOCHS_FT} epochs...')\nprint(f'{\"Epoch\":>6}  {\"Train Loss\":>12}  {\"Val Acc\":>10}')\nprint('-' * 32)\n\nfor epoch in range(1, N_EPOCHS_FT + 1):\n    # Training\n    model_ft.train()\n    epoch_loss = 0.0\n    for batch in train_loader_ft:\n        input_ids      = batch['input_ids'].to(DEVICE)\n        attention_mask = batch['attention_mask'].to(DEVICE)\n        labels         = batch['labels'].to(DEVICE)\n        optimizer_ft.zero_grad()\n        outputs = model_ft(input_ids=input_ids,\n                           attention_mask=attention_mask,\n                           labels=labels)\n        outputs.loss.backward()\n        torch.nn.utils.clip_grad_norm_(model_ft.parameters(), 1.0)\n        optimizer_ft.step()\n        epoch_loss += outputs.loss.item()\n    avg_loss = epoch_loss / len(train_loader_ft)\n    ft_train_losses.append(avg_loss)\n\n    # Validation\n    model_ft.eval()\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for batch in test_loader_ft:\n            input_ids      = batch['input_ids'].to(DEVICE)\n            attention_mask = batch['attention_mask'].to(DEVICE)\n            outputs = model_ft(input_ids=input_ids, attention_mask=attention_mask)\n            preds   = outputs.logits.argmax(dim=-1).cpu().numpy()\n            all_preds.extend(preds)\n            all_labels.extend(batch['labels'].numpy())\n    val_acc = accuracy_score(all_labels, all_preds)\n    ft_val_accs.append(val_acc)\n    print(f'{epoch:>6}  {avg_loss:>12.4f}  {val_acc:>10.4f}')\n\nprint('Fine-tuning complete.')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.4.3 -- Evaluate and visualise fine-tuning results\n\nprint(f'Final fine-tuned model accuracy: {ft_val_accs[-1]:.4f}')\nprint()\nprint(classification_report(all_labels, all_preds,\n                             target_names=['Software-focused', 'Data-focused']))\n\n# Training curve\nfig, axes = plt.subplots(1, 2, figsize=(13, 4))\n\naxes[0].plot(range(1, N_EPOCHS_FT+1), ft_train_losses, 'o-', color='#E8722A',\n             linewidth=2, markersize=8)\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Training Loss')\naxes[0].set_title('DistilBERT Fine-tuning: Training Loss')\naxes[0].set_xticks(range(1, N_EPOCHS_FT+1))\n\naxes[1].plot(range(1, N_EPOCHS_FT+1), ft_val_accs, 'o-', color='#2E75B6',\n             linewidth=2, markersize=8)\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Validation Accuracy')\naxes[1].set_title('DistilBERT Fine-tuning: Validation Accuracy')\naxes[1].set_xticks(range(1, N_EPOCHS_FT+1))\naxes[1].set_ylim(0.5, 1.0)\n\nplt.suptitle('SO 2025 Role Classifier: Fine-tuned DistilBERT',\n             fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Compare with TF-IDF baseline\ntfidf_binary = Pipeline([\n    ('tfidf', TfidfVectorizer(max_features=300, ngram_range=(1,2))),\n    ('clf',   LogisticRegression(max_iter=500, random_state=RANDOM_STATE)),\n])\ntfidf_binary.fit(train_df['primary_role'], train_df['is_data_role'])\nbaseline_acc = accuracy_score(test_df['is_data_role'],\n                               tfidf_binary.predict(test_df['primary_role']))\n\nprint(f'Comparison on data-role classification:')\nprint(f'  TF-IDF + Logistic Regression: {baseline_acc:.4f}')\nprint(f'  Fine-tuned DistilBERT:        {ft_val_accs[-1]:.4f}')\nprint(f'  Improvement:                  {(ft_val_accs[-1]-baseline_acc)*100:+.1f} percentage points')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 8.5 -- Understanding Attention\n\nThe transformer's key innovation is the **attention mechanism** -- a learned\nweighting that lets the model focus on the most relevant parts of the input\nwhen encoding each token. Visualising attention weights gives intuition\nfor what the model is 'looking at' when making predictions.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.5.1 -- Extract and visualise attention weights\n\nfrom transformers import AutoModel\n\n# Load the base model with output_attentions=True\nattn_model = AutoModel.from_pretrained(\n    MODEL_NAME, output_attentions=True\n).to(DEVICE)\nattn_model.eval()\n\n# Encode a sample text\nsample_text = 'Machine learning engineer building recommendation systems'\ninputs = tokenizer(sample_text, return_tensors='pt').to(DEVICE)\ntokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n\nwith torch.no_grad():\n    outputs = attn_model(**inputs)\n\n# outputs.attentions: tuple of (n_layers,) each shape (batch, heads, seq, seq)\n# Average across all heads in the last layer\nlast_layer_attn = outputs.attentions[-1][0]          # shape (heads, seq, seq)\navg_attn        = last_layer_attn.mean(dim=0).cpu().numpy()  # shape (seq, seq)\n\n# Plot attention heatmap\nfig, ax = plt.subplots(figsize=(10, 8))\nim = ax.imshow(avg_attn, cmap='Blues', aspect='auto')\nax.set_xticks(range(len(tokens)))\nax.set_yticks(range(len(tokens)))\nax.set_xticklabels(tokens, rotation=45, ha='right', fontsize=10)\nax.set_yticklabels(tokens, fontsize=10)\nplt.colorbar(im, ax=ax, shrink=0.8)\nax.set_title(f'DistilBERT Attention Weights (last layer, averaged over heads)\\n\"{sample_text}\"',\n             fontsize=11)\nplt.tight_layout()\nplt.show()\n\nprint('Rows = query token (what is attending)')\nprint('Cols = key token (what is being attended to)')\nprint('Bright cells = high attention weight')\nprint('[CLS] often attends broadly -- it aggregates the full sequence for classification')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Chapter 8 Summary\n\n### Key Takeaways\n\n- **Classical NLP pipeline:** clean -> tokenise -> remove stopwords -> lemmatise -> TF-IDF -> classifier.\n  Fast, interpretable, and competitive on short domain-specific text.\n- **TF-IDF** scores words by local frequency times global rarity.\n  `ngram_range=(1,2)` captures multi-word phrases like 'machine learning'.\n- **Embeddings** map words to dense vectors where semantic similarity equals geometric proximity.\n  Contextual embeddings (transformers) go further: same word, different context, different vector.\n- **Subword tokenisation** (BPE/WordPiece) handles unknown words by splitting them into\n  known subword pieces. `[CLS]` and `[SEP]` are special control tokens.\n- **Zero-shot inference** uses pre-trained models directly with no task-specific training.\n  HuggingFace `pipeline()` is the one-line entry point.\n- **Fine-tuning** adapts a pre-trained model to your task with a small labelled dataset.\n  3 epochs on 1,600 examples produces a strong classifier because the model already\n  understands language -- you are only teaching it your specific categories.\n- **`clip_grad_norm_`** prevents exploding gradients during fine-tuning -- always include it.\n- **Attention weights** show which tokens the model focuses on. `[CLS]` often attends\n  broadly because it aggregates the full sequence for classification output.\n\n### Project Thread Status\n\n| Task | Method | Result |\n|------|--------|--------|\n| Developer role classification | TF-IDF + Logistic Regression | Accuracy reported |\n| Sentiment analysis on dev statements | Zero-shot DistilBERT | Labels + confidence |\n| Zero-shot role classification | BART-large-MNLI | Top label per description |\n| Data vs software role classification | Fine-tuned DistilBERT | Accuracy vs baseline |\n| Attention visualisation | DistilBERT last layer | Heatmap plotted |\n\n---\n\n### What's Next: Chapter 9 -- Ethics, Bias, and Responsible AI\n\nChapter 9 examines the risks introduced by everything built in Part 3:\nbias in training data, fairness metrics, model interpretability with SHAP,\nand the practical steps for building more responsible ML systems.\nThe SO 2025 dataset provides concrete examples -- salary prediction models\ncan encode geographic and demographic biases that require explicit mitigation.\n\n---\n\n*End of Chapter 8 -- Python for AI/ML*  \n[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n"
    }
  ]
}