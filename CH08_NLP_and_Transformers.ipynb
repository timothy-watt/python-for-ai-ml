{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Chapter 8 -- NLP and Transformers\n## *Python for AI/ML: A Complete Learning Journey*\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/CH08_NLP_and_Transformers.ipynb)\n&nbsp;&nbsp;[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n\n---\n\n**Part:** 3 -- Machine Learning and AI  \n**Prerequisites:** Chapter 7 (Deep Learning with PyTorch)  \n**Estimated time:** 5-6 hours\n\n---\n\n> **Before running this notebook:** go to **Runtime ‚Üí Change runtime type ‚Üí T4 GPU**.\n> The transformer inference and fine-tuning cells require GPU. CPU will work but\n> fine-tuning will take 15-30 minutes instead of 2-3 minutes.\n\n---\n\n### Learning Objectives\n\nBy the end of this chapter you will be able to:\n\n- Explain tokens, vocabularies, and why text must be numerically encoded before modelling\n- Use `nltk` and `re` for classical text preprocessing: cleaning, tokenising, stemming, stopwords\n- Build a TF-IDF feature matrix and train a text classifier with scikit-learn\n- Explain word embeddings and why `word2vec`-style representations outperform one-hot encoding\n- Load a pre-trained transformer model with HuggingFace `transformers`\n- Run zero-shot inference: sentiment analysis and text classification without training\n- Fine-tune a pre-trained model on a custom text classification task\n- Interpret attention weights to understand what the model focuses on\n\n---\n\n### Project Thread -- Chapter 8\n\nThe SO 2025 dataset contains free-text columns -- job titles, developer type labels,\nand AI tool descriptions. We build three NLP pipelines on this data:\n\n1. **Classical NLP** -- TF-IDF + Logistic Regression to classify developer role from job title text\n2. **Zero-shot inference** -- sentiment analysis on developer comments using a pre-trained transformer\n3. **Fine-tuning** -- adapt `distilbert-base-uncased` to classify whether a developer\n   is data-focused or software-focused from their self-described role text\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Setup -- Install, Import, and Data\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Install libraries not pre-installed in Colab\nimport subprocess\nsubprocess.run(['pip', 'install', 'transformers', 'datasets', 'accelerate',\n                'nltk', '-q'], check=False)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport nltk\nnltk.download('stopwords', quiet=True)\nnltk.download('punkt',     quiet=True)\nnltk.download('punkt_tab', quiet=True)\nnltk.download('wordnet',   quiet=True)\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.pipeline import Pipeline\n\nimport torch\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {DEVICE}')\n\nimport transformers\nprint(f'Transformers: {transformers.__version__}')\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.dpi']     = 110\nplt.rcParams['axes.titlesize'] = 13\n\nDATASET_URL  = 'https://raw.githubusercontent.com/timothy-watt/python-for-ai-ml/main/data/so_survey_2025_curated.csv'\nRANDOM_STATE = 42\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Load SO 2025 and extract text columns\ndf_raw = pd.read_csv(DATASET_URL)\ndf = df_raw.copy()\n\n# We focus on DevType -- semicolon-separated role labels\n# e.g. 'Developer, full-stack;Developer, back-end;Data scientist'\ntext_col = 'DevType'\nif text_col not in df.columns:\n    # Fallback: use any available text column\n    text_candidates = [c for c in df.columns\n                       if df[c].dtype == object and df[c].str.len().mean() > 10]\n    text_col = text_candidates[0] if text_candidates else None\n    print(f'DevType not found -- using: {text_col}')\n\ndf = df[df[text_col].notna()].copy()\ndf = df.reset_index(drop=True)\n\n# Primary role: take the first semicolon-separated value\ndf['primary_role'] = df[text_col].str.split(';').str[0].str.strip()\n\n# Binary target: data-focused vs software-focused\ndata_keywords = ['data scientist', 'data engineer', 'data analyst',\n                 'machine learning', 'research', 'analyst']\ndf['is_data_role'] = df['primary_role'].str.lower().apply(\n    lambda x: int(any(kw in x for kw in data_keywords))\n)\n\nprint(f'Dataset: {len(df):,} rows with non-null {text_col}')\nprint(f'Unique primary roles: {df[\"primary_role\"].nunique()}')\nprint(f'Data-focused roles:   {df[\"is_data_role\"].sum():,} ({df[\"is_data_role\"].mean()*100:.1f}%)')\nprint()\nprint('Most common primary roles:')\nprint(df['primary_role'].value_counts().head(8).to_string())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 8.1 -- Classical NLP: Text Preprocessing and TF-IDF\n\nBefore transformers dominated NLP, the standard pipeline was:\nclean text ‚Üí tokenise ‚Üí remove stopwords ‚Üí stem/lemmatise ‚Üí TF-IDF features ‚Üí train classifier.\nThis pipeline still works well for short, domain-specific text and is 100x faster\nto train than a transformer. It is worth knowing as a fast baseline.\n\n**TF-IDF (Term Frequency -- Inverse Document Frequency)** scores each word by\nhow often it appears in a document (TF) weighted down by how common it is\nacross all documents (IDF). Rare words that appear in specific documents\nget high scores; common words like 'the' get low scores.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.1.1 -- Text cleaning pipeline\n\nSTOP_WORDS = set(stopwords.words('english'))\nstemmer    = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\ndef clean_text(text: str, use_lemma: bool = True) -> str:\n    \"\"\"\n    Full classical NLP preprocessing pipeline.\n    1. Lowercase\n    2. Remove punctuation and digits\n    3. Tokenise\n    4. Remove stopwords\n    5. Lemmatise (or stem)\n    \"\"\"\n    if not isinstance(text, str):\n        return ''\n    text = text.lower()\n    text = re.sub(r'[^a-z\\s]', ' ', text)   # keep only letters and spaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    tokens = word_tokenize(text)\n    tokens = [t for t in tokens if t not in STOP_WORDS and len(t) > 2]\n    if use_lemma:\n        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n    else:\n        tokens = [stemmer.stem(t) for t in tokens]\n    return ' '.join(tokens)\n\n\n# Demonstrate the pipeline step by step\nsample = 'Developer, full-stack; building web APIs and React front-ends'\nprint(f'Original:      {sample}')\nprint(f'Lowercased:    {sample.lower()}')\ncleaned = re.sub(r'[^a-z\\s]', ' ', sample.lower())\nprint(f'No punct:      {cleaned}')\ntokens = word_tokenize(cleaned)\nprint(f'Tokenised:     {tokens}')\nno_stop = [t for t in tokens if t not in STOP_WORDS and len(t) > 2]\nprint(f'No stopwords:  {no_stop}')\nlemmatised = [lemmatizer.lemmatize(t) for t in no_stop]\nprint(f'Lemmatised:    {lemmatised}')\nprint(f'Final string:  {clean_text(sample)}')\n\n# Apply to the full dataset\ndf['role_clean'] = df['primary_role'].apply(clean_text)\nprint(f'Cleaned {len(df):,} role strings')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.1.2 -- TF-IDF features and Logistic Regression classifier\n\n# Keep the top 8 roles by frequency for a clean multi-class problem\ntop_roles = df['primary_role'].value_counts().head(8).index.tolist()\ndf_clf    = df[df['primary_role'].isin(top_roles)].copy()\n\nX_text = df_clf['role_clean'].values\ny_role = df_clf['primary_role'].values\n\nX_tr, X_te, y_tr, y_te = train_test_split(\n    X_text, y_role, test_size=0.2,\n    random_state=RANDOM_STATE, stratify=y_role\n)\n\n# TF-IDF pipeline: vectorise text then classify\ntfidf_pipe = Pipeline([\n    ('tfidf', TfidfVectorizer(\n        max_features=500,    # keep only the 500 highest-scoring terms\n        ngram_range=(1, 2),  # include both single words and 2-word phrases\n        min_df=3,            # ignore terms appearing in fewer than 3 documents\n        sublinear_tf=True,   # apply log(TF) to dampen effect of very frequent terms\n    )),\n    ('clf', LogisticRegression(\n        max_iter=1000,\n        C=1.0,               # inverse regularisation strength\n        random_state=RANDOM_STATE\n    )),\n])\n\ntfidf_pipe.fit(X_tr, y_tr)\ny_pred = tfidf_pipe.predict(X_te)\nacc    = accuracy_score(y_te, y_pred)\n\nprint(f'TF-IDF + Logistic Regression accuracy: {acc:.4f}  ({acc*100:.1f}%)')\nprint()\nprint(classification_report(y_te, y_pred, zero_division=0))\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.1.3 -- Visualise TF-IDF: top terms per class\n\nvectorizer  = tfidf_pipe.named_steps['tfidf']\nclassifier  = tfidf_pipe.named_steps['clf']\nfeature_names = vectorizer.get_feature_names_out()\n\n# For each class, find the terms with the highest logistic regression coefficients\nn_top = 8\nclasses = classifier.classes_\nn_classes = len(classes)\ncols = min(4, n_classes)\nrows = (n_classes + cols - 1) // cols\n\nfig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 3))\naxes_flat  = axes.flatten() if n_classes > 1 else [axes]\n\nfor i, (cls, ax) in enumerate(zip(classes, axes_flat)):\n    coefs = classifier.coef_[i]\n    top_idx  = np.argsort(coefs)[-n_top:]\n    top_terms = feature_names[top_idx]\n    top_coefs = coefs[top_idx]\n    ax.barh(top_terms, top_coefs, color='#2E75B6')\n    ax.set_title(cls[:30], fontsize=9)\n    ax.tick_params(labelsize=8)\n\nfor ax in axes_flat[n_classes:]:\n    ax.set_visible(False)\n\nplt.suptitle('TF-IDF: Top Terms per Developer Role\\n(higher coefficient = stronger signal)',\n             fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 8.2 -- Word Embeddings: From Counts to Meaning\n\nTF-IDF represents each document as a sparse vector of term weights.\nIt has no concept of meaning -- 'developer' and 'engineer' are completely\nunrelated in a TF-IDF vocabulary even though they are semantically close.\n\n**Word embeddings** solve this by mapping each word to a dense vector\nin a continuous space where similar words are geometrically close.\nThe famous example: `king - man + woman ‚âà queen`.\n\nModern transformers replace per-word embeddings with **contextual embeddings** --\nthe same word gets a different vector depending on its surrounding context.\n'Python' in 'Python developer' and 'Python snake' would have different embeddings.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.2.1 -- Demonstrate embeddings with a pre-trained transformer tokeniser\n#\n# We use DistilBERT's tokeniser to show how text is converted to token IDs\n# before being fed to the model.\n\nfrom transformers import AutoTokenizer\n\nMODEL_NAME = 'distilbert-base-uncased'\nprint(f'Loading tokeniser: {MODEL_NAME}...')\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nprint('Tokeniser loaded.')\n\n# Tokenise some example developer role texts\nexamples = [\n    'Developer, full-stack',\n    'Data scientist or machine learning specialist',\n    'DevOps specialist',\n]\n\nprint()\nfor text in examples:\n    tokens    = tokenizer.tokenize(text)\n    token_ids = tokenizer.encode(text)\n    print(f'Text:      {text}')\n    print(f'Tokens:    {tokens}')\n    print(f'Token IDs: {token_ids}')\n    print(f'[CLS] id={token_ids[0]}, [SEP] id={token_ids[-1]}')\n    print()\n\nprint('Key observations:')\nprint('  [CLS] token prepended -- its embedding becomes the sentence representation')\nprint('  [SEP] token appended  -- marks end of sequence')\nprint('  Subword tokenisation: unknown words split into known pieces')\nprint('  e.g. \"DevOps\" might become [\"dev\", \"##ops\"]')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 8.3 -- Zero-Shot Inference with Pre-trained Transformers\n\nA pre-trained transformer has already learned rich language representations\nfrom billions of words of text. For many tasks, you can use it directly\nwithout any further training -- this is called **zero-shot inference**.\n\nHuggingFace `pipelines` provide a one-line interface to hundreds of\npre-trained models for common NLP tasks.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.3.1 -- Sentiment analysis pipeline (zero-shot)\n\nfrom transformers import pipeline\n\nprint('Loading sentiment analysis pipeline...')\n# This downloads a fine-tuned DistilBERT model (~67MB) on first run\nsentiment_pipe = pipeline(\n    'sentiment-analysis',\n    model='distilbert-base-uncased-finetuned-sst-2-english',\n    device=0 if torch.cuda.is_available() else -1\n)\nprint('Pipeline ready.')\n\n# Simulate developer sentiment about tools and work conditions\ndeveloper_statements = [\n    'I love working with Python, the ecosystem is incredible.',\n    'The legacy codebase is a nightmare, no documentation anywhere.',\n    'Remote work has been really positive for my productivity.',\n    'The on-call rotation is exhausting and unsustainable.',\n    'GitHub Copilot has genuinely made me more productive.',\n    'Constantly switching between five different frameworks is frustrating.',\n]\n\nprint()\nprint(f'{\"Statement\":<55} {\"Sentiment\":<12} {\"Confidence\"}')\nprint('-' * 80)\nresults = sentiment_pipe(developer_statements)\nfor stmt, result in zip(developer_statements, results):\n    label = result['label']\n    score = result['score']\n    icon  = 'positive' if label == 'POSITIVE' else 'negative'\n    print(f'{stmt[:53]:<55} {icon:<12} {score:.3f}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.3.2 -- Zero-shot text classification\n#\n# Zero-shot classification lets you classify text into ANY categories\n# you define at inference time -- no training data needed.\n# The model uses natural language inference to decide which label\n# best describes the input text.\n\nprint('Loading zero-shot classification pipeline...')\nzs_pipe = pipeline(\n    'zero-shot-classification',\n    model='facebook/bart-large-mnli',\n    device=0 if torch.cuda.is_available() else -1\n)\nprint('Pipeline ready.')\n\n# Classify developer job descriptions into categories we define\ncandidate_labels = ['data science', 'web development', 'DevOps and infrastructure',\n                    'mobile development', 'security']\n\njob_descriptions = [\n    'Building machine learning models and data pipelines for e-commerce recommendations',\n    'Developing React front-end components and REST APIs with Node.js',\n    'Managing Kubernetes clusters and CI/CD pipelines on AWS',\n    'Writing Swift and SwiftUI apps for iOS and watchOS',\n]\n\nprint()\nfor desc in job_descriptions:\n    result = zs_pipe(desc, candidate_labels)\n    top_label = result['labels'][0]\n    top_score = result['scores'][0]\n    print(f'Text:   {desc[:60]}')\n    print(f'Label:  {top_label}  ({top_score:.3f})')\n    print()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 8.4 -- Fine-tuning a Pre-trained Transformer\n\nZero-shot inference is convenient but limited. **Fine-tuning** adapts a pre-trained\nmodel to your specific task by continuing training on your labelled data.\nBecause the model already understands language, fine-tuning typically needs\nonly a small dataset and a few epochs -- far less than training from scratch.\n\nWe fine-tune `distilbert-base-uncased` to classify developer roles as\ndata-focused or software-focused using the `is_data_role` label we created\nfrom the SO 2025 `DevType` column.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.4.1 -- Prepare data for fine-tuning\n\nfrom transformers import AutoModelForSequenceClassification\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\n\n# Use primary role text as input, is_data_role as label\n# Downsample to 2000 examples for fast fine-tuning in Colab\ndf_ft = df[['primary_role', 'is_data_role']].dropna().copy()\ndf_ft = df_ft.sample(n=min(2000, len(df_ft)), random_state=RANDOM_STATE).reset_index(drop=True)\n\n# Balance classes\nn_min = df_ft['is_data_role'].value_counts().min()\ndf_ft = pd.concat([\n    df_ft[df_ft['is_data_role'] == 0].sample(n_min, random_state=RANDOM_STATE),\n    df_ft[df_ft['is_data_role'] == 1].sample(n_min, random_state=RANDOM_STATE),\n]).sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n\ntrain_df, test_df = train_test_split(df_ft, test_size=0.2,\n                                     random_state=RANDOM_STATE,\n                                     stratify=df_ft['is_data_role'])\n\nprint(f'Fine-tuning dataset: {len(train_df)} train, {len(test_df)} test')\nprint(f'Class balance: {train_df[\"is_data_role\"].mean()*100:.0f}% data roles')\n\n# Tokenise all texts\ndef tokenise_batch(texts: list[str], max_length: int = 64) -> dict:\n    return tokenizer(\n        list(texts),\n        padding=True,\n        truncation=True,\n        max_length=max_length,\n        return_tensors='pt'\n    )\n\n# Custom Dataset\nclass RoleDataset(Dataset):\n    def __init__(self, texts: list[str], labels: list[int], max_length: int = 64) -> None:\n        self.encodings = tokenizer(\n            list(texts), padding=True, truncation=True,\n            max_length=max_length, return_tensors='pt'\n        )\n        self.labels = torch.tensor(labels.values, dtype=torch.long)\n\n    def __len__(self) -> int:\n        return len(self.labels)\n\n    def __getitem__(self, idx: int) -> dict:\n        return {\n            'input_ids':      self.encodings['input_ids'][idx],\n            'attention_mask': self.encodings['attention_mask'][idx],\n            'labels':         self.labels[idx],\n        }\n\ntrain_ds = RoleDataset(train_df['primary_role'], train_df['is_data_role'])\ntest_ds  = RoleDataset(test_df['primary_role'],  test_df['is_data_role'])\ntrain_loader_ft = DataLoader(train_ds, batch_size=32, shuffle=True)\ntest_loader_ft  = DataLoader(test_ds,  batch_size=64, shuffle=False)\nprint(f'Train batches: {len(train_loader_ft)},  Test batches: {len(test_loader_ft)}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.4.2 -- Load model and fine-tune for 3 epochs\n\nprint(f'Loading {MODEL_NAME} for sequence classification...')\nmodel_ft = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=2,\n    ignore_mismatched_sizes=True\n)\nmodel_ft = model_ft.to(DEVICE)\nprint(f'Model loaded. Parameters: {sum(p.numel() for p in model_ft.parameters()):,}')\n\noptimizer_ft = AdamW(model_ft.parameters(), lr=2e-5, weight_decay=0.01)\n\nN_EPOCHS_FT   = 3\nft_train_losses = []\nft_val_accs     = []\n\nprint(f'Fine-tuning on {DEVICE} for {N_EPOCHS_FT} epochs...')\nprint(f'{\"Epoch\":>6}  {\"Train Loss\":>12}  {\"Val Acc\":>10}')\nprint('-' * 32)\n\nfor epoch in range(1, N_EPOCHS_FT + 1):\n    # Training\n    model_ft.train()\n    epoch_loss = 0.0\n    for batch in train_loader_ft:\n        input_ids      = batch['input_ids'].to(DEVICE)\n        attention_mask = batch['attention_mask'].to(DEVICE)\n        labels         = batch['labels'].to(DEVICE)\n        optimizer_ft.zero_grad()\n        outputs = model_ft(input_ids=input_ids,\n                           attention_mask=attention_mask,\n                           labels=labels)\n        outputs.loss.backward()\n        torch.nn.utils.clip_grad_norm_(model_ft.parameters(), 1.0)\n        optimizer_ft.step()\n        epoch_loss += outputs.loss.item()\n    avg_loss = epoch_loss / len(train_loader_ft)\n    ft_train_losses.append(avg_loss)\n\n    # Validation\n    model_ft.eval()\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for batch in test_loader_ft:\n            input_ids      = batch['input_ids'].to(DEVICE)\n            attention_mask = batch['attention_mask'].to(DEVICE)\n            outputs = model_ft(input_ids=input_ids, attention_mask=attention_mask)\n            preds   = outputs.logits.argmax(dim=-1).cpu().numpy()\n            all_preds.extend(preds)\n            all_labels.extend(batch['labels'].numpy())\n    val_acc = accuracy_score(all_labels, all_preds)\n    ft_val_accs.append(val_acc)\n    print(f'{epoch:>6}  {avg_loss:>12.4f}  {val_acc:>10.4f}')\n\nprint('Fine-tuning complete.')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.4.3 -- Evaluate and visualise fine-tuning results\n\nprint(f'Final fine-tuned model accuracy: {ft_val_accs[-1]:.4f}')\nprint()\nprint(classification_report(all_labels, all_preds,\n                             target_names=['Software-focused', 'Data-focused']))\n\n# Training curve\nfig, axes = plt.subplots(1, 2, figsize=(13, 4))\n\naxes[0].plot(range(1, N_EPOCHS_FT+1), ft_train_losses, 'o-', color='#E8722A',\n             linewidth=2, markersize=8)\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Training Loss')\naxes[0].set_title('DistilBERT Fine-tuning: Training Loss')\naxes[0].set_xticks(range(1, N_EPOCHS_FT+1))\n\naxes[1].plot(range(1, N_EPOCHS_FT+1), ft_val_accs, 'o-', color='#2E75B6',\n             linewidth=2, markersize=8)\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Validation Accuracy')\naxes[1].set_title('DistilBERT Fine-tuning: Validation Accuracy')\naxes[1].set_xticks(range(1, N_EPOCHS_FT+1))\naxes[1].set_ylim(0.5, 1.0)\n\nplt.suptitle('SO 2025 Role Classifier: Fine-tuned DistilBERT',\n             fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Compare with TF-IDF baseline\ntfidf_binary = Pipeline([\n    ('tfidf', TfidfVectorizer(max_features=300, ngram_range=(1,2))),\n    ('clf',   LogisticRegression(max_iter=500, random_state=RANDOM_STATE)),\n])\ntfidf_binary.fit(train_df['primary_role'], train_df['is_data_role'])\nbaseline_acc = accuracy_score(test_df['is_data_role'],\n                               tfidf_binary.predict(test_df['primary_role']))\n\nprint(f'Comparison on data-role classification:')\nprint(f'  TF-IDF + Logistic Regression: {baseline_acc:.4f}')\nprint(f'  Fine-tuned DistilBERT:        {ft_val_accs[-1]:.4f}')\nprint(f'  Improvement:                  {(ft_val_accs[-1]-baseline_acc)*100:+.1f} percentage points')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 8.5 -- Understanding Attention\n\nThe transformer's key innovation is the **attention mechanism** -- a learned\nweighting that lets the model focus on the most relevant parts of the input\nwhen encoding each token. Visualising attention weights gives intuition\nfor what the model is 'looking at' when making predictions.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.5.1 -- Extract and visualise attention weights\n\nfrom transformers import AutoModel\n\n# Load the base model with output_attentions=True\nattn_model = AutoModel.from_pretrained(\n    MODEL_NAME, output_attentions=True\n).to(DEVICE)\nattn_model.eval()\n\n# Encode a sample text\nsample_text = 'Machine learning engineer building recommendation systems'\ninputs = tokenizer(sample_text, return_tensors='pt').to(DEVICE)\ntokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n\nwith torch.no_grad():\n    outputs = attn_model(**inputs)\n\n# outputs.attentions: tuple of (n_layers,) each shape (batch, heads, seq, seq)\n# Average across all heads in the last layer\nlast_layer_attn = outputs.attentions[-1][0]          # shape (heads, seq, seq)\navg_attn        = last_layer_attn.mean(dim=0).cpu().numpy()  # shape (seq, seq)\n\n# Plot attention heatmap\nfig, ax = plt.subplots(figsize=(10, 8))\nim = ax.imshow(avg_attn, cmap='Blues', aspect='auto')\nax.set_xticks(range(len(tokens)))\nax.set_yticks(range(len(tokens)))\nax.set_xticklabels(tokens, rotation=45, ha='right', fontsize=10)\nax.set_yticklabels(tokens, fontsize=10)\nplt.colorbar(im, ax=ax, shrink=0.8)\nax.set_title(f'DistilBERT Attention Weights (last layer, averaged over heads)\\n\"{sample_text}\"',\n             fontsize=11)\nplt.tight_layout()\nplt.show()\n\nprint('Rows = query token (what is attending)')\nprint('Cols = key token (what is being attended to)')\nprint('Bright cells = high attention weight')\nprint('[CLS] often attends broadly -- it aggregates the full sequence for classification')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 8.6 -- Retrieval-Augmented Generation (RAG)\n\n**The problem with fine-tuning for knowledge:** fine-tuning teaches a model\n*how* to behave, not *what* to know. If you fine-tune on your company's\ndocumentation, the knowledge is frozen at training time and expensive to update.\n\n**RAG** solves this by splitting the problem in two:\n\n1. **Retrieve** -- at query time, search a document store for the most relevant chunks\n2. **Generate** -- pass the retrieved chunks as context to a language model and ask it\n   to answer using that context\n\nThe documents live outside the model and can be updated without retraining.\nThis is the dominant architecture for production Q&A systems over private documents.\n\n```\n  Query\n    |\n    v\n  Embed query ‚îÄ‚îÄ> Vector similarity search ‚îÄ‚îÄ> Top-k document chunks\n                       (FAISS / ChromaDB)              |\n                                                        v\n                                             [context + query] ‚îÄ‚îÄ> LLM ‚îÄ‚îÄ> Answer\n```\n\n**Our implementation:**\n- **Corpus:** SO 2025 developer job descriptions (synthetic, from the dataset)\n- **Embeddings:** `sentence-transformers/all-MiniLM-L6-v2` (~80MB, fast, high quality)\n- **Vector store:** FAISS (Facebook AI Similarity Search, CPU-only, runs locally)\n- **Generation:** HuggingFace `pipeline` with a small generative model\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.6.1 -- Install RAG dependencies\n\nimport subprocess\nsubprocess.run(['pip', 'install', 'sentence-transformers', 'faiss-cpu', '-q'], check=False)\n\nimport numpy as np\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer\nimport faiss\n\nprint(f'sentence-transformers and faiss-cpu ready')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.6.2 -- Build the document corpus from SO 2025\n#\n# We construct synthetic job-description documents from the survey fields\n# so the RAG system has something meaningful to retrieve.\n\ndf_rag = pd.read_csv(DATASET_URL)\n\ndef build_doc(row: \"pd.Series\") -> str | None:\n    \"\"\"Build a short text document from a survey row.\"\"\"\n    parts = []\n    if pd.notna(row.get('DevType')):\n        parts.append(f\"Role: {row['DevType']}\")\n    if pd.notna(row.get('LanguageHaveWorkedWith')):\n        langs = row['LanguageHaveWorkedWith'].replace(';', ', ')\n        parts.append(f\"Languages: {langs}\")\n    if pd.notna(row.get('Country')):\n        parts.append(f\"Country: {row['Country']}\")\n    if pd.notna(row.get('EdLevel')):\n        parts.append(f\"Education: {row['EdLevel']}\")\n    if pd.notna(row.get('YearsCodePro')):\n        parts.append(f\"Professional coding experience: {row['YearsCodePro']} years\")\n    if pd.notna(row.get('ConvertedCompYearly')):\n        parts.append(f\"Annual compensation: ${float(row['ConvertedCompYearly']):,.0f}\")\n    if pd.notna(row.get('AIToolCurrently')):\n        tools = row['AIToolCurrently'].replace(';', ', ')\n        parts.append(f\"AI tools currently used: {tools}\")\n    return ' | '.join(parts) if parts else None\n\ndf_rag['document'] = df_rag.apply(build_doc, axis=1)\ndocs = df_rag['document'].dropna().tolist()\n\n# Use a representative 2000-doc subset for speed\nimport random\nrandom.seed(42)\ndocs_subset = random.sample(docs, min(2000, len(docs)))\n\nprint(f'Documents built: {len(docs_subset):,}')\nprint(f'Sample document:')\nprint(f'  {docs_subset[0]}')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.6.3 -- Embed all documents and build a FAISS vector index\n\n# Load the embedding model\n# all-MiniLM-L6-v2: 22M parameters, 384-dim embeddings, very fast\nprint('Loading sentence embedding model...')\nembedder = SentenceTransformer('all-MiniLM-L6-v2')\n\nprint(f'Embedding {len(docs_subset):,} documents...')\ndoc_embeddings = embedder.encode(\n    docs_subset,\n    batch_size=128,\n    show_progress_bar=True,\n    convert_to_numpy=True\n)\n\nprint(f'Embedding matrix shape: {doc_embeddings.shape}')\nprint(f'  {doc_embeddings.shape[0]} documents x {doc_embeddings.shape[1]} dimensions')\n\n# Build FAISS index\n# IndexFlatIP: exact inner-product (cosine) search\n# Normalise first so inner product == cosine similarity\nfaiss.normalize_L2(doc_embeddings)\ndimension = doc_embeddings.shape[1]\nindex = faiss.IndexFlatIP(dimension)\nindex.add(doc_embeddings)\n\nprint(f'FAISS index built: {index.ntotal:,} vectors indexed')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.6.4 -- Retrieval: find the most relevant documents for a query\n\ndef retrieve(query: str, k: int = 3, verbose: bool = True) -> list[dict]:\n    \"\"\"Embed the query and return the top-k most similar documents.\"\"\"\n    q_emb = embedder.encode([query], convert_to_numpy=True)\n    faiss.normalize_L2(q_emb)\n    scores, indices = index.search(q_emb, k)\n    results = []\n    for rank, (score, idx) in enumerate(zip(scores[0], indices[0]), 1):\n        results.append({'rank': rank, 'score': float(score), 'doc': docs_subset[idx]})\n        if verbose:\n            print(f'[{rank}] Score={score:.4f}')\n            print(f'     {docs_subset[idx][:120]}...' if len(docs_subset[idx]) > 120\n                  else f'     {docs_subset[idx]}')\n    return results\n\n\ntest_queries = [\n    'senior Python developer with machine learning experience',\n    'frontend engineer using JavaScript and TypeScript',\n    'data scientist in Germany using AI tools',\n]\n\nfor query in test_queries:\n    print(f'Query: \"{query}\"')\n    retrieve(query, k=2)\n    print()\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.6.5 -- Full RAG pipeline: retrieve + generate answer\n#\n# We use the retrieved documents as context and ask a model to\n# synthesise an answer grounded in that context.\n# Using flan-t5-base: small (250M params), instruction-following, no GPU needed.\n\nfrom transformers import pipeline as hf_pipeline\n\nprint('Loading generative model (flan-t5-base, ~1GB)...')\ngenerator = hf_pipeline(\n    'text2text-generation',\n    model='google/flan-t5-base',\n    max_new_tokens=150\n)\n\ndef rag_answer(question: str, k: int = 3) -> tuple[str, list]:\n    \"\"\"\n    Full RAG pipeline:\n    1. Retrieve top-k relevant documents\n    2. Build a context-augmented prompt\n    3. Generate an answer grounded in the retrieved context\n    \"\"\"\n    # Step 1: retrieve\n    results = retrieve(question, k=k, verbose=False)\n    context = '\\n'.join([f'- {r[\"doc\"]}' for r in results])\n\n    # Step 2: build prompt with context\n    prompt = (\n        f'Based on the following developer profiles from the Stack Overflow 2025 survey:\\n'\n        f'{context}\\n\\n'\n        f'Answer this question: {question}'\n    )\n\n    # Step 3: generate\n    response = generator(prompt)[0]['generated_text'].strip()\n    return response, results\n\n\nrag_questions = [\n    'What programming languages are commonly used by senior developers with high salaries?',\n    'What AI tools do data scientists typically use?',\n]\n\nfor question in rag_questions:\n    print(f'Question: {question}')\n    answer, sources = rag_answer(question, k=3)\n    print(f'Answer:   {answer}')\n    print(f'Sources:  {len(sources)} documents retrieved')\n    for s in sources:\n        print(f'  [{s[\"rank\"]}] {s[\"doc\"][:80]}...')\n    print()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 8.7 -- API-Based LLM Integration\n\nSection 8.6 built a RAG system using a small local model (Flan-T5).\nIn production, most teams call a hosted LLM API instead ‚Äî larger models,\nno GPU required, better reasoning, pay-per-token pricing.\n\n**The pattern is identical to local RAG:**\nretrieve relevant context ‚Üí inject into a prompt ‚Üí call the API ‚Üí return the answer.\nThe only change is the generation step.\n\nWe implement this in a **provider-agnostic** way: a single `LLMClient` class\nthat works with OpenAI, Anthropic, or any OpenAI-compatible endpoint\n(Groq, Together AI, Ollama, etc.) by swapping one parameter.\n\n**API key handling:** keys are never hardcoded. We use environment variables\n(`OPENAI_API_KEY`, `ANTHROPIC_API_KEY`) or Colab Secrets (recommended).\n\n**Cost awareness:** always log token usage. A single RAG query with 3 retrieved\nchunks is typically 500-1,500 tokens ‚Äî a fraction of a cent at current pricing.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.7.1 -- Provider-agnostic LLM client\n\nimport os\nimport json as json_lib\nfrom typing import Optional\n\nclass LLMClient:\n    \"\"\"\n    Thin wrapper around LLM APIs.\n    Supports OpenAI and Anthropic with a unified interface.\n    Falls back to a mock response when no API key is set (for demo purposes).\n    \"\"\"\n\n    PROVIDERS = ['openai', 'anthropic', 'mock']\n\n    def __init__(self, provider: str = 'mock', model: Optional[str] = None):\n        self.provider = provider.lower()\n        assert self.provider in self.PROVIDERS, f'Unknown provider: {provider}'\n\n        # Auto-detect available provider from environment\n        if self.provider == 'mock':\n            if os.environ.get('OPENAI_API_KEY'):\n                self.provider = 'openai'\n            elif os.environ.get('ANTHROPIC_API_KEY'):\n                self.provider = 'anthropic'\n\n        # Default models\n        if model:\n            self.model = model\n        elif self.provider == 'openai':\n            self.model = 'gpt-4o-mini'\n        elif self.provider == 'anthropic':\n            self.model = 'claude-haiku-4-5-20251001'\n        else:\n            self.model = 'mock'\n\n        self.total_tokens = 0\n        print(f'LLMClient ready: provider={self.provider}, model={self.model}')\n\n    def chat(self, system: str, user: str, max_tokens: int = 300) -> str:\n        \"\"\"Send a chat request and return the response text.\"\"\"\n        if self.provider == 'openai':\n            return self._call_openai(system, user, max_tokens)\n        elif self.provider == 'anthropic':\n            return self._call_anthropic(system, user, max_tokens)\n        else:\n            return self._mock_response(user)\n\n    def _call_openai(self, system: str, user: str, max_tokens: int) -> str:\n        try:\n            import openai\n            client = openai.OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n            resp   = client.chat.completions.create(\n                model=self.model,\n                messages=[{'role': 'system', 'content': system},\n                           {'role': 'user',   'content': user}],\n                max_tokens=max_tokens,\n                temperature=0.2,\n            )\n            self.total_tokens += resp.usage.total_tokens\n            return resp.choices[0].message.content.strip()\n        except Exception as e:\n            return f'[OpenAI error: {e}]'\n\n    def _call_anthropic(self, system: str, user: str, max_tokens: int) -> str:\n        try:\n            import anthropic\n            client = anthropic.Anthropic(api_key=os.environ['ANTHROPIC_API_KEY'])\n            resp   = client.messages.create(\n                model=self.model,\n                system=system,\n                messages=[{'role': 'user', 'content': user}],\n                max_tokens=max_tokens,\n            )\n            self.total_tokens += resp.usage.input_tokens + resp.usage.output_tokens\n            return resp.content[0].text.strip()\n        except Exception as e:\n            return f'[Anthropic error: {e}]'\n\n    def _mock_response(self, user: str) -> str:\n        \"\"\"Return a canned response when no API key is available.\"\"\"\n        return (\n            '[MOCK RESPONSE -- set OPENAI_API_KEY or ANTHROPIC_API_KEY to use a real LLM]\\n'\n            f'Your question was: {user[:100]}...\\n'\n            'In production, a real LLM would synthesise the retrieved context '\n            'into a coherent answer here.'\n        )\n\n    def usage_summary(self) -> None:\n        print(f'Total tokens used this session: {self.total_tokens:,}')\n        # Approximate cost at mid-2025 pricing\n        cost_map = {'gpt-4o-mini': 0.15/1e6, 'claude-haiku-4-5-20251001': 0.25/1e6}\n        rate = cost_map.get(self.model, 0.5/1e6)\n        print(f'Estimated cost: ${self.total_tokens * rate:.6f} '\n              f'(at ${rate*1e6:.2f}/1M tokens)')\n\n\n# Instantiate -- auto-detects provider from environment\nllm = LLMClient()\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.7.2 -- API RAG: combine FAISS retrieval with LLM generation\n#\n# Reuses the embedder and FAISS index built in Section 8.6\n# Swap the generation step from Flan-T5 to the API client\n\nSYSTEM_PROMPT = \"\"\"\nYou are a data analyst assistant with expertise in developer compensation\nand career trends. You answer questions using only the provided context\nfrom the Stack Overflow 2025 Developer Survey. Be concise and specific.\nIf the context does not contain enough information to answer, say so clearly.\n\"\"\".strip()\n\n\ndef api_rag_answer(question: str, k: int = 4) -> dict:\n    \"\"\"\n    Full API-based RAG pipeline:\n    1. Embed the question with sentence-transformers\n    2. Retrieve top-k documents from FAISS index\n    3. Build a context-augmented prompt\n    4. Call the LLM API (or mock if no key is set)\n    5. Return answer + sources + token usage\n    \"\"\"\n    # Step 1 & 2: retrieve\n    results = retrieve(question, k=k, verbose=False)\n    context_lines = [f'{i+1}. {r[\"doc\"]}' for i, r in enumerate(results)]\n    context = '\\n'.join(context_lines)\n\n    # Step 3: build user message with context\n    user_msg = (\n        f'Context (from SO 2025 Developer Survey):\\n{context}\\n\\n'\n        f'Question: {question}\\n\\n'\n        f'Answer based only on the context above:'\n    )\n\n    # Step 4: call API\n    answer = llm.chat(system=SYSTEM_PROMPT, user=user_msg, max_tokens=250)\n\n    return {\n        'question': question,\n        'answer':   answer,\n        'sources':  results,\n        'n_sources': len(results),\n    }\n\n\n# Run the API RAG pipeline\nquestions = [\n    'What programming languages are most common among high-earning developers?',\n    'How does AI tool adoption vary by country in this survey?',\n    'What education levels are typical for data scientists?',\n]\n\nprint('API-based RAG responses:')\nprint('=' * 65)\nfor q in questions:\n    result = api_rag_answer(q)\n    print(f'Q: {result[\"question\"]}')\n    print(f'A: {result[\"answer\"]}')\n    print(f'   ({result[\"n_sources\"]} documents retrieved)')\n    print('-' * 65)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# 8.7.3 -- Prompt engineering: structured output\n#\n# LLMs can be instructed to return JSON, making their output\n# directly usable in downstream code without parsing.\n\nSTRUCTURED_SYSTEM = \"\"\"\nYou are a data extraction assistant. Given context from the SO 2025 survey,\nextract structured information and return ONLY valid JSON with no other text.\n\"\"\".strip()\n\ndef extract_structured(question: str, schema_description: str, k: int = 4) -> dict:\n    \"\"\"\n    Use the LLM to extract structured data from retrieved context.\n    Returns a parsed dict if the response is valid JSON, else raw text.\n    \"\"\"\n    results = retrieve(question, k=k, verbose=False)\n    context = '\\n'.join([f'{i+1}. {r[\"doc\"]}' for i, r in enumerate(results)])\n\n    user_msg = (\n        f'Context:\\n{context}\\n\\n'\n        f'Task: {question}\\n'\n        f'Return your answer as JSON with this structure: {schema_description}\\n'\n        f'Return ONLY the JSON object, no markdown fences or explanation.'\n    )\n\n    raw = llm.chat(system=STRUCTURED_SYSTEM, user=user_msg, max_tokens=300)\n\n    # Strip markdown fences if present\n    clean = raw.strip().lstrip('```json').lstrip('```').rstrip('```').strip()\n    try:\n        return json_lib.loads(clean)\n    except json_lib.JSONDecodeError:\n        return {'raw_response': raw, 'parse_error': True}\n\n\n# Extract structured insights\nresult = extract_structured(\n    question='From the developer profiles, summarise the top 3 languages and typical salary range',\n    schema_description='{\"top_languages\": [\"lang1\", \"lang2\", \"lang3\"], \"salary_range\": {\"low\": int, \"high\": int, \"currency\": \"USD\"}, \"sample_size\": int}',\n    k=5\n)\n\nprint('Structured extraction result:')\nprint(json_lib.dumps(result, indent=2))\nprint()\nllm.usage_summary()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Section 8.7 Key Takeaways\n\n- **Provider-agnostic clients** insulate your code from vendor lock-in.\n  Switching from OpenAI to Anthropic (or a local Ollama model) is one parameter change.\n- **API keys belong in environment variables, never in code.**\n  In Colab use Secrets (`üîë` icon in the left sidebar) ‚Äî they persist across sessions\n  and are never stored in the notebook file.\n- **The RAG pattern is the same regardless of the generation backend:**\n  embed ‚Üí retrieve ‚Üí augment prompt ‚Üí generate.\n  The choice of local model vs API affects cost, latency, and privacy, not the architecture.\n- **Structured output** (instructing the LLM to return JSON) makes LLM responses\n  directly consumable by downstream code without fragile string parsing.\n  Always strip markdown fences before calling `json.loads()`.\n- **Token tracking** is essential in production. Log `usage.total_tokens` on every\n  API call and set `max_tokens` explicitly to prevent runaway costs.\n- **Mock/fallback mode** lets the notebook run without an API key ‚Äî useful for\n  testing pipelines and CI environments where keys are not available.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Concept Check Questions\n\n> Test your understanding before moving on. Answer each question without referring back to the notebook, then expand to check.\n\n**Q1.** What is **subword tokenisation** and why is it better than word-level for rare words?\n\n<details><summary>Show answer</summary>\n\nSubword tokenisation splits rare words into sub-units: 'unhappiness' ‚Üí ['un', '##happi', '##ness']. Any word can be represented by combining known subwords ‚Äî no 'unknown token' fallback that discards information. The model also shares knowledge across morphologically related words.\n\n</details>\n\n**Q2.** What role does the `[CLS]` token play in a BERT-style model?\n\n<details><summary>Show answer</summary>\n\n`[CLS]` is prepended to every input. After self-attention, the transformer aggregates information from all tokens into this position. Its final hidden state encodes a sequence-level representation used by the classification head.\n\n</details>\n\n**Q3.** Explain the RAG architecture in three sentences without using acronyms.\n\n<details><summary>Show answer</summary>\n\nA large collection of documents is split into chunks, converted into dense numerical vectors by a language model, and stored in a vector index. When a question arrives, it is also converted to a vector and the index returns the most similar chunks. Those chunks are injected into a prompt alongside the question, and a language model generates a grounded answer.\n\n</details>\n\n**Q4.** What is the difference between **fine-tuning** and **few-shot prompting**? When to use each?\n\n<details><summary>Show answer</summary>\n\n**Fine-tuning** updates model weights on labelled examples ‚Äî higher accuracy for well-defined tasks with 100+ examples, but requires GPU and creates a new artefact to maintain. **Few-shot prompting** includes examples in the prompt without changing weights ‚Äî zero maintenance, works with API-only access, but less accurate on complex tasks.\n\n</details>\n\n**Q5.** An LLM ignores your 'return only JSON' instruction and wraps the response in markdown fences. How do you handle this robustly?\n\n<details><summary>Show answer</summary>\n\nStrip fences before parsing: `clean = raw.strip().lstrip('```json').lstrip('```').rstrip('```').strip()`. Wrap `json.loads(clean)` in `try/except json.JSONDecodeError`. For production, use a model with native JSON mode to avoid this entirely.\n\n</details>\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Coding Exercises\n\n> Three exercises per chapter: **üîß Guided** (fill-in-the-blanks) ¬∑ **üî® Applied** (write from scratch) ¬∑ **üèóÔ∏è Extension** (go beyond the chapter)\n\nExercises use the SO 2025 developer survey dataset.\nExpand each **Solution** block only after attempting the exercise.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exercise 1 üîß Guided ‚Äî TF-IDF job title seniority classifier\n\nComplete the TF-IDF ‚Üí Logistic Regression pipeline that classifies\ndeveloper job titles into four seniority levels:\n`junior`, `mid`, `senior`, `lead/principal`.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\n\ntitles = (\n    ['junior developer']*80 + ['junior software engineer']*60 +\n    ['software engineer']*100 + ['developer']*80 +\n    ['senior engineer']*90 + ['senior developer']*70 +\n    ['staff engineer']*40 + ['principal engineer']*40 + ['tech lead']*40\n)\nlabels = ['junior']*140 + ['mid']*180 + ['senior']*160 + ['lead']*120\n\npipe = Pipeline([\n    ('tfidf', TfidfVectorizer(\n        # YOUR CODE: set ngram_range, min_df, max_features\n    )),\n    ('clf', LogisticRegression(\n        # YOUR CODE: set multi_class, max_iter\n    ))\n])\n\nscores = cross_val_score(pipe, titles, labels, cv=5, scoring='accuracy')\nprint(f'CV Accuracy: {scores.mean():.3f} ¬± {scores.std():.3f}')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<details><summary>üí° Hint</summary>\n\n`TfidfVectorizer(ngram_range=(1,2), min_df=2, max_features=500)` is a good starting point.\n`LogisticRegression(multi_class='multinomial', max_iter=500, C=1.0)`\n\n</details>\n\n<details><summary>‚úÖ Solution</summary>\n\n```python\npipe = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1,2),min_df=1,max_features=500)),\n                  ('clf', LogisticRegression(max_iter=500,C=1.0))])\n```\n\n</details>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exercise 2 üî® Applied ‚Äî Sentence embedding similarity search\n\nBuild a simple semantic search engine for SO 2025 developer survey responses.\nUsing a sentence transformer model, embed a corpus of job descriptions,\nthen implement `find_similar(query, top_k=5)` that returns the most\nsimilar job descriptions using cosine similarity.\n\nUse `sentence-transformers` (install if needed) or fall back to TF-IDF vectors.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "JOB_DESCRIPTIONS = [\n    'Python developer building ML pipelines and data products',\n    'Full stack JavaScript engineer working on React and Node',\n    'Data scientist specialising in NLP and large language models',\n    'DevOps engineer managing Kubernetes and cloud infrastructure',\n    'Backend engineer working with Python, FastAPI, and PostgreSQL',\n    'Machine learning engineer deploying models to production at scale',\n    'Data engineer building Spark and dbt data pipelines',\n    'Frontend developer focused on accessibility and performance',\n]\n\ndef find_similar(query: str, corpus: list[str], top_k: int = 3) -> list[tuple[str,float]]:\n    \"\"\"Return top_k (description, similarity_score) pairs.\"\"\"\n    # YOUR CODE\n    pass\n\nresults = find_similar('NLP researcher working on transformers', JOB_DESCRIPTIONS)\nfor desc, score in results:\n    print(f'{score:.3f}  {desc}')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<details><summary>üí° Hint</summary>\n\nTF-IDF fallback: fit `TfidfVectorizer` on the corpus, transform query,\ncompute `cosine_similarity(query_vec, corpus_vecs)`, sort descending.\nFor sentence-transformers: `SentenceTransformer('all-MiniLM-L6-v2').encode(texts)`\n\n</details>\n\n<details><summary>‚úÖ Solution</summary>\n\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\ndef find_similar(query, corpus, top_k=3):\n    vec=TfidfVectorizer().fit(corpus)\n    c_vecs=vec.transform(corpus); q_vec=vec.transform([query])\n    sims=cosine_similarity(q_vec,c_vecs)[0]\n    top=np.argsort(sims)[::-1][:top_k]\n    return [(corpus[i], sims[i]) for i in top]\n```\n\n</details>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Exercise 3 üèóÔ∏è Extension ‚Äî RAG system: 'which language pays most in country X?'\n\nBuild a mini RAG pipeline that answers natural language salary questions.\n\nComponents:\n1. A knowledge base of (language, country, median_salary) facts stored as text chunks\n2. A retriever that returns the top-3 most relevant chunks for a query\n3. A simple answer extractor that parses the retrieved chunks\n\nThe system should handle queries like:\n'What language pays best in Germany?' and\n'Which programming language has the highest median salary in the US?'\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import json\n\n# Knowledge base ‚Äî salary facts (simulated SO 2025 data)\nKB = [\n    {'lang':'Python','country':'US','median_salary':125000},\n    {'lang':'Java','country':'US','median_salary':118000},\n    {'lang':'JavaScript','country':'US','median_salary':108000},\n    {'lang':'Python','country':'Germany','median_salary':78000},\n    {'lang':'Rust','country':'Germany','median_salary':85000},\n    {'lang':'JavaScript','country':'Germany','median_salary':72000},\n    {'lang':'Python','country':'India','median_salary':18000},\n    {'lang':'Java','country':'India','median_salary':16000},\n]\n\ndef build_chunks(kb: list[dict]) -> list[str]:\n    # Convert each record to a text chunk\n    pass\n\ndef retrieve(query: str, chunks: list[str], top_k: int = 3) -> list[str]:\n    # Return top_k relevant chunks\n    pass\n\ndef answer_query(query: str, chunks: list[str]) -> str:\n    # Parse chunks and formulate answer\n    pass\n\nchunks = build_chunks(KB)\nfor q in ['What pays best in Germany?', 'Top language salary in US?']:\n    print(f'Q: {q}\\nA: {answer_query(q, chunks)}\\n')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<details><summary>üí° Hint</summary>\n\nFor `build_chunks`: convert each dict to a descriptive sentence like\n'Python developers in Germany earn a median salary of $78,000 per year.'\nFor `retrieve`: use TF-IDF cosine similarity (same pattern as Exercise 2).\nFor `answer_query`: retrieve top chunks, find the one with the highest salary mentioned.\n\n</details>\n\n<details><summary>‚úÖ Solution</summary>\n\n```python\ndef build_chunks(kb):\n    return [f'{r[\"lang\"]} developers in {r[\"country\"]} earn a median salary of ${r[\"median_salary\"]:,} per year.' for r in kb]\ndef retrieve(query, chunks, top_k=3):\n    return find_similar(query, chunks, top_k)  # reuse from Ex 2\ndef answer_query(query, chunks):\n    results = retrieve(query, chunks)\n    # Extract highest salary from top chunks\n    import re\n    best = max(results, key=lambda x: int(re.sub(r'[^0-9]','',x[0].split('$')[1].split(' ')[0]) or 0) if '$' in x[0] else 0)\n    return best[0]\n```\n\n</details>\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Chapter 8 Summary\n\n### Key Takeaways\n\n- **Classical NLP pipeline:** clean ‚Üí tokenise ‚Üí remove stopwords ‚Üí lemmatise ‚Üí TF-IDF ‚Üí classifier.\n  Fast, interpretable, and competitive on short domain-specific text.\n- **TF-IDF** scores words by local frequency times global rarity.\n  `ngram_range=(1,2)` captures multi-word phrases like 'machine learning'.\n- **Contextual embeddings** (transformers) map the same word to different vectors\n  depending on context. `[CLS]` aggregates the full sequence for classification.\n- **Zero-shot inference** uses pre-trained models directly with no task-specific training.\n- **Fine-tuning** adapts a pre-trained model with a small labelled dataset.\n  3 epochs on ~1,600 examples produces a strong classifier.\n- **`clip_grad_norm_`** prevents exploding gradients during fine-tuning.\n- **Sentence transformers** produce fixed-size embeddings optimised for semantic\n  similarity -- identical sentences get near-identical vectors.\n- **FAISS** indexes millions of vectors and returns the top-k nearest neighbours\n  in milliseconds using approximate or exact search.\n- **API-based LLM integration** follows the same RAG pattern but replaces the local\n  model with an API call. Use a provider-agnostic client class so swapping vendors\n  requires changing one parameter, not rewriting the pipeline.\n- **Structured output** prompting (return JSON only) makes LLM responses\n  directly parseable by downstream code without fragile string parsing.\n- **RAG** separates knowledge (the document store, updatable) from reasoning\n  (the LLM, fixed). It is the dominant architecture for Q&A over private documents.\n  The three steps are always: chunk, embed, index at build time;\n  retrieve, augment prompt, generate at query time.\n\n### Project Thread Status\n\n| Task | Method | Result |\n|------|--------|--------|\n| Developer role classification | TF-IDF + Logistic Regression | Accuracy reported |\n| Sentiment analysis | Zero-shot DistilBERT | Labels + confidence |\n| Zero-shot role classification | BART-large-MNLI | Top label per description |\n| Fine-tuned role classifier | DistilBERT 3 epochs | Accuracy vs baseline |\n| Attention visualisation | DistilBERT last layer | Heatmap plotted |\n| RAG over SO 2025 profiles | MiniLM + FAISS + Flan-T5 | End-to-end Q&A |\n| API-based RAG | LLMClient + FAISS | Provider-agnostic Q&A |\n| Structured extraction | JSON-mode prompting | Parsed dict output |\n\n---\n\n### What's Next: Chapter 9 -- Computer Vision with PyTorch\n\nChapter 9 applies the PyTorch training loop from Chapter 7 to image data:\nCNNs from scratch, transfer learning with ResNet-18, feature map visualisation,\nobject detection with Faster R-CNN, and semantic segmentation with DeepLabV3.\nImages are the third major data modality after tabular (Ch 5‚Äì6) and text (Ch 8).\n\n### (old marker to remove) -- Computer Vision with PyTorch\n\nChapter 9 examines the risks introduced by everything built in Part 3:\nbias in training data, fairness metrics, model interpretability with SHAP,\nand the practical steps for building more responsible ML systems.\n\n---\n\n*End of Chapter 8 -- Python for AI/ML*  \n[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n"
    }
  ]
}