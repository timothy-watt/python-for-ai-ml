{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Appendix G — Docker and Containerisation for ML\n## *Python for AI/ML: A Complete Learning Journey*\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/APP_G_Docker_Containerisation.ipynb)\n&nbsp;&nbsp;[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n\n---\n\n**Prerequisites:** Chapter 11 (MLOps and Production ML — FastAPI endpoint)  \n\n### Learning Objectives\n\n- Explain why containers solve the 'works on my machine' problem\n- Write a `Dockerfile` that packages the Chapter 11 FastAPI salary endpoint\n- Build and run a Docker image locally\n- Write a `docker-compose.yml` that runs the API alongside an MLflow tracking server\n- Understand image layers, caching, and how to keep images small\n- Push an image to Docker Hub and understand cloud registry options\n- Know when to use Docker vs a cloud function vs a managed endpoint\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## G.1 — Why Containers?\n\nWhen you trained your salary model in Chapter 11, it worked perfectly in Colab.\nBut Colab runs Python 3.11 with specific library versions. Your colleague's laptop\nruns Python 3.9. The production server runs 3.12. The FastAPI app that works\nlocally throws `ImportError` in production.\n\n**A container packages your code together with its entire runtime environment:**\nPython version, all libraries, system dependencies, and configuration.\nThe container runs identically on any machine that has Docker installed.\n\n```\n  Without containers:          With containers:\n  ┌─────────────────┐          ┌─────────────────────────────────┐\n  │  Your code      │          │  Container                       │\n  │  (Python 3.11)  │          │  ┌─────────────────┐            │\n  ├─────────────────┤          │  │  Your code      │            │\n  │  Dev machine    │          │  │  Python 3.11    │            │\n  │  (libraries A)  │          │  │  Libraries A    │            │\n  └─────────────────┘          │  └─────────────────┘            │\n  ≠                            ├─────────────────────────────────┤\n  ┌─────────────────┐          │  Docker runtime                  │\n  │  Prod server    │          ├─────────────────────────────────┤\n  │  (Python 3.12)  │          │  Any OS, any machine             │\n  │  (libraries B)  │          └─────────────────────────────────┘\n  └─────────────────┘          = identical everywhere\n```\n\n**Core Docker concepts:**\n\n- **Image** — a read-only snapshot of a filesystem. Built from a `Dockerfile`.\n- **Container** — a running instance of an image. Isolated from the host.\n- **Layer** — each instruction in a `Dockerfile` adds a layer. Layers are cached;\n  unchanged layers are reused on rebuild, making builds fast.\n- **Registry** — a store for images. Docker Hub is the public default;\n  AWS ECR, GCP Artifact Registry, and Azure ACR are common private options.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## G.2 — Writing a Dockerfile for the Salary API\n\nThe Chapter 11 FastAPI endpoint serves salary predictions from the MLflow-registered\nmodel. Below is the production-grade `Dockerfile` for that service.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# G.2.1 -- Generate all Docker files for the salary API service\n\nimport os\n\nos.makedirs('/tmp/salary_api_docker', exist_ok=True)\n\n# ── Dockerfile ────────────────────────────────────────────────────\nDOCKERFILE = '''\n# ── Stage 1: builder ─────────────────────────────────────────────\n# Use a full Python image to install dependencies\nFROM python:3.11-slim AS builder\n\nWORKDIR /app\n\n# Install build tools (needed for some Python packages)\nRUN apt-get update && apt-get install -y --no-install-recommends \\\\\n    build-essential \\\\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy and install dependencies first (layer caching)\n# If requirements.txt doesn't change, this layer is reused on rebuild\nCOPY requirements.txt .\nRUN pip install --no-cache-dir --prefix=/install -r requirements.txt\n\n# ── Stage 2: runtime ─────────────────────────────────────────────\n# Use a minimal image for the final container\nFROM python:3.11-slim\n\nWORKDIR /app\n\n# Copy installed packages from builder stage\nCOPY --from=builder /install /usr/local\n\n# Copy application code\nCOPY salary_api.py .\n\n# Create non-root user for security best practice\nRUN useradd --no-create-home --shell /bin/false appuser\nUSER appuser\n\n# Document the port (does not actually publish it)\nEXPOSE 8000\n\n# Health check: Docker will restart the container if this fails\nHEALTHCHECK --interval=30s --timeout=10s --start-period=15s --retries=3 \\\\\n    CMD python -c \"import urllib.request; urllib.request.urlopen('http://localhost:8000/health')\"\n\n# Start the FastAPI server\n# --workers 2: two worker processes for concurrent requests\nCMD [\"uvicorn\", \"salary_api:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--workers\", \"2\"]\n'''\n\n# ── requirements.txt ──────────────────────────────────────────────\nREQUIREMENTS = '''\nfastapi==0.115.0\nuvicorn[standard]==0.32.0\npydantic==2.9.2\nmlflow==2.17.0\nscikit-learn==1.5.2\nnumpy==1.26.4\npandas==2.2.3\nhttpx==0.27.2\n'''\n\n# ── .dockerignore ─────────────────────────────────────────────────\nDOCKERIGNORE = '''\n# Never copy these into the image\n__pycache__/\n*.py[cod]\n.env\n.git/\n*.ipynb\nmlruns/\nmlflow.db\ndata/\nmodels/\ntests/\n*.md\nDockerfile*\ndocker-compose*.yml\n'''\n\nfor fname, content in [\n    ('Dockerfile',       DOCKERFILE),\n    ('requirements.txt', REQUIREMENTS),\n    ('.dockerignore',    DOCKERIGNORE),\n]:\n    path = f'/tmp/salary_api_docker/{fname}'\n    with open(path, 'w') as f:\n        f.write(content.lstrip('\\n'))\n    print(f'Written: {fname}')\n\nprint()\nprint('Key Dockerfile patterns used:')\nprint('  Multi-stage build:  builder stage installs deps; runtime stage is minimal')\nprint('  Layer caching:      COPY requirements.txt before COPY . (slow deps cached)')\nprint('  Non-root user:      run as appuser, not root (security best practice)')\nprint('  HEALTHCHECK:        Docker auto-restarts unhealthy containers')\nprint('  .dockerignore:      keeps image small by excluding dev files')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## G.3 — docker-compose: API + MLflow Tracking Server\n\nA real ML deployment has multiple services: the prediction API,\nthe MLflow tracking server, and a database for MLflow metadata.\n`docker-compose` orchestrates multiple containers as a single application.\n\n**Service architecture:**\n\n```\n  ┌──────────────────────────────────────────────────────┐\n  │  docker-compose network                               │\n  │                                                       │\n  │  ┌──────────────────┐    ┌────────────────────────┐  │\n  │  │  salary-api      │    │  mlflow-server          │  │\n  │  │  :8000           │◄──►│  :5000                  │  │\n  │  │  FastAPI +       │    │  Tracking UI            │  │\n  │  │  loaded model    │    │  Model Registry         │  │\n  │  └──────────────────┘    └────────────────────────┘  │\n  │           │                         │                  │\n  │  ┌────────▼─────────────────────────▼──────────────┐  │\n  │  │  shared volume: /mlruns  (model artefacts)       │  │\n  │  └─────────────────────────────────────────────────┘  │\n  └──────────────────────────────────────────────────────┘\n       Port 8000 and 5000 exposed to host machine\n```\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# G.3.1 -- Generate docker-compose.yml\n\nDOCKER_COMPOSE = '''\n# docker-compose.yml\n# Runs the salary prediction API alongside the MLflow tracking server\n# Usage: docker-compose up --build\n\nversion: \"3.9\"\n\nservices:\n\n  # ── MLflow Tracking Server ───────────────────────────────────────\n  mlflow-server:\n    image: python:3.11-slim\n    command: >\n      bash -c \"pip install mlflow==2.17.0 -q &&\n               mlflow server\n               --host 0.0.0.0\n               --port 5000\n               --backend-store-uri sqlite:////mlruns/mlflow.db\n               --default-artifact-root /mlruns/artefacts\"\n    ports:\n      - \"5000:5000\"              # MLflow UI at http://localhost:5000\n    volumes:\n      - mlruns_data:/mlruns      # persist runs and models between restarts\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:5000/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n\n  # ── Salary Prediction API ────────────────────────────────────────\n  salary-api:\n    build:\n      context: .                 # build from Dockerfile in current dir\n      dockerfile: Dockerfile\n    ports:\n      - \"8000:8000\"              # API at http://localhost:8000\n      #                            Swagger docs at http://localhost:8000/docs\n    environment:\n      # Model URI: load the Production-stage model from the MLflow registry\n      MODEL_URI: \"models:/so2025_salary_predictor/Production\"\n      MLFLOW_TRACKING_URI: \"http://mlflow-server:5000\"\n    volumes:\n      - mlruns_data:/mlruns      # shared with mlflow-server\n    depends_on:\n      mlflow-server:\n        condition: service_healthy\n    restart: unless-stopped      # auto-restart on crash\n\nvolumes:\n  mlruns_data:                   # named volume persists between docker-compose up/down\n'''\n\nwith open('/tmp/salary_api_docker/docker-compose.yml', 'w') as f:\n    f.write(DOCKER_COMPOSE.lstrip('\\n'))\nprint('Written: docker-compose.yml')\n\nprint()\nprint('To run the full stack:')\nprint('  cd /tmp/salary_api_docker')\nprint('  docker-compose up --build')\nprint()\nprint('Then access:')\nprint('  API:         http://localhost:8000')\nprint('  API docs:    http://localhost:8000/docs')\nprint('  MLflow UI:   http://localhost:5000')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## G.4 — Build, Run, and Debug\n\nThe commands below are run in a terminal (not in Colab).\nColab does not have Docker installed, but these are the exact commands\nyou would run on your local machine or a CI server.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# G.4.1 -- Commonly used Docker commands (reference)\n\nDOCKER_COMMANDS = {\n    'Build the image': [\n        'docker build -t salary-api:v1.0 .',\n        '# -t  tag the image as salary-api version 1.0',\n        '# .   use the Dockerfile in the current directory',\n    ],\n    'Run the container': [\n        'docker run -d \\\\',\n        '  -p 8000:8000 \\\\',\n        '  -e MODEL_URI=\"runs:/abc123/model\" \\\\',\n        '  --name salary-api \\\\',\n        '  salary-api:v1.0',\n        '# -d  run in background (detached)',\n        '# -p  map host port 8000 to container port 8000',\n        '# -e  pass environment variables',\n    ],\n    'View logs': [\n        'docker logs salary-api',\n        'docker logs -f salary-api  # follow (like tail -f)',\n    ],\n    'Open a shell inside running container': [\n        'docker exec -it salary-api /bin/bash',\n    ],\n    'Stop and remove container': [\n        'docker stop salary-api',\n        'docker rm salary-api',\n    ],\n    'List images and containers': [\n        'docker images                    # all images',\n        'docker ps                        # running containers',\n        'docker ps -a                     # all containers (including stopped)',\n    ],\n    'Docker Compose': [\n        'docker-compose up --build        # build and start all services',\n        'docker-compose up -d             # start in background',\n        'docker-compose logs -f           # follow logs from all services',\n        'docker-compose down              # stop and remove containers',\n        'docker-compose down -v           # also delete named volumes',\n    ],\n    'Push to Docker Hub': [\n        'docker login',\n        'docker tag salary-api:v1.0 yourusername/salary-api:v1.0',\n        'docker push yourusername/salary-api:v1.0',\n    ],\n}\n\nfor section, commands in DOCKER_COMMANDS.items():\n    print(f'# {section}')\n    for cmd in commands:\n        print(f'  {cmd}')\n    print()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## G.5 — Keeping Images Small and Deploying to the Cloud\n\n**Why image size matters:** smaller images pull faster, start faster,\ncost less to store in a registry, and have a smaller attack surface.\n\n**Techniques used in our Dockerfile:**\n\n- **`python:3.11-slim`** instead of `python:3.11`: removes dev tools, saves ~800MB\n- **Multi-stage build:** build dependencies are installed in stage 1 and only\n  the compiled packages are copied to stage 2 — no compilers in the final image\n- **`--no-cache-dir`** on pip: don't cache wheel files inside the image\n- **`.dockerignore`:** prevents notebooks, test files, and large data files\n  from being copied into the image context\n\n**Cloud deployment options:**\n\n| Platform | How to deploy | Best for |\n|----------|--------------|----------|\n| **Google Cloud Run** | `gcloud run deploy` | Serverless, auto-scales to zero |\n| **AWS App Runner** | Push to ECR → deploy | Managed, no Kubernetes needed |\n| **Azure Container Apps** | `az containerapp create` | Integrated with Azure ML |\n| **Railway / Render** | Connect Docker Hub repo | Simplest for side projects |\n| **Kubernetes (GKE/EKS)** | Helm chart or manifest | Large-scale, full control |\n\nFor a FastAPI salary prediction endpoint, **Cloud Run** is the recommended starting\npoint: zero infrastructure management, scales to zero when idle (no idle cost),\nand deploys with a single command:\n\n```bash\n# Build and push to Google Container Registry\ngcloud builds submit --tag gcr.io/YOUR_PROJECT/salary-api\n\n# Deploy to Cloud Run\ngcloud run deploy salary-api \\\\\n  --image gcr.io/YOUR_PROJECT/salary-api \\\\\n  --platform managed \\\\\n  --region us-central1 \\\\\n  --allow-unauthenticated \\\\\n  --set-env-vars MODEL_URI=models:/so2025_salary_predictor/Production\n```\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Appendix G Summary\n\n### Key Takeaways\n\n- **Containers solve environment reproducibility.** The same image runs identically\n  on a developer laptop, a CI server, and a cloud VM.\n- **Multi-stage builds** keep images small: use a `builder` stage with compilers\n  and only copy the compiled output to the minimal runtime stage.\n- **Layer caching** is the most important build performance technique.\n  Always `COPY requirements.txt` and install dependencies *before* `COPY . .`\n  so the slow install step is cached when only your code changes.\n- **`.dockerignore`** is as important as `.gitignore`. Without it, notebooks,\n  test data, and `.git/` directories bloat the image build context.\n- **`docker-compose`** orchestrates multi-service stacks locally.\n  The `depends_on: condition: service_healthy` pattern ensures services start\n  in the right order.\n- **`restart: unless-stopped`** makes containers self-healing — Docker automatically\n  restarts them on crash or server reboot.\n- **Cloud Run** is the lowest-friction path to production for a FastAPI ML endpoint:\n  no Kubernetes, scales to zero, pay only for actual requests.\n\n---\n\n*End of Appendix G — Python for AI/ML*  \n[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n"
    }
  ]
}