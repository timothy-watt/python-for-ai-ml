{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Appendix I — Failure Modes and Troubleshooting\n## *Python for AI/ML: A Complete Learning Journey*\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/APP_I_Failure_Modes.ipynb)\n&nbsp;&nbsp;[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n\n---\n\nMost ML tutorials show you the happy path: clean data, well-behaved gradients,\nmetrics that improve monotonically. Real projects don't work that way.\n\nThis appendix documents the **failure modes that silently ruin ML projects** —\nbugs that don't crash your code but produce models that look fine and aren't.\nEach section follows the same structure:\n\n- **What goes wrong** — the failure and why it's dangerous\n- **How to detect it** — reproducible diagnostic code\n- **How to fix it** — the correct pattern\n- **Worked example** — applied to the SO 2025 salary dataset\n\n### Contents\n\n| Section | Failure Mode | Chapters it affects |\n|---------|-------------|---------------------|\n| I.1 | Silent data leakage | Ch 6, 8, 12 |\n| I.2 | Train/test contamination | Ch 6, 7 |\n| I.3 | NaN propagation | Ch 3, 6 |\n| I.4 | Overfitting that looks like underfitting | Ch 6, 7 |\n| I.5 | Class imbalance disasters | Ch 6, 10 |\n| I.6 | Gradient problems in deep learning | Ch 7, 11 |\n| I.7 | Tokenisation and embedding gotchas | Ch 8 |\n| I.8 | Training-serving skew | Ch 12 |\n| I.9 | Diagnostic checklist | All |\n\n**How to use this appendix:** Read it end-to-end once, then return to specific\nsections when something in your project doesn't make sense.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Setup\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom typing import Optional\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (accuracy_score, f1_score, roc_auc_score,\n                              precision_recall_curve, confusion_matrix,\n                              ConfusionMatrixDisplay)\nfrom sklearn.impute import SimpleImputer\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\n# ── Synthetic SO 2025 dataset (consistent with all chapters) ──────\ndef make_so_dataset(n: int = 8000, nan_frac: float = 0.0,\n                    seed: int = RANDOM_STATE) -> pd.DataFrame:\n    rng = np.random.default_rng(seed)\n    years   = rng.exponential(6, n).clip(0, 35)\n    salary  = np.exp(10.8 + 0.07 * years + rng.normal(0, 0.5, n)).clip(20000, 500000)\n    df = pd.DataFrame({\n        'YearsCodePro':        years,\n        'ConvertedCompYearly': salary,\n        'uses_python':         rng.integers(0, 2, n),\n        'uses_sql':            rng.integers(0, 2, n),\n        'uses_js':             rng.integers(0, 2, n),\n        'uses_ai':             rng.integers(0, 2, n),\n        'EdLevel':             rng.choice(\n            ['Bachelor', 'Master', 'PhD', 'No degree'], n,\n            p=[0.45, 0.30, 0.10, 0.15]),\n        'survey_year':         rng.integers(2020, 2026, n),  # for temporal leakage demo\n    })\n    if nan_frac > 0:\n        mask = rng.random((n, 4)) < nan_frac\n        for j, col in enumerate(['YearsCodePro','uses_python','uses_sql','uses_js']):\n            df.loc[mask[:, j], col] = np.nan\n    return df\n\ndf = make_so_dataset()\nprint(f'Dataset: {len(df):,} rows, {df.shape[1]} columns')\nprint(df.dtypes)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## I.1 — Silent Data Leakage\n\n**What goes wrong:** The model has access to information during training that\nit cannot possibly have at prediction time. It learns to exploit this shortcut\ninstead of the actual signal you want it to learn.\n\nData leakage is dangerous because it is *silent*: the model produces excellent\nvalidation scores that completely evaporate in production.\n\n### Three common leakage patterns\n\n**1. Temporal leakage** — using future data to predict the past.\nExample: training a salary model on 2020–2025 data, including a feature\nderived from 2025 survey trends, then 'predicting' 2022 salaries.\n\n**2. Target leakage** — a feature that is computed *from* the target,\nor is only known *because* you know the target.\nExample: `total_compensation` as a feature when predicting `base_salary`.\n\n**3. Row leakage** — test set rows appear in the training set.\nChecking `len(X_train) + len(X_test) == len(df)` is not enough\nif you have duplicate rows in the dataset.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# I.1 -- Demonstrating and detecting data leakage\n\n# ── Pattern 1: Temporal leakage ───────────────────────────────────\ndf['high_earner'] = (df['ConvertedCompYearly'] >=\n                     df['ConvertedCompYearly'].quantile(0.60)).astype(int)\n\n# LEAKY: compute a 'future trend' feature on the full dataset,\n# then split. Year 2025 data 'leaks' into earlier rows via the mean.\ndf['mean_salary_by_year'] = df.groupby('survey_year')['ConvertedCompYearly']\\\n                              .transform('mean')  # ← uses ALL rows including test rows\n\nFEATURES_LEAKY  = ['YearsCodePro','uses_python','uses_sql','mean_salary_by_year']\nFEATURES_CLEAN  = ['YearsCodePro','uses_python','uses_sql','uses_js']\n\ndef quick_eval(X: pd.DataFrame, y: pd.Series, label: str) -> float:\n    X_tr, X_te, y_tr, y_te = train_test_split(\n        X, y, test_size=0.2, random_state=RANDOM_STATE)\n    clf = GradientBoostingClassifier(n_estimators=50, random_state=RANDOM_STATE)\n    clf.fit(X_tr.fillna(0), y_tr)\n    acc = accuracy_score(y_te, clf.predict(X_te.fillna(0)))\n    print(f'  {label:<45} accuracy = {acc:.4f}')\n    return acc\n\nprint('Leaky vs clean features:')\nacc_leaky = quick_eval(df[FEATURES_LEAKY], df['high_earner'], 'Leaky (mean_salary_by_year included)')\nacc_clean = quick_eval(df[FEATURES_CLEAN], df['high_earner'], 'Clean (no future-derived features)')\nprint(f'\\nLeakage inflation: +{acc_leaky - acc_clean:.4f} accuracy points')\nprint('This gap disappears entirely in production — a 100% phantom gain.')\n\n# ── Pattern 3: Row leakage via duplicates ─────────────────────────\nprint('\\nRow leakage detection:')\ndf_with_dupes = pd.concat([df, df.sample(200, random_state=0)], ignore_index=True)\nX = df_with_dupes[FEATURES_CLEAN]\ny = df_with_dupes['high_earner']\nX_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n\n# Detect: hash every row and check for train/test overlap\ndef row_hashes(df_: pd.DataFrame) -> set:\n    return set(pd.util.hash_pandas_object(df_, index=False))\n\ntrain_hashes = row_hashes(X_tr)\ntest_hashes  = row_hashes(X_te)\noverlap      = train_hashes & test_hashes\nprint(f'  Duplicate rows in dataset:      {df_with_dupes.duplicated().sum()}')\nprint(f'  Train/test row overlap (hashed): {len(overlap)} rows')\nprint(f'  Fix: df.drop_duplicates() before splitting')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# I.1 -- Target leakage: how to spot it in feature importances\n\nfrom sklearn.inspection import permutation_importance\n\n# Introduce a leaky feature: partial_salary = salary * noise (correlated with target)\nrng = np.random.default_rng(1)\ndf['partial_salary'] = df['ConvertedCompYearly'] * rng.uniform(0.85, 1.15, len(df))\n\nFEATURES_WITH_LEAK = FEATURES_CLEAN + ['partial_salary']\nX = df[FEATURES_WITH_LEAK].fillna(0)\ny = df['high_earner']\n\nX_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\nclf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\nclf.fit(X_tr, y_tr)\n\nimportances = pd.Series(clf.feature_importances_, index=FEATURES_WITH_LEAK).sort_values()\n\nfig, ax = plt.subplots(figsize=(8, 3.5))\ncolours = ['#C0392B' if f == 'partial_salary' else '#2E75B6' for f in importances.index]\nimportances.plot.barh(ax=ax, color=colours)\nax.set_title('Feature Importances — Red = Leaky Feature\\n'\n             'A single feature dominating importance is a leakage red flag')\nax.set_xlabel('Mean Decrease in Impurity')\n\n# Add legend\nfrom matplotlib.patches import Patch\nax.legend(handles=[Patch(color='#C0392B', label='Leaky feature'),\n                   Patch(color='#2E75B6', label='Legitimate feature')])\nplt.tight_layout()\nplt.show()\n\nprint('Rule of thumb: if one feature has >> 50% importance, investigate for leakage.')\nprint('Apply permutation importance on the TEST SET as a second check —')\nprint('leaky features score high on train but lower on fresh data.')\n\n# Cleanup\ndf.drop(columns=['mean_salary_by_year','partial_salary'], inplace=True)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## I.2 — Train/Test Contamination\n\n**What goes wrong:** Preprocessing steps are fitted on the *entire* dataset\n(including the test set) before the train/test split. The test set is no longer\na fair representation of unseen data — the model has seen its statistical\nproperties during fitting.\n\nThis is one of the most common mistakes made by practitioners who have\nlearned sklearn but haven't yet internalised the Pipeline pattern.\n\n```python\n# ❌ WRONG — scaler sees test data during fit\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)          # full dataset scaled\nX_train, X_test = train_test_split(X_scaled)\n\n# ✅ CORRECT — scaler only sees training data\nX_train, X_test = train_test_split(X)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)     # fit on train only\nX_test  = scaler.transform(X_test)          # apply to test\n\n# ✅ BEST — Pipeline handles this automatically and correctly\npipe = Pipeline([('scaler', StandardScaler()), ('clf', LogisticRegression())])\npipe.fit(X_train, y_train)                  # scaler.fit only on X_train\npipe.predict(X_test)                        # scaler.transform applied cleanly\n```\n\n**Why it matters with encoders:** With `StandardScaler`, contamination is subtle\n(mean/std shift). With `TargetEncoder`, it's catastrophic — target statistics\nfrom test rows directly leak into training features.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# I.2 -- Quantifying contamination inflation\n\nfrom sklearn.preprocessing import TargetEncoder\n\ndf['EdLevel_code'] = df['EdLevel'].astype('category').cat.codes\nFEATURES = ['YearsCodePro','uses_python','uses_sql','uses_js','uses_ai','EdLevel_code']\nX = df[FEATURES].fillna(0)\ny = df['high_earner']\n\nX_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n\n# ── Contaminated: scale entire X before split ────────────────────\nscaler_contam = StandardScaler()\nX_all_scaled  = pd.DataFrame(scaler_contam.fit_transform(X), columns=FEATURES)\nX_tr_c, X_te_c = X_all_scaled.iloc[X_tr.index], X_all_scaled.iloc[X_te.index]\n\n# ── Clean: Pipeline enforces correct order ────────────────────────\npipe_clean = Pipeline([\n    ('scaler', StandardScaler()),\n    ('clf',    GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE))\n])\n\n# Contaminated model\nclf_c = GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE)\nclf_c.fit(X_tr_c, y_tr)\nacc_contam = accuracy_score(y_te, clf_c.predict(X_te_c))\n\n# Clean model\npipe_clean.fit(X_tr, y_tr)\nacc_clean = accuracy_score(y_te, pipe_clean.predict(X_te))\n\nprint('Train/test contamination via full-dataset scaling:')\nprint(f'  Contaminated accuracy: {acc_contam:.4f}')\nprint(f'  Clean accuracy:        {acc_clean:.4f}')\nprint(f'  Inflation:             {acc_contam - acc_clean:+.4f}')\nprint()\nprint('With StandardScaler: inflation is small (data is already similar scale).')\nprint('With TargetEncoder or OrdinalEncoder on high-cardinality features,')\nprint('the inflation can exceed 10 percentage points.')\nprint()\nprint('Canonical detection: cross_val_score on the full dataset via Pipeline.')\nprint('If CV score >> hold-out score, contamination is likely.')\n\n# Cleanup\ndf.drop(columns=['EdLevel_code'], inplace=True)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## I.3 — NaN Propagation\n\n**What goes wrong:** Missing values silently corrupt downstream computations.\nThe failure modes are model-specific:\n\n- **sklearn tree models** (GBM, RandomForest): handle NaN internally,\n  but the *missingness pattern itself* becomes a learned signal — your\n  model may be predicting salary from *who didn't answer the survey*,\n  not from the features you think you're using.\n\n- **Linear models / SVM / KNN**: crash with `ValueError: Input X contains NaN`.\n  This is the *good* failure mode — it tells you immediately.\n\n- **PyTorch / deep learning**: NaN in a single training example propagates\n  through the entire batch via backprop. Loss goes to `nan`; gradients\n  become `nan`; all weights become `nan`. Model is silently destroyed.\n\n- **Target column NaN**: rows are silently dropped by some libraries,\n  or cause a crash in others. The silent drop is more dangerous.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# I.3 -- NaN propagation: detection and safe imputation patterns\n\ndf_nan = make_so_dataset(n=5000, nan_frac=0.15)\ndf_nan['high_earner'] = (df_nan['ConvertedCompYearly'] >=\n                          df_nan['ConvertedCompYearly'].quantile(0.60)).astype(int)\n\nFEATURES = ['YearsCodePro','uses_python','uses_sql','uses_js','uses_ai']\n\n# ── Step 1: Audit NaN before any processing ───────────────────────\nprint('=== NaN Audit ===')\nnan_summary = df_nan[FEATURES].isnull().sum()\nnan_pct     = (nan_summary / len(df_nan) * 100).round(1)\naudit = pd.DataFrame({'missing_count': nan_summary, 'missing_pct': nan_pct})\nprint(audit[audit['missing_count'] > 0].to_string())\n\n# ── Step 2: Detect NaN-as-signal (missingness correlation with target) ─\nprint('\\nNaN-as-signal check (missingness correlation with target):')\nfor col in FEATURES:\n    if df_nan[col].isnull().any():\n        is_missing  = df_nan[col].isnull().astype(int)\n        corr        = is_missing.corr(df_nan['high_earner'])\n        flag        = '⚠ potential signal' if abs(corr) > 0.05 else '  ok'\n        print(f'  {col:<20} corr(missing, target) = {corr:+.3f}  {flag}')\n\n# ── Step 3: Safe imputation inside Pipeline ───────────────────────\nprint('\\nImputation strategies:')\nX = df_nan[FEATURES]\ny = df_nan['high_earner']\nX_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n\nfor strategy in ['mean', 'median', 'most_frequent']:\n    pipe = Pipeline([\n        ('imputer', SimpleImputer(strategy=strategy)),\n        ('clf',     GradientBoostingClassifier(n_estimators=50, random_state=RANDOM_STATE))\n    ])\n    pipe.fit(X_tr, y_tr)\n    acc = accuracy_score(y_te, pipe.predict(X_te))\n    print(f'  SimpleImputer(strategy={strategy!r:<15}) accuracy = {acc:.4f}')\n\n# ── Step 4: Assert no NaN escapes the pipeline ────────────────────\nprint('\\nDefensive assertion pattern:')\nprint('  After .fit_transform() or .transform(), always assert:')\nprint('  assert not np.isnan(X_transformed).any(), \"NaN escaped imputer\"')\nprint()\nprint('For PyTorch: add NaN checks at dataloader and loss computation:')\nprint('  assert not torch.isnan(batch).any(), f\"NaN in batch at step {step}\"')\nprint('  assert not torch.isnan(loss),        \"NaN loss — check inputs\"')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## I.4 — Overfitting That Looks Like Underfitting\n\n**What goes wrong:** Your model's validation metrics suggest it isn't learning,\nbut the true problem is that it's learning the *wrong thing* — and the metric\nyou're watching isn't sensitive enough to show it.\n\n### Symptom 1: High train accuracy, low test accuracy (classic overfitting)\nThe model memorised training examples. Fix: regularisation, more data, simpler model.\n\n### Symptom 2: High accuracy on both sets, but terrible real-world performance\nUsually class imbalance + accuracy as metric. A model that always predicts the\nmajority class can hit 90%+ accuracy while being completely useless.\n\n### Symptom 3: Train loss and val loss both high and not improving\nCould be: wrong learning rate, wrong architecture, wrong loss function,\nor the labels are too noisy to learn from. Learning curves distinguish them.\n\n### Symptom 4: Perfect training metrics, random test metrics\nAlmost always leakage (see I.1) — the model learned the shortcut perfectly.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# I.4 -- Learning curves: the right diagnostic for training problems\n\nfrom sklearn.model_selection import learning_curve\n\ndf_lc = make_so_dataset(n=8000)\ndf_lc['high_earner'] = (df_lc['ConvertedCompYearly'] >=\n                         df_lc['ConvertedCompYearly'].quantile(0.60)).astype(int)\nFEATURES = ['YearsCodePro','uses_python','uses_sql','uses_js','uses_ai']\nX = df_lc[FEATURES].fillna(0)\ny = df_lc['high_earner']\n\ndef plot_learning_curve(estimator, X: pd.DataFrame, y: pd.Series,\n                         title: str, ax, cv: int = 5) -> None:\n    \"\"\"Plot learning curves showing train vs validation score as n_samples grows.\"\"\"\n    train_sizes, train_scores, val_scores = learning_curve(\n        estimator, X, y,\n        train_sizes=np.linspace(0.1, 1.0, 8),\n        cv=cv, scoring='accuracy', n_jobs=-1)\n    ts_mean = train_scores.mean(axis=1)\n    vs_mean = val_scores.mean(axis=1)\n    ts_std  = train_scores.std(axis=1)\n    vs_std  = val_scores.std(axis=1)\n\n    ax.fill_between(train_sizes, ts_mean-ts_std, ts_mean+ts_std, alpha=0.15, color='#2E75B6')\n    ax.fill_between(train_sizes, vs_mean-vs_std, vs_mean+vs_std, alpha=0.15, color='#C0392B')\n    ax.plot(train_sizes, ts_mean, 'o-', color='#2E75B6', label='Train')\n    ax.plot(train_sizes, vs_mean, 's-', color='#C0392B', label='Validation')\n    gap = ts_mean[-1] - vs_mean[-1]\n    ax.set_title(f'{title}\\nFinal gap: {gap:.3f}')\n    ax.set_xlabel('Training examples')\n    ax.set_ylabel('Accuracy')\n    ax.legend(loc='lower right')\n    ax.set_ylim(0.5, 1.01)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4.5))\n\n# Underfitting: logistic regression on this nonlinear problem\nplot_learning_curve(LogisticRegression(max_iter=200), X, y,\n                    'Underfitting\\n(too simple: LogReg)', axes[0])\n\n# Good fit\nplot_learning_curve(\n    GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=RANDOM_STATE),\n    X, y, 'Good fit\\n(GBM depth=3)', axes[1])\n\n# Overfitting: very deep trees, no regularisation\nplot_learning_curve(\n    GradientBoostingClassifier(n_estimators=300, max_depth=8,\n                                min_samples_leaf=1, random_state=RANDOM_STATE),\n    X, y, 'Overfitting\\n(GBM depth=8, 300 trees)', axes[2])\n\nfig.suptitle('Learning Curves: Diagnosing Fit Quality', fontsize=13, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint('Interpretation guide:')\nprint('  Underfitting: both curves low and flat — add features or model capacity')\nprint('  Good fit:     small gap, both curves converging at reasonable accuracy')\nprint('  Overfitting:  large gap between train and validation — regularise')\nprint('  Leakage:      validation curve ABOVE or equal to train curve')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# I.4 -- Confusion matrix: what accuracy hides\n\nfrom sklearn.dummy import DummyClassifier\n\n# Create a moderately imbalanced dataset (80/20)\ndf_imb = make_so_dataset(n=5000)\ndf_imb['rare_event'] = (df_imb['ConvertedCompYearly'] >=\n                         df_imb['ConvertedCompYearly'].quantile(0.80)).astype(int)\nX = df_imb[['YearsCodePro','uses_python','uses_sql','uses_js','uses_ai']].fillna(0)\ny = df_imb['rare_event']\n\nX_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n\nmodels = {\n    'Always predict majority': DummyClassifier(strategy='most_frequent'),\n    'GBM (no class weight)':   GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE),\n    'GBM (class_weight)':      GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE),\n}\n\n# Manually add class weight for third model\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\nprint(f'Class distribution: {dict(y.value_counts().sort_index())}')\nprint(f'{\"Model\":<30} {\"Accuracy\":>10} {\"F1 (minority)\":>15} {\"AUC-ROC\":>10}')\nprint('-' * 70)\n\nfor ax, (name, clf) in zip(axes, models.items()):\n    if 'class_weight' in name:\n        # Resample manually: oversample minority in training\n        min_idx = y_tr[y_tr == 1].index\n        oversample = pd.concat([X_tr, X_tr.loc[min_idx.repeat(3)]])\n        y_over     = pd.concat([y_tr, y_tr.loc[min_idx.repeat(3)]])\n        clf.fit(oversample, y_over)\n    else:\n        clf.fit(X_tr, y_tr)\n\n    preds = clf.predict(X_te)\n    acc   = accuracy_score(y_te, preds)\n    f1    = f1_score(y_te, preds, zero_division=0)\n    try:\n        auc = roc_auc_score(y_te, clf.predict_proba(X_te)[:, 1])\n    except Exception:\n        auc = float('nan')\n\n    print(f'{name:<30} {acc:>10.4f} {f1:>15.4f} {auc:>10.4f}')\n    cm = confusion_matrix(y_te, preds)\n    ConfusionMatrixDisplay(cm, display_labels=['Majority','Minority']).plot(ax=ax, colorbar=False)\n    ax.set_title(f'{name}\\nAcc={acc:.3f}, F1={f1:.3f}')\n\nplt.tight_layout()\nplt.show()\nprint('\\nConclusion: the majority classifier achieves high accuracy but zero F1.')\nprint('Always report F1 or AUC-PR alongside accuracy on imbalanced problems.')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## I.5 — Class Imbalance Disasters\n\n**What goes wrong:** Your dataset has many more examples of one class.\nModels minimising cross-entropy loss will learn to mostly predict the majority\nclass — it's the locally optimal thing to do given the loss surface.\n\n### The metrics that actually matter\n\n| Metric | Use when... | Watch out for... |\n|--------|------------|------------------|\n| Accuracy | Classes roughly balanced | Completely misleading on imbalanced data |\n| F1 score | Moderate imbalance | Doesn't account for true negatives |\n| AUC-ROC | Binary classification | Optimistic when positives are rare |\n| **AUC-PR** | **Severe imbalance, positives rare** | **The right metric for rare events** |\n| MCC | Best single-number summary | Less interpretable |\n\n### SMOTE: what it does and when it backfires\nSMOTE (Synthetic Minority Oversampling) generates synthetic examples by\ninterpolating between existing minority class examples. It helps with moderate\nimbalance on structured data. It **backfires** when:\n- Applied before train/test split (data leakage)\n- Applied to high-dimensional data (interpolated examples are unrealistic)\n- Applied to text or images (interpolation in raw feature space is meaningless)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# I.5 -- PR-AUC vs ROC-AUC: the metric that survives severe imbalance\n\nfrom sklearn.metrics import average_precision_score, PrecisionRecallDisplay, RocCurveDisplay\n\n# Severe imbalance: 95/5\ndf_sev = make_so_dataset(n=10000)\ndf_sev['rare_event'] = (df_sev['ConvertedCompYearly'] >=\n                         df_sev['ConvertedCompYearly'].quantile(0.95)).astype(int)\nprint(f'Class balance: {dict(df_sev[\"rare_event\"].value_counts().sort_index())}')\nprint(f'Minority fraction: {df_sev[\"rare_event\"].mean():.1%}')\n\nFEATURES = ['YearsCodePro','uses_python','uses_sql','uses_js','uses_ai']\nX = df_sev[FEATURES].fillna(0)\ny = df_sev['rare_event']\nX_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2,\n                                            stratify=y, random_state=RANDOM_STATE)\n\nclf = GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE)\nclf.fit(X_tr, y_tr)\nproba = clf.predict_proba(X_te)[:, 1]\n\nroc_auc = roc_auc_score(y_te, proba)\npr_auc  = average_precision_score(y_te, proba)\ndummy_pr_auc = y_te.mean()  # random classifier's PR-AUC baseline\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4.5))\nRocCurveDisplay.from_predictions(y_te, proba, ax=axes[0],\n                                  name=f'GBM (AUC={roc_auc:.3f})')\naxes[0].plot([0,1],[0,1],'k--',label='Random (AUC=0.500)')\naxes[0].set_title('ROC Curve\\n(optimistic on imbalanced data)')\naxes[0].legend()\n\nPrecisionRecallDisplay.from_predictions(y_te, proba, ax=axes[1],\n                                         name=f'GBM (AP={pr_auc:.3f})')\naxes[1].axhline(dummy_pr_auc, color='k', linestyle='--',\n                label=f'Random baseline ({dummy_pr_auc:.3f})')\naxes[1].set_title('Precision-Recall Curve\\n(honest on imbalanced data)')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f'ROC-AUC:   {roc_auc:.3f}  (looks good — misleading)')\nprint(f'PR-AUC:    {pr_auc:.3f}  (honest — much lower, shows real challenge)')\nprint(f'Random PR: {dummy_pr_auc:.3f}  (baseline: always predict minority fraction)')\nprint()\nprint('Rule: if minority class < 10%, use PR-AUC as your primary metric.')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## I.6 — Gradient Problems in Deep Learning\n\n**What goes wrong:** Gradients in deep networks can vanish (become too small\nto update early layers) or explode (become so large they corrupt weights).\nBoth failures are subtle — the training loop runs without errors.\n\n| Problem | Symptom | Root cause | Fix |\n|---------|---------|-----------|-----|\n| Vanishing | Early layer weights barely change; loss plateaus | Sigmoid/tanh in deep nets; bad init | BatchNorm, ReLU, residual connections |\n| Exploding | Loss goes to `nan` or `inf` after a few steps | LR too high; gradient accumulation | Gradient clipping; lower LR |\n| Dead ReLU | Some neurons output zero forever | Negative bias at init; high LR | LeakyReLU; careful LR; batch norm |\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# I.6 -- Detecting and fixing gradient problems\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef make_salary_tensors(n: int = 4000) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Synthetic salary regression data as tensors.\"\"\"\n    df_ = make_so_dataset(n)\n    FEAT = ['YearsCodePro','uses_python','uses_sql','uses_js','uses_ai']\n    X = torch.tensor(df_[FEAT].fillna(0).values, dtype=torch.float32)\n    y = torch.tensor(\n        np.log(df_['ConvertedCompYearly'].values / 1000), dtype=torch.float32).unsqueeze(1)\n    return X.to(DEVICE), y.to(DEVICE)\n\nX_all, y_all = make_salary_tensors()\nn_tr = int(len(X_all) * 0.8)\nX_tr, X_te = X_all[:n_tr], X_all[n_tr:]\ny_tr, y_te = y_all[:n_tr], y_all[n_tr:]\n\n# ── Gradient norm logger ─────────────────────────────────────────\ndef train_with_grad_logging(\n    model:        nn.Module,\n    lr:           float,\n    n_epochs:     int = 20,\n    clip:         Optional[float] = None,\n    label:        str = '',\n) -> dict:\n    \"\"\"Train and log per-epoch gradient norms and losses.\"\"\"\n    model = model.to(DEVICE)\n    optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n    history = {'loss': [], 'grad_norm': []}\n\n    for epoch in range(n_epochs):\n        model.train()\n        optimiser.zero_grad()\n        out  = model(X_tr)\n        loss = F.mse_loss(out, y_tr)\n        if torch.isnan(loss):\n            print(f'  [{label}] NaN loss at epoch {epoch} — exploding gradients')\n            history['loss'].append(float('nan'))\n            history['grad_norm'].append(float('nan'))\n            break\n        loss.backward()\n\n        # Log gradient norm before clipping\n        total_norm = sum(p.grad.data.norm(2).item() ** 2\n                         for p in model.parameters() if p.grad is not None) ** 0.5\n        history['grad_norm'].append(total_norm)\n\n        if clip:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n\n        optimiser.step()\n        history['loss'].append(loss.item())\n\n    return history\n\n\ndef make_deep_sigmoid(depth: int = 6) -> nn.Sequential:\n    \"\"\"Deep network with sigmoid activations -- prone to vanishing gradients.\"\"\"\n    layers = []\n    in_f = 5\n    for _ in range(depth):\n        layers += [nn.Linear(in_f, 32), nn.Sigmoid()]\n        in_f = 32\n    layers.append(nn.Linear(32, 1))\n    return nn.Sequential(*layers)\n\ndef make_deep_relu(depth: int = 6) -> nn.Sequential:\n    \"\"\"Deep network with ReLU + BatchNorm -- healthy gradient flow.\"\"\"\n    layers = []\n    in_f = 5\n    for _ in range(depth):\n        layers += [nn.Linear(in_f, 32), nn.BatchNorm1d(32), nn.ReLU()]\n        in_f = 32\n    layers.append(nn.Linear(32, 1))\n    return nn.Sequential(*layers)\n\n\nprint('Training comparison (20 epochs):')\nh_vanish  = train_with_grad_logging(make_deep_sigmoid(), lr=0.001,  label='Sigmoid (vanishing)')\nh_explode = train_with_grad_logging(make_deep_sigmoid(), lr=5.0,    label='High LR (exploding)')\nh_clip    = train_with_grad_logging(make_deep_sigmoid(), lr=5.0, clip=1.0, label='High LR + clip')\nh_relu    = train_with_grad_logging(make_deep_relu(),    lr=0.001,  label='ReLU+BN (healthy)')\n\nfig, axes = plt.subplots(1, 2, figsize=(13, 4.5))\nfor h, label, ls in [\n    (h_vanish,  'Sigmoid deep (vanishing)', '-'),\n    (h_explode, 'High LR (exploding/NaN)',  '--'),\n    (h_clip,    'High LR + grad clip',      '-.'),\n    (h_relu,    'ReLU + BatchNorm',         ':'),\n]:\n    valid = [v for v in h['loss'] if not (isinstance(v, float) and v != v)]\n    valid_norms = [v for v in h['grad_norm'] if not (isinstance(v, float) and v != v)]\n    axes[0].plot(range(len(valid)), valid,       label=label, linestyle=ls, linewidth=2)\n    axes[1].plot(range(len(valid_norms)), valid_norms, label=label, linestyle=ls, linewidth=2)\n\naxes[0].set_title('Training Loss'); axes[0].set_xlabel('Epoch'); axes[0].legend(fontsize=8)\naxes[1].set_title('Gradient Norm'); axes[1].set_xlabel('Epoch')\naxes[1].set_yscale('log'); axes[1].legend(fontsize=8)\nplt.tight_layout(); plt.show()\n\nprint('Vanishing: sigmoid deep net — gradient norm near zero, loss barely moves')\nprint('Exploding: high LR — gradient norm spikes, loss hits NaN')\nprint('Fix:       gradient clipping (max_norm=1.0) + ReLU/BN architecture')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## I.7 — Tokenisation and Embedding Gotchas\n\n**What goes wrong:** Transformer pipelines have several silent failure modes\nthat produce plausible-looking outputs with degraded (or random) quality.\n\n### Truncation silently cuts off content\nMost BERT-family models have a 512-token maximum. The tokeniser's default is\n`truncation=True`, which silently drops everything after token 512.\nFor a developer job description (often 600–1000 tokens), the model never\nsees the salary range, requirements, or benefits — typically the last third.\n\n### Attention mask not passed to the model\nPadding tokens (`[PAD]`) are added to make batches uniform. If you don't pass\n`attention_mask` to the model, it attends to padding tokens as if they were\nreal content. This degrades performance and is a very common beginner mistake.\n\n### Tokeniser/model mismatch at inference\nIf you fine-tune with `bert-base-uncased` but accidentally load\n`bert-base-cased` at inference time, the vocabulary is different.\nThe model runs without error — it just produces garbage predictions.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# I.7 -- Tokenisation diagnostics\n\ntry:\n    from transformers import AutoTokenizer\n    HF_AVAILABLE = True\nexcept ImportError:\n    HF_AVAILABLE = False\n    print('transformers not installed — showing conceptual examples only')\n\n# ── Truncation audit ─────────────────────────────────────────────\nsample_texts = [\n    'Python developer with 5 years experience. ' * 30,  # ~150 tokens\n    'Senior ML engineer. ' * 80,                         # ~400 tokens\n    'Full stack developer experienced in React, Node, and AWS. ' * 25,  # ~300 tokens\n]\n\nif HF_AVAILABLE:\n    tokeniser = AutoTokenizer.from_pretrained('bert-base-uncased')\n\n    print('Truncation audit (max_length=512):')\n    print(f'{\"Text preview\":<45} {\"Tokens\":>8} {\"Truncated?\":>12} {\"% kept\":>8}')\n    print('-' * 78)\n    for text in sample_texts:\n        n_raw = len(tokeniser.encode(text, truncation=False))\n        n_trunc = len(tokeniser.encode(text, truncation=True, max_length=512))\n        truncated = n_raw > 512\n        pct_kept = min(512, n_raw) / n_raw * 100\n        print(f'{text[:42]!r:<45} {n_raw:>8} {str(truncated):>12} {pct_kept:>7.0f}%')\n\n    print()\n    print('Mitigation strategies for long documents:')\n    print('  1. Sliding window: tokenise with stride, pool CLS embeddings')\n    print('  2. First+last: concatenate first 128 and last 384 tokens')\n    print('  3. Longformer/BigBird: models supporting up to 4096 tokens')\n    print()\n\n    # ── Attention mask demo ──────────────────────────────────────\n    print('Attention mask check:')\n    batch = tokeniser(['short text', 'a much longer text with more tokens'],\n                       padding=True, return_tensors='pt')\n    print(f'  input_ids shape:      {batch[\"input_ids\"].shape}')\n    print(f'  attention_mask shape: {batch[\"attention_mask\"].shape}')\n    print(f'  Mask row 0: {batch[\"attention_mask\"][0].tolist()}')\n    print(f'  Mask row 1: {batch[\"attention_mask\"][1].tolist()}')\n    print(f'  Zeros in mask = padding positions model must ignore')\n    print(f'  Always pass attention_mask= to model(**batch)')\nelse:\n    print('Conceptual example (transformers not installed):')\n    print('  tokens = tokeniser(text, truncation=True, max_length=512)')\n    print('  # Check: does len(tokens[\"input_ids\"]) == 512?')\n    print('  # If yes — content was cut. Audit long texts explicitly.')\n    print()\n    print('  Always audit your corpus:')\n    print('  lengths = [len(tokeniser.encode(t)) for t in texts]')\n    print('  pct_truncated = sum(l > 512 for l in lengths) / len(lengths)')\n    print('  print(f\"{pct_truncated:.1%} of texts will be truncated\")')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## I.8 — Training-Serving Skew\n\n**What goes wrong:** The feature computation during training and during\ninference produces different values for the same raw input.\nThe model was trained on correctly computed features; at serving time,\nslightly different features are fed in. The model degrades silently.\n\n### Common causes\n\n- **Different libraries:** pandas vs SQL vs Spark computing the same feature differently\n- **Different imputation:** training uses `df.fillna(median)` with the training median;\n  production code uses a hardcoded number that drifts from the actual median\n- **Feature order mismatch:** model trained on `[A, B, C]`; served `[B, A, C]`.\n  Tree models are column-order agnostic; linear models are not\n- **Timezone or rounding differences:** `days_since_survey` computed differently\n  in training pipeline vs real-time inference\n\n### The fix: one feature computation function, used everywhere\nExport the *entire* sklearn Pipeline (including all preprocessing), not just\nthe model. The pipeline is your contract: same input → same features → same output.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# I.8 -- Detecting and preventing training-serving skew\n\nimport joblib\nfrom io import BytesIO\n\n# ── Simulate skew: training uses Pipeline median imputation;\n# serving code accidentally uses a hardcoded value ─────────────────\ndf_skew = make_so_dataset(n=5000, nan_frac=0.10)\ndf_skew['high_earner'] = (df_skew['ConvertedCompYearly'] >=\n                           df_skew['ConvertedCompYearly'].quantile(0.60)).astype(int)\nFEATURES = ['YearsCodePro','uses_python','uses_sql','uses_js','uses_ai']\nX = df_skew[FEATURES]\ny = df_skew['high_earner']\nX_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n\n# Train with Pipeline (correct)\npipe = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('clf',     GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE))\n])\npipe.fit(X_tr, y_tr)\nacc_correct = accuracy_score(y_te, pipe.predict(X_te))\n\n# Simulate skew: serving code fills NaN with 0 instead of learned median\nX_te_skewed = X_te.fillna(0)  # wrong imputation at serving time\n# Extract just the GBM (no pipeline wrapper) as if someone exported model separately\ngbm_only = pipe.named_steps['clf']\n# Feed skewed features directly to the model\nX_te_skewed_imputed_wrong = X_te_skewed.values  # filled with 0\nX_te_correct_imputed = pipe.named_steps['imputer'].transform(X_te)  # pipeline median\n\nacc_skewed  = accuracy_score(y_te, gbm_only.predict(X_te_skewed_imputed_wrong))\nacc_pipeline = accuracy_score(y_te, gbm_only.predict(X_te_correct_imputed))\n\nprint('Training-serving skew demonstration:')\nprint(f'  Pipeline (correct imputation):         {acc_pipeline:.4f}')\nprint(f'  Skewed  (fillna(0) instead of median): {acc_skewed:.4f}')\nprint(f'  Accuracy drop due to skew:             {acc_pipeline - acc_skewed:+.4f}')\nprint()\n\n# ── Feature schema validation at serving time ─────────────────────\nprint('Feature schema validation (add to your FastAPI endpoint):')\nexpected_features = list(X_tr.columns)\nexpected_dtypes   = dict(X_tr.dtypes)\n\ndef validate_serving_features(X_input: pd.DataFrame) -> list[str]:\n    \"\"\"Return list of schema violations. Empty = clean.\"\"\"\n    errors = []\n    missing = set(expected_features) - set(X_input.columns)\n    extra   = set(X_input.columns) - set(expected_features)\n    order_ok = list(X_input.columns) == expected_features\n    if missing: errors.append(f'Missing features: {missing}')\n    if extra:   errors.append(f'Unexpected features: {extra}')\n    if not order_ok: errors.append(f'Feature order mismatch')\n    return errors\n\n# Good input\nprint('  Good input:', validate_serving_features(X_te) or 'OK')\n# Bad input: wrong order\nX_wrong_order = X_te[list(reversed(FEATURES))]\nprint('  Wrong order:', validate_serving_features(X_wrong_order))\n# Bad input: missing feature\nprint('  Missing col:', validate_serving_features(X_te.drop(columns=['uses_ai'])))\n\nprint()\nprint('Best practice: serialise and serve the full Pipeline, not just the model.')\nbuf = BytesIO()\njoblib.dump(pipe, buf)\nbuf.seek(0)\npipe_reloaded = joblib.load(buf)\nacc_reloaded = accuracy_score(y_te, pipe_reloaded.predict(X_te))\nprint(f'Reloaded pipeline accuracy: {acc_reloaded:.4f} (identical = no skew)')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## I.9 — Diagnostic Checklist\n\nUse this checklist when your model's behaviour is puzzling.\nWork through it top to bottom — each stage rules out a class of failures.\n\n### Stage 1 — Data integrity\n- [ ] `df.isnull().sum()` — any unexpected NaN?\n- [ ] `df.duplicated().sum()` — any duplicate rows that could cause row leakage?\n- [ ] `df.dtypes` — are all columns the expected types? (int vs float, object vs category)\n- [ ] `df[target].value_counts()` — is the class distribution what you expect?\n- [ ] Is there a time column? Have you ensured no future data leaks into training?\n\n### Stage 2 — Preprocessing integrity\n- [ ] Is all preprocessing inside a `Pipeline`? No `fit_transform` on full dataset?\n- [ ] Does the training pipeline include imputers for all potentially null columns?\n- [ ] Are categorical encoders seeing all categories they'll encounter at inference?\n- [ ] Are any features derived from the target or from post-event information?\n\n### Stage 3 — Model sanity checks\n- [ ] `train_accuracy >> test_accuracy`? → overfitting: regularise or reduce capacity\n- [ ] Both low? → underfitting: add features or model capacity\n- [ ] Both high but F1 / AUC-PR low? → class imbalance: check confusion matrix\n- [ ] `test_accuracy ≈ train_accuracy` and both suspiciously high? → likely leakage\n- [ ] Loss = `nan` after step 1? → NaN in data, exploding gradient, or wrong loss function\n\n### Stage 4 — Deep learning specific\n- [ ] Log gradient norms per epoch. Near zero = vanishing. Spike then NaN = exploding\n- [ ] Visualise activations. All-zero layers = dead ReLUs\n- [ ] Is `optimizer.zero_grad()` called before each backward pass?\n- [ ] Is `model.eval()` called before inference (disables BatchNorm/Dropout)?\n- [ ] Is `torch.no_grad()` used during evaluation (prevents gradient accumulation)?\n\n### Stage 5 — NLP / transformer specific\n- [ ] What fraction of your texts exceed `max_length`? Is truncation acceptable?\n- [ ] Is `attention_mask` passed to the model in every forward call?\n- [ ] Are you using the same tokeniser checkpoint at train and inference time?\n- [ ] Are special tokens `[CLS]`, `[SEP]` handled consistently?\n\n### Stage 6 — Production / serving\n- [ ] Is the full Pipeline (not just the model) serialised and loaded at serving time?\n- [ ] Are feature names and order validated at the API boundary?\n- [ ] Is PSI monitoring running? Has it alerted recently?\n- [ ] Is the serving environment using the same library versions as training?\n\n### Decision tree: 'My model is performing worse than expected'\n\n```\nModel performing worse than expected\n│\n├─► Was performance good during development?\n│   ├─► NO  → Was it good on training data?\n│   │         ├─► NO  → Underfitting: more features, more capacity, fewer constraints\n│   │         └─► YES → Overfitting: regularise, more data, early stopping\n│   │\n│   └─► YES → Check for leakage: shuffle test, feature importances, temporal audit\n│\n├─► Did performance drop after deployment?\n│   ├─► Suddenly → Training-serving skew: feature computation changed?\n│   │              Leakage source removed in production?\n│   └─► Gradually → Data drift: PSI monitoring (Chapter 12)\n│\n└─► Is the metric the right one?\n    └─► Imbalanced classes → Switch to F1 / PR-AUC\n        Deep learning NaN → Check gradients, inputs, loss function\n```\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "---\n\n## Summary\n\nThe eight failure modes covered in this appendix share a common property:\n**they don't crash your code**. They produce models that train, evaluate,\nand deploy — but underperform, mislead, or silently deteriorate.\n\n| Failure mode | Detection method | Key fix |\n|-------------|-----------------|--------|\n| Data leakage | Feature importance dominance; shuffle test | Temporal hold-out; no future-derived features |\n| Train/test contamination | CV score vs hold-out comparison | Always use Pipeline |\n| NaN propagation | `isnull().sum()`; missingness correlation | Explicit imputation inside Pipeline |\n| Wrong fit diagnosis | Learning curves | Plot train vs val score by sample size |\n| Class imbalance | Confusion matrix; PR-AUC vs ROC-AUC | Report F1 and PR-AUC; oversample or class weights |\n| Gradient problems | Gradient norm logging | Gradient clipping; ReLU + BatchNorm |\n| Tokenisation | Truncation audit; attention mask check | `len(tokens) > max_length` alert; always pass mask |\n| Training-serving skew | Schema validation; pipeline reload test | Serialise full Pipeline |\n\n**The single most impactful habit:** run the diagnostic checklist (Section I.9)\nat the end of every modelling session — before declaring a result final.\n\n---\n\n*End of Appendix I — Python for AI/ML*  \n[![Back to TOC](https://img.shields.io/badge/Back_to-Table_of_Contents-1B3A5C?style=flat-square)](https://colab.research.google.com/github/timothy-watt/python-for-ai-ml/blob/main/Python_for_AIML_TOC.ipynb)\n"
    }
  ]
}